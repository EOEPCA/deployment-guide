{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deployment Guide","text":"<p>The Deployment Guide describes how each building-block comprising the EOEPCA Reference Implementation is configured and deployed. A full system deployment is described, in which components are deployed with complementary configurations that facilitate their integration as a coherent system. Nevertheless, each component can be cherry-picked from this system deployment for individual re-use.</p> <p>The deployment is organised into the following sections:</p> <ul> <li>Cluster   Establish the Kubernetes cluster and other prerequisites for the deployment of the EOEPCA system.</li> <li>EOEPCA   Deployment of the EOEPCA components.</li> <li>Examples   Deployment examples.</li> </ul>"},{"location":"cluster/cluster-prerequisites/","title":"Cluster Prerequisites","text":"<p>The following prerequisite components are assumed to be deployed in the cluster.</p>"},{"location":"cluster/cluster-prerequisites/#nginx-ingress-controller","title":"Nginx Ingress Controller","text":"<pre><code># Install the Nginx Ingress Controller helm chart\nhelm upgrade -i --version='&lt;4.5.0' \\\n  --repo https://kubernetes.github.io/ingress-nginx \\\n  ingress-nginx ingress-nginx \\\n  --wait\n</code></pre> <p>Note</p> <p>For Kubernetes version 1.22 and earlier the version of the Nginx Ingress Controller must be before v4.5.0.</p> <p>To target the Nginx Ingress Controller the <code>kubernetes.io/ingress.class: nginx</code> annotation must be applied to the Ingress resource\u2026 <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    ...\n</code></pre></p>"},{"location":"cluster/cluster-prerequisites/#cert-manager","title":"Cert Manager","text":"<pre><code># Install the Cert Manager helm chart\nhelm upgrade -i --namespace cert-manager --create-namespace \\\n  --repo https://charts.jetstack.io \\\n  --set installCRDs=true \\\n  cert-manager cert-manager\n</code></pre>"},{"location":"cluster/cluster-prerequisites/#letsencrypt-certificates","title":"Letsencrypt Certificates","text":"<p>Once the Certificate Manager is deployed, then we can establish <code>ClusterIssuer</code> operators in the cluster to support use of TLS with service <code>Ingress</code> endpoints.</p> <p>For Letsencrypt we can define two <code>ClusterIssuer</code> - for <code>production</code> and for <code>staging</code>.</p> <p>NOTE that these require the cluster to be publicly accessible, in order for the <code>http01</code> acme flow to verify the domain ownership. Local development deployments will typically not have public IP/DNS - in which case the system deployment can proceed, but without TLS support for the service endpoints.</p>"},{"location":"cluster/cluster-prerequisites/#production","title":"Production","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-production\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: eoepca.systemteam@telespazio.com\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource that will be used to store the account's private key.\n      name: letsencrypt-production-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre>"},{"location":"cluster/cluster-prerequisites/#staging","title":"Staging","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # You must replace this email address with your own.\n    # Let's Encrypt will use this to contact you about expiring\n    # certificates, and issues related to your account.\n    email: eoepca.systemteam@telespazio.com\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      # Secret resource that will be used to store the account's private key.\n      name: letsencrypt-staging-account-key\n    # Add a single challenge solver, HTTP01 using nginx\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre> <p>To exploit the specified ClusterIssuer the <code>cert-manager.io/cluster-issuer</code> annotation must be applied to the Ingress resource. For example\u2026 <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/cluster-issuer: letsencrypt-production\n    ...\n</code></pre></p>"},{"location":"cluster/cluster-prerequisites/#sealed-secrets","title":"Sealed Secrets","text":"<p>The EOEPCA development team maintain their deployment configurations in GitHub - for declarative, reproducible cluster deployments.</p> <p>Various <code>Secret</code> are relied upon by the system services. Secrets should not be exposed by commit to GitHub.</p> <p>Instead <code>SealedSecret</code> are committed to GitHub, which are encrypted, and can only be decrypted by the <code>sealed-secret-controller</code> that runs within the cluster. The <code>sealed-secret-controller</code> decrypts the <code>SealedSecret</code> to a regular <code>Secret</code> (of the same name) that can then be consumed by the cluster components.</p> <p>The <code>sealed-secret-controller</code> is deployed to the cluster using the helm chart\u2026</p> <pre><code>helm install --version 2.1.8 --create-namespace --namespace infra \\\n  --repo https://bitnami-labs.github.io/sealed-secrets \\\n  eoepca-sealed-secrets sealed-secrets\n</code></pre> <p>Once the controller is deployed within the cluster, then the <code>kubeseal</code> command can be used to create a <code>SealedSecret</code> from a regular <code>Secret</code>, as follows\u2026</p> <p>Create example Secret\u2026 <pre><code>kubectl -n test create secret generic mysecret \\\n  --from-literal=password=changeme \\\n  --dry-run=client -o yaml \\\n  &gt; mysecret.yaml\n</code></pre></p> <p>Create SealedSecret from Secret using kubeseal\u2026 <pre><code>kubeseal -o yaml \\\n  --controller-name eoepca-sealed-secrets \\\n  --controller-namespace infra \\\n  &lt; mysecret.yaml \\\n  &gt; mysecret-sealed.yaml\n</code></pre></p>"},{"location":"cluster/cluster-prerequisites/#references","title":"References","text":"<ul> <li>Sealed Secrets on GitHub</li> <li><code>kubeseal</code> Release</li> </ul>"},{"location":"cluster/cluster-prerequisites/#minio-object-storage","title":"MinIO Object Storage","text":"<p>Various building blocks require access to an S3-compatible object storage service. In particular the ADES processing service expects to stage-out its processing results to S3 object storage. Ideally the cloud provider for your deployment will make available a suitable object storage service.</p> <p>As a workaround, in the absence of an existing object storage, it is possible to use MinIO to establish an object storage service within the Kubernetes cluster. We use the minio helm chart provided by the MinIO Project.</p> <pre><code># Install the minio helm chart\nhelm upgrade -i -f minio-values.yaml --namespace rm --create-namespace \\\n  --repo https://charts.min.io/ \\\n  minio minio \\\n  --wait\n</code></pre> <p>Note</p> <p>The Kubernetes namespace <code>rm</code> is used above as an example, and can be changed according to your deployment preference.</p> <p>The minio deployment is customised via the values file <code>minio-values.yaml</code>, for example\u2026</p> <pre><code>existingSecret: minio-auth\nreplicas: 2\n\ningress:\n  enabled: true\n  ingressClassName: nginx\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: '600'\n  path: /\n  hosts:\n    - minio.192-168-49-2.nip.io\n  tls:\n    - secretName: minio-tls\n      hosts:\n        - minio.192-168-49-2.nip.io\n\nconsoleIngress:\n  enabled: true\n  ingressClassName: nginx\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: '600'\n  path: /\n  hosts:\n    - console.minio.192-168-49-2.nip.io\n  tls:\n  - secretName: minio-console-tls\n    hosts:\n      - console.minio.192-168-49-2.nip.io\n\nresources:\n  requests:\n    memory: 1Gi\n\npersistence:\n  storageClass: standard\n\nbuckets:\n  - name: eoepca\n  - name: cache-bucket\n</code></pre> <p>Note</p> <ul> <li>The example values assuming a TLS configuration using <code>letsencrypt</code> certificate provider</li> <li>The admin credentials are provided by the Kubernetes secret named <code>minio-auth</code> - see below</li> <li>The annotation <code>nginx.ingress.kubernetes.io/proxy-body-size</code> was found to be required to allow transfer of large files (such as data products) through the nginx proxy</li> </ul>"},{"location":"cluster/cluster-prerequisites/#minio-credentials-secret","title":"Minio Credentials Secret","text":"<p>The Minio admin credentials are provided via a Kubernetes secret that is referenced from the Minio helm chart deployment values. For example\u2026</p> <pre><code>kubectl -n rm create secret generic minio-auth \\\n  --from-literal=rootUser=\"eoepca\" \\\n  --from-literal=rootPassword=\"changeme\"\n</code></pre> <p>Note</p> <p>The secret must be created in the same Kubernetes namespace as the Minio service deployment - e.g. <code>rm</code> namespce in the example above.</p>"},{"location":"cluster/cluster-prerequisites/#s3cmd-configuration","title":"s3cmd Configuration","text":"<p>The <code>s3cmd</code> can be configured for access to the MinIO deployment. The <code>--configure</code> option can be used to prepare a suitable configuration file for <code>s3cmd</code>\u2026</p> <pre><code>s3cmd -c mys3cfg --configure\n</code></pre> <p>In response to the prompts, the following configuration selections are applicable to the above settings\u2026</p> <pre><code>Access Key: eoepca\nSecret Key: changeme\nDefault Region: us-east-1\nS3 Endpoint: minio.192-168-49-2.nip.io\nDNS-style bucket+hostname:port template for accessing a bucket: minio.192-168-49-2.nip.io\nEncryption password: \nPath to GPG program: /usr/bin/gpg\nUse HTTPS protocol: True\nHTTP Proxy server name: \nHTTP Proxy server port: 0\n</code></pre> <p>Save the configuration file, and check access to the S3 object store with\u2026</p> <pre><code># Create a bucket\ns3cmd -c mys3cfg mb s3://eoepca\n\n# List buckets\ns3cmd -c mys3cfg ls\n</code></pre> <p>For example, using our sample deployment, the following can be used to interface with the MinIO service deployed in minikube\u2026 <pre><code>s3cmd -c deploy/cluster/s3cfg ls\n</code></pre></p>"},{"location":"cluster/cluster-prerequisites/#references_1","title":"References","text":"<ul> <li>MinIO Website</li> <li>MinIO Helm Chart</li> <li>MinIO on GitHub</li> </ul>"},{"location":"cluster/helm-repositories/","title":"Helm Repositories","text":"<p>Note</p> <p>This section identifies some helm chart repositories that can be referenced (for convenience) via <code>helm add</code>. Nevertheless, all helm commands included in the guide specifically reference the source helm repository via the <code>--repo</code> argument to the <code>helm install</code> command - and thus it is not specifically necessary to <code>add</code> these repositories in advance.</p>"},{"location":"cluster/helm-repositories/#eoepca-helm-charts","title":"EOEPCA Helm Charts","text":"<p>The EOEPCA building-blocks are engineered as containers for deployment to a Kubernetes cluster. Each building block defines a Helm Chart to facilitate its deployment.</p> <p>The EOEPCA Helm Chart Repository is configured with <code>helm</code> as follows\u2026 <pre><code>helm repo add eoepca https://eoepca.github.io/helm-charts/\n</code></pre></p>"},{"location":"cluster/helm-repositories/#third-party-helm-charts","title":"Third-party Helm Charts","text":"<p>In addition to the EOEPCA Helm Chart Repository, a variety of third party helm repositories are relied upon, as identified below.</p>"},{"location":"cluster/helm-repositories/#cert-manager","title":"Cert Manager","text":"<pre><code>helm repo add jetstack https://charts.jetstack.io\n</code></pre>"},{"location":"cluster/helm-repositories/#nginx-ingress-controller","title":"Nginx Ingress Controller","text":"<pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n</code></pre>"},{"location":"cluster/helm-repositories/#minio","title":"Minio","text":"<pre><code>helm repo add minio https://charts.min.io/\n</code></pre>"},{"location":"cluster/helm-repositories/#sealed-secrets-bitnami","title":"Sealed Secrets (Bitnami)","text":"<pre><code>helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\n</code></pre>"},{"location":"cluster/helm-repositories/#harbor","title":"Harbor","text":"<pre><code>helm repo add harbor https://helm.goharbor.io\n</code></pre>"},{"location":"cluster/helm-repositories/#repo-update","title":"Repo Update","text":"<p>Refresh the local repo cache, after <code>helm repo add</code>\u2026</p> <pre><code>helm repo update\n</code></pre>"},{"location":"cluster/kubernetes/","title":"Kubernetes Cluster","text":"<p>The EOEPCA Reference Implementation has been developed with Kubernetes as its deployment target. The system components have been developed, deployed and tested using a cluster at version <code>v1.22.5</code>.</p>"},{"location":"cluster/kubernetes/#rancher-kubernetes-engine-rke","title":"Rancher Kubernetes Engine (RKE)","text":"<p>The development, integration and test clusters have been established using Rancher Kubernetes Engine (RKE) at version <code>v1.22.5</code>.</p> <p>An example of the creation of the EOEPCA Kubernetes clusters can be found on the GitHub Kubernetes Setup page. CREODIAS has been used for the development hosting infrastructure - which provides OpenStack infrastructure that is backed by Cloudferro. An example of the Terraform configurations used to automate the creation of the cloud infrastructure that underpins the RKE deployment can be found on the GitHub CREODIAS Setup page.</p>"},{"location":"cluster/kubernetes/#local-kubernetes","title":"Local Kubernetes","text":"<p>To make a full deployment of the EOEPCA Reference Implementation requires a multi-node node cluster with suitable resources. For example, the development cluster comprises:</p> <ul> <li>1 Master node (2 vCPU, 8 GB RAM)</li> <li>5 Worker nodes (4 vCPU, 16 GB RAM)</li> <li>1 NFS server (2 vCPU, 8 GB RAM)</li> </ul> <p>Limited local deployment can be made using a suitable local single-node kuberbetes deployment using - for example using minikube\u2026</p> <pre><code>minikube -p eoepca start --cpus max --memory max --kubernetes-version v1.22.5\nminikube profile eoepca\n</code></pre> <p>With such a deployment it is possible to deploy individual building-blocks for local development, or building-blocks in combination - within the constraints of the local host resources.</p>"},{"location":"cluster/prerequisite-tooling/","title":"Prerequisite Tooling","text":"<p>There are some standard tools referenced in this guide. These are detailed in the following subsections.</p>"},{"location":"cluster/prerequisite-tooling/#docker","title":"docker","text":"<p>Docker faciliates the creation, management and execution of containers. Whilst not strictly necessary to support deployment to an existing/managed Kubernetes cluster, it can nevertheless be useful to have local access to the docker tooling. For example, if minikube is used to follow this guide using a local k8s cluster, then this is best achieved using minikube\u2019s docker driver.</p> <p>Docker is most easily installed with\u2026 <pre><code>curl -fsSL https://get.docker.com | sh\n</code></pre></p> <p>For convenience, add your user to the <code>docker</code> group\u2026 <pre><code>sudo usermod -aG docker ${USER}\n</code></pre></p> <p>Logout/in to refresh your session\u2019s group permissions.</p>"},{"location":"cluster/prerequisite-tooling/#kubectl","title":"kubectl","text":"<p>Kubectl is the main tool for interaction with a Kubernetes cluster. The latest version can be installed with\u2026 <pre><code>mkdir -p $HOME/.local/bin \\\n&amp;&amp; curl -fsSLo $HOME/.local/bin/kubectl \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" \\\n&amp;&amp; chmod +x $HOME/.local/bin/kubectl\n</code></pre></p> <p>See the official kubectl installation documentation for more installation options.</p>"},{"location":"cluster/prerequisite-tooling/#helm","title":"helm","text":"<p>Helm is the Kubernetes package manager, in which components are deployed to a Kubernetes cluster via helm charts. The helm charts are instantiated for deployment via \u2018values\u2019 that configure the chart templates.</p> <p>The latest helm version can be installed with\u2026 <pre><code>export HELM_INSTALL_DIR=\"$HOME/.local/bin\" \\\n&amp;&amp; curl -sfL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n</code></pre></p> <p>See the official helm installation documentation for more installation options.</p>"},{"location":"cluster/prerequisite-tooling/#minikube","title":"minikube","text":"<p>Minikube is a tool that allows to create a local (single-node) Kubernetes cluster for development/testing. It is not designed for production use. In the absence of access to a \u2018full\u2019 Kubernetes cluster, this guide can be followed using minikube.</p> <p>The latest version of minikube can be installed with\u2026 <pre><code>mkdir -p $HOME/.local/bin \\\n&amp;&amp; curl -fsSLo $HOME/.local/bin/minikube \"https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\" \\\n&amp;&amp; chmod +x $HOME/.local/bin/minikube\n</code></pre></p> <p>See the official minikube installation documentation for more installation options.</p>"},{"location":"eoepca/ades/","title":"Application Deployment &amp; Execution Service (ADES)","text":"<p>The ADES provides a platform-hosted execution engine through which users can initiate parameterised processing jobs using applications made available within the platform - supporting the efficient execution of the processing \u2018close to the data\u2019. Users can deploy specific \u2018applications\u2019 to the ADES, which may be their own applications, or those published by other platform users.</p>"},{"location":"eoepca/ades/#helm-chart","title":"Helm Chart","text":"<p>The ADES is deployed via the <code>ades</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are fully documented in the README for the <code>ades</code> chart.</p> <pre><code>helm install --version 2.0.24 --values ades-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  ades ades\n</code></pre>"},{"location":"eoepca/ades/#values","title":"Values","text":"<p>At minimum, values for the following attributes should be specified:</p> <ul> <li>Details of the S3 Object Store for stage-out of processing results</li> <li>Dynamic provisioning StorageClass of <code>ReadWriteMany</code> storage</li> <li>(optional) Specification of Ingress for reverse-proxy access to the service Note that this is only required in the case that the ADES will not be protected by the <code>resource-guard</code> component - ref. Resource Protection. Otherwise the ingress will be handled by the <code>resource-guard</code> - use <code>ingress.enabled: false</code>.</li> </ul> <p>Example <code>ades-values.yaml</code>\u2026</p> <pre><code>workflowExecutor:\n  inputs:\n    # Stage-in from CREODIAS eodata\n    # (only works within CREODIAS - i.e. on a Cloudferro VM)\n    STAGEIN_AWS_SERVICEURL: http://data.cloudferro.com\n    STAGEIN_AWS_ACCESS_KEY_ID: test\n    STAGEIN_AWS_SECRET_ACCESS_KEY: test\n    STAGEIN_AWS_REGION: RegionOne\n    # Stage-out to minio S3\n    # (use this if the ADES is not configured to stage-out to the Workspace)\n    STAGEOUT_AWS_SERVICEURL: http://minio.192-168-49-2.nip.io\n    STAGEOUT_AWS_ACCESS_KEY_ID: eoepca\n    STAGEOUT_AWS_SECRET_ACCESS_KEY: changeme\n    STAGEOUT_AWS_REGION: us-east-1\n    STAGEOUT_OUTPUT: s3://eoepca\n  processingStorageClass: standard\n  processingVolumeTmpSize: \"6Gi\"\n  processingVolumeOutputSize: \"6Gi\"\n  processingMaxRam: \"8Gi\"\n  processingMaxCores: \"4\"\nwps:\n  pepBaseUrl: \"http://ades-pep:5576\"\n  usePep: \"false\"\npersistence:\n  storageClass: standard\ningress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-production\n  hosts:\n    - host: ades.192-168-49-2.nip.io\n      paths: \n        - path: /\n          pathType: ImplementationSpecific\n  tls:\n    - hosts:\n        - ades.192-168-49-2.nip.io\n      secretName: ades-tls\nresources:\n  requests:\n    cpu: 100m\n    memory: 500Mi\n  limits:\n    cpu: 2\n    memory: 4Gi\n</code></pre> <p>Note</p> <p>The <code>resources:</code> above have been limited for the benefit of a minikube deployment. For a production deployment the values should be tuned (upwards) according to operational needs. Additionally, the following ADES values should also be considered\u2026 <pre><code>workflowExecutor:\n  # Max ram to use for a job\n  processingMaxRam: \"16Gi\"\n  # Max number of CPU cores to use concurrently for a job\n  processingMaxCores: \"8\"\n</code></pre></p>"},{"location":"eoepca/ades/#stage-in-stage-out-configuration","title":"Stage-in / Stage-out Configuration","text":"<p>The ADES hosts applications that are deployed and invoked in accordance with the OGC Best Practise for Application Package. Thus, the ADES provides a conformant environment within which the application is integrated for execution. A key part of the ADES\u2019s role in this is to faciltate the provision of input data to the application (stage-in), and the handling of the results output at the conclusion of application execution (stage-out).</p> <p>The ADES helm chart configures (by default) the ADES with implementations of the stage-in and stage-out functions that use the Spatio Temporal Asset Router Services utility.</p> <p>The ADES provides hooks for system integrators to override these defaults to implement their own stage-in and stage-out behaviour - for example, to integrate with their platform\u2019s own catalogue and data offering. The stage-in and stage-out are specified as CWL via the helm values\u2026</p> <pre><code>workflowExecutor:\n  stagein:\n    cwl: |\n      cwlVersion: v1.0\n      ...\n  stageout:\n    cwl: |\n      cwlVersion: v1.0\n      ...\n</code></pre> <p>For a detailed description see ADES stage-in/out configuration in the ADES wiki.</p>"},{"location":"eoepca/ades/#workspace-integration","title":"Workspace Integration","text":"<p>The ADES has the facility to integrate with the EOEPCA Workspace component for registration of staged-out processing results. This is disabled by default (<code>useResourceManager: false</code>).</p> <p>When enabled, the ADES will register the staged-out products with the user\u2019s Workspace, such that they are indexed and available via the user\u2019s Resource Catalogue and Data Access services.</p> <p>Example <code>ades-values.yaml</code> (snippet)\u2026</p> <pre><code>workflowExecutor:\n  ...\n  useResourceManager: \"true\"\n  resourceManagerWorkspacePrefix: \"ws\"\n  resourceManagerEndpoint: \"https://workspace-api.192-168-49-2.nip.io\"\n  platformDomain: \"https://auth.192-168-49-2.nip.io\"\n  ...\n</code></pre> <p>The value <code>resourceManagerWorkspacePrefix</code> must be consistent with that configured for the Workspace API deployment, (ref. value <code>prefixForName</code>).</p>"},{"location":"eoepca/ades/#protection","title":"Protection","text":"<p>As described in section Resource Protection, the <code>resource-guard</code> component can be inserted into the request path of the ADES service to provide access authorization decisions</p> <pre><code>helm install --version 1.3.1 --values ades-guard-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  ades-guard resource-guard\n</code></pre> <p>The <code>resource-guard</code> must be configured with the values applicable to the ADES for the Policy Enforcement Point (<code>pep-engine</code>) and the UMA User Agent\u2026</p> <p>Example <code>ades-guard-values.yaml</code>\u2026</p> <pre><code>#---------------------------------------------------------------------------\n# Global values\n#---------------------------------------------------------------------------\nglobal:\n  context: ades\n  domain: 192-168-49-2.nip.io\n  nginxIp: 192.168.49.2\n  certManager:\n    clusterIssuer: letsencrypt-production\n#---------------------------------------------------------------------------\n# PEP values\n#---------------------------------------------------------------------------\npep-engine:\n  configMap:\n    asHostname: auth\n    pdpHostname: auth\n  # customDefaultResources:\n  #   - name: \"ADES Service for user 'eric'\"\n  #     description: \"Protected Access for eric to his space in the ADES\"\n  #     resource_uri: \"/eric\"\n  #     scopes: []\n  #     default_owner: \"a9812efe-fc0c-49d3-8115-0f36883a84b9\"\n  #   - name: \"ADES Service for user 'bob'\"\n  #     description: \"Protected Access for bob to his space in the ADES\"\n  #     resource_uri: \"/bob\"\n  #     scopes: []\n  #     default_owner: \"4ccae3a1-3fad-4ffe-bfa7-cce851143780\"\n  volumeClaim:\n    name: eoepca-proc-pvc\n    create: false\n#---------------------------------------------------------------------------\n# UMA User Agent values\n#---------------------------------------------------------------------------\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: ades\n        paths:\n          - path: /(.*)\n            service:\n              name: ades\n              port: 80\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n  client:\n    credentialsSecretName: \"proc-client\"\n  logging:\n    level: \"info\"\n  unauthorizedResponse: 'Bearer realm=\"https://portal.192-168-49-2.nip.io/oidc/authenticate/\"'\n  openAccess: false\n  insecureTlsSkipVerify: true\n</code></pre> <p>Note</p> <ul> <li>TLS is enabled by the specification of <code>certManager.clusterIssuer</code></li> <li>The <code>letsencrypt</code> Cluster Issuer relies upon the deployment being accessible from the public internet via the <code>global.domain</code> DNS name. If this is not the case, e.g. for a local minikube deployment in which this is unlikely to be so. In this case the TLS will fall-back to the self-signed certificate built-in to the nginx ingress controller</li> <li><code>insecureTlsSkipVerify</code> may be required in the case that good TLS certificates cannot be established, e.g. if letsencrypt cannot be used for a local deployment. Otherwise the certificates offered by login-service Authorization Server will fail validation in the Resource Guard.</li> <li><code>customDefaultResources</code> can be specified to apply initial protection to the endpoint</li> </ul>"},{"location":"eoepca/ades/#client-secret","title":"Client Secret","text":"<p>The Resource Guard requires confidential client credentials to be configured through the file <code>client.yaml</code>, delivered via a kubernetes secret..</p> <p>Example <code>client.yaml</code>\u2026</p> <pre><code>client-id: a98ba66e-e876-46e1-8619-5e130a38d1a4\nclient-secret: 73914cfc-c7dd-4b54-8807-ce17c3645558\n</code></pre> <p>Example <code>Secret</code>\u2026</p> <pre><code>kubectl -n proc create secret generic proc-client \\\n  --from-file=client.yaml \\\n  --dry-run=client -o yaml \\\n  &gt; proc-client-secret.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: proc-client\n  namespace: proc\ndata:\n  client.yaml: Y2xpZW50LWlkOiBhOThiYTY2ZS1lODc2LTQ2ZTEtODYxOS01ZTEzMGEzOGQxYTQKY2xpZW50LXNlY3JldDogNzM5MTRjZmMtYzdkZC00YjU0LTg4MDctY2UxN2MzNjQ1NTU4\n</code></pre> <p>The client credentials are obtained by registration of a client at the login service web interface - e.g. https://auth.192-168-49-2.nip.io. In addition there is a helper script that can be used to create a basic client and obtain the credentials, as described in section Resource Protection\u2026 <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io \"Resource Guard\" | tee client.yaml\n</code></pre></p>"},{"location":"eoepca/ades/#ades-usage-samples","title":"ADES Usage Samples","text":"<p>This section includes some sample requests to test the deployed ADES.</p> <p>Note</p> <ol> <li>It assumed that the ADES is subject to access protection (ref. Resource Protection), in which case a User ID Token must be provided with the request - typically in the HTTP header, such as <code>Authorization: Bearer</code> or <code>X-User-Id</code>.    See section User ID Token for more details.</li> <li>The samples assume a user <code>eric</code></li> <li>The <code>snuggs</code> application is used in the example below. See also Application Package Example.</li> </ol>"},{"location":"eoepca/ades/#list-processes","title":"List Processes","text":"<p>List available processes.</p> <pre><code>curl --location --request GET 'https://ades.192-168-49-2.nip.io/eric/wps3/processes' \\\n--header 'X-User-Id: &lt;user-id-token&gt;' \\\n--header 'Accept: application/json'\n</code></pre>"},{"location":"eoepca/ades/#deploy-process","title":"Deploy Process","text":"<p>Deploy the sample application <code>snuggs</code>.</p> <pre><code>curl --location --request POST 'https://ades.192-168-49-2.nip.io/eric/wps3/processes' \\\n--header 'X-User-Id: &lt;user-id-token&gt;' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": [\n        {\n            \"id\": \"applicationPackage\",\n            \"input\": {\n                \"format\": {\n                    \"mimeType\": \"application/cwl\"\n                },\n                \"value\": {\n                    \"href\": \"https://raw.githubusercontent.com/EOEPCA/app-snuggs/main/app-package.cwl\"\n                }\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"format\": {\n                \"mimeType\": \"string\",\n                \"schema\": \"string\",\n                \"encoding\": \"string\"\n            },\n            \"id\": \"deployResult\",\n            \"transmissionMode\": \"value\"\n        }\n    ],\n    \"mode\": \"auto\",\n    \"response\": \"raw\"\n}'\n</code></pre>"},{"location":"eoepca/ades/#get-process-details","title":"Get Process Details","text":"<p>Get details for a deployed process.</p> <pre><code>curl --location --request GET 'https://ades.192-168-49-2.nip.io/eric/wps3/processes/snuggs-0_3_0' \\\n--header 'X-User-Id: &lt;user-id-token&gt;' \\\n--header 'Accept: application/json'\n</code></pre>"},{"location":"eoepca/ades/#execute-process","title":"Execute Process","text":"<p>Execute a process with supplied parameterisation.</p> <pre><code>curl --location --request POST 'https://ades.192-168-49-2.nip.io/eric/wps3/processes/snuggs-0_3_0/jobs' \\\n--header 'X-User-Id: &lt;user-id-token&gt;' \\\n--header 'Accept: application/json' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"inputs\": [\n        {\n            \"id\": \"input_reference\",\n            \"input\": {\n                \"dataType\": {\n                    \"name\": \"application/json\"\n                },\n                \"value\": \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_36RTT_20191205_0_L2A\"\n            }\n        },\n        {\n            \"id\": \"s_expression\",\n            \"input\": {\n                \"dataType\": {\n                    \"name\": \"string\"\n                },\n                \"value\": \"ndvi:(/ (- B05 B03) (+ B05 B03))\"\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"format\": {\n                \"mimeType\": \"string\",\n                \"schema\": \"string\",\n                \"encoding\": \"string\"\n            },\n            \"id\": \"wf_outputs\",\n            \"transmissionMode\": \"value\"\n        }\n    ],\n    \"mode\": \"auto\",\n    \"response\": \"raw\"\n}'\n</code></pre>"},{"location":"eoepca/ades/#job-status","title":"Job Status","text":"<p>Once a processes execution has been initiated then its progress can monitored via a job-specific URL that is returned in the HTTP response headers of the execute request.</p> <pre><code>curl --location --request GET 'https://ades.192-168-49-2.nip.io/eric/watchjob/processes/snuggs-0_3_0/jobs/2e0fabf4-4ed6-11ec-b857-626a98159388' \\\n--header 'X-User-Id: &lt;user-id-token&gt;' \\\n--header 'Accept: application/json'\n</code></pre>"},{"location":"eoepca/ades/#job-result","title":"Job Result","text":"<p>Once the job execution has completed, then the results can be obtained.</p> <pre><code>curl --location --request GET 'https://ades.192-168-49-2.nip.io/eric/watchjob/processes/snuggs-0_3_0/jobs/2e0fabf4-4ed6-11ec-b857-626a98159388/result' \\\n--header 'X-User-Id: &lt;user-id-token&gt;' \\\n--header 'Accept: application/json'\n</code></pre>"},{"location":"eoepca/ades/#undeploy-process","title":"Undeploy Process","text":"<p>A process can be deleted (undeployed).</p> <pre><code>curl --location --request DELETE 'https://ades.192-168-49-2.nip.io/eric/wps3/processes/snuggs-0_3_0' \\\n--header 'X-User-Id: &lt;user-id-token&gt;' \\\n--header 'Accept: application/json'\n</code></pre>"},{"location":"eoepca/ades/#application-package-example","title":"Application Package Example","text":"<p>For a (trivial) example application package see Example Application Package, which provides a description and illustration of the basics of creating an application that integrates with the expectations of the ADES stage-in and stage-out.</p> <p>For further reference see\u2026</p> <ul> <li>Application Packages<ul> <li>OGC Best Practise for Application Package</li> <li>Example Application Package</li> </ul> </li> <li>Common Workflow Language (CWL)<ul> <li>Guide for CWL in Earth Observation</li> <li>CWL Specification</li> <li>CWL User Guide</li> </ul> </li> </ul>"},{"location":"eoepca/ades/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the ADES can be found at:</p> <ul> <li>Helm Chart</li> <li>Wiki</li> <li>GitHub Repository</li> <li>ADES stage-in/out configuration</li> </ul>"},{"location":"eoepca/application-hub/","title":"Application Hub","text":"<p>The Application Hub provides a set of web-based tooling, including JupyterLab for interactive analysis, Code Server for application development, and the capability to add user-defined interactive dashboards.</p>"},{"location":"eoepca/application-hub/#helm-chart","title":"Helm Chart","text":"<p>The Application Hub is deployed via the <code>application-hub</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values, which are detailed in the default values file for the chart.</p> <pre><code>helm install --version 2.0.49 --values application-hub-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  application-hub application-hub\n</code></pre>"},{"location":"eoepca/application-hub/#values","title":"Values","text":"<p>The Application Hub supports many values to configure the service - ref. the default values file for the chart.</p> <p>Typically, values for the following attributes may be specified:</p> <ul> <li>The fully-qualified public URL for the service</li> <li>Specification of Ingress for reverse-proxy access to the service</li> <li>Storage class for persistence</li> <li>Node selector rule - required by JupyterHub to spawn container workloads</li> <li>Values for integration with the user workspace</li> <li>Integration of JupyterHub with the Login Service (identity provider) via OpenID Connect configuration</li> <li>OIDC client credentials from a secret</li> </ul> <p>Example <code>application-hub-values.yaml</code>\u2026</p> <pre><code>ingress:\n  enabled: true\n  annotations: {}\n  hosts:\n    - host: applicationhub.192-168-49-2.nip.io\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls:\n    - secretName: applicationhub-tls\n      hosts:\n      - applicationhub.192-168-49-2.nip.io\n  clusterIssuer: \"${TLS_CLUSTER_ISSUER}\"\n\njupyterhub:\n  ingress:\n    enabled: true\n  fullnameOverride: \"application-hub\"\n  hub:\n    existingSecret: application-hub-secrets\n    extraEnv: \n        JUPYTERHUB_ENV: \"dev\"\n        JUPYTERHUB_SINGLE_USER_IMAGE: \"eoepca/pde-container:1.0.3\"\n        OAUTH_CALLBACK_URL: https://applicationhub.192-168-49-2.nip.io/hub/oauth_callback\n        OAUTH2_USERDATA_URL: https://auth.192-168-49-2.nip.io/oxauth/restv1/userinfo\n        OAUTH2_TOKEN_URL: https://auth.192-168-49-2.nip.io/oxauth/restv1/token\n        OAUTH2_AUTHORIZE_URL: https://auth.192-168-49-2.nip.io/oxauth/restv1/authorize\n        OAUTH_LOGOUT_REDIRECT_URL: \"https://applicationhub.192-168-49-2.nip.io\"\n        OAUTH2_USERNAME_KEY: \"user_name\"\n        STORAGE_CLASS: \"standard\"\n        RESOURCE_MANAGER_WORKSPACE_PREFIX: \"ws\"\n\n        JUPYTERHUB_CRYPT_KEY:\n          valueFrom:\n            secretKeyRef:\n              name: application-hub-secrets\n              key: JUPYTERHUB_CRYPT_KEY\n\n        OAUTH_CLIENT_ID:\n          valueFrom:\n            secretKeyRef:\n              name: application-hub-secrets\n              key: OAUTH_CLIENT_ID\n\n        OAUTH_CLIENT_SECRET:\n          valueFrom:\n            secretKeyRef:\n              name: application-hub-secrets\n              key: OAUTH_CLIENT_SECRET\n\n    image:\n      name: eoepca/application-hub\n      tag: \"1.0.0\"\n      pullPolicy: Always\n      pullSecrets: []\n\n    db:\n      type: sqlite-pvc\n      upgrade:\n      pvc:\n        annotations: {}\n        selector: {}\n        accessModes:\n          - ReadWriteOnce\n        storage: 1Gi\n        subPath:\n        storageClassName: standard\n\n  singleuser:\n    image:\n      name: jupyter/minimal-notebook\n      tag: \"2343e33dec46\"\n    profileList: \n    - display_name:  \"Minimal environment\"\n      description: \"To avoid too much bells and whistles: Python.\"\n      default: \"True\"\n    - display_name:  \"EOEPCA profile\"\n      description: \"Sample profile\"\n      kubespawner_override:\n        cpu_limit\": 4\n        mem_limit\": \"8G\"\n\nnodeSelector:\n  key: minikube.k8s.io/primary\n  value: \\\"true\\\"\n</code></pre>"},{"location":"eoepca/application-hub/#client-and-credentials","title":"Client and Credentials","text":"<p>The Application Hub requires an OIDC client to registered with the Login Service in order to enable user identity integration. The client can be created via the login service web interface - e.g. https://auth.192-168-49-2.nip.io.</p> <p>In addition there is a helper script that can be used to create a basic client and obtain the credentials - using an approach that it similar to that described for Resource Protection\u2026 <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io \"Application Hub\" | tee client-apphub.yaml\n</code></pre></p> <p>This command creates the client and outputs the credentials (to file and stdout), which must then be applied in a Kubernetes secret that is expected by the Application Hub deployment\u2026</p> <pre><code>kubectl -n proc create secret generic application-hub-secrets \\\n  --from-literal=JUPYTERHUB_CRYPT_KEY=\"$(openssl rand -hex 32)\" \\\n  --from-literal=OAUTH_CLIENT_ID=\"$(cat client-apphub.yaml | grep client-id | cut -d\\  -f2)\" \\\n  --from-literal=OAUTH_CLIENT_SECRET=\"$(cat client-apphub.yaml | grep client-secret | cut -d\\  -f2)\"\n</code></pre> <p>For example\u2026</p> <pre><code>apiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\n  name: application-hub-secrets\n  namespace: proc\ndata:\n  JUPYTERHUB_CRYPT_KEY: YjA4OGEyZGU3Mzg4ZWQxNmM1Zjg2Njc0YTA5MzlhNzI5YTY5NzU1NDJhYjYwZTllNWU2ZTZhYTQ5ZTc5ZDM5Zg==\n  OAUTH_CLIENT_ID: Y2NhNDNmM2ItODQyZC00NzNmLTk3Y2YtYWUxOTNkZWJhOWMx\n  OAUTH_CLIENT_SECRET: ZWFkYjk5NDQtOTRkYS00MTU3LTg1ZDgtNWJhMmJmODg5ZjE2\n</code></pre>"},{"location":"eoepca/application-hub/#post-deployment-manual-steps","title":"Post-deployment Manual Steps","text":"<p>The deployment of the Application Hub has been designed, as far as possible, to automate the configuration. However, there remain some steps that must be performed manually after the scripted deployment has completed\u2026</p> <ul> <li>Configure OIDC Client</li> <li>Configure Groups and Users</li> </ul>"},{"location":"eoepca/application-hub/#oidc-client","title":"OIDC Client","text":"<p>The client that is created by the script <code>./deploy/bin/register-client</code> (as per above) needs to be manually adjusted using the Web UI of the Login Service\u2026</p> <ul> <li>In a browser, navigate to the Login Service (Gluu) - https://auth.192-168-49-2.nip.io/ - and login as the <code>admin</code> user</li> <li>Open <code>OpenID Connect -&gt; Clients</code> and search for the client created earlier - <code>Application Hub</code></li> <li>Fix the setting <code>Authentication method for the Token Endpoint</code>  for the <code>ApplicationHub</code> - <code>client_secret_post</code> -&gt; <code>client_secret_basic</code></li> <li>Save the update</li> </ul>"},{"location":"eoepca/application-hub/#groups-and-users","title":"Groups and Users","text":"<p>The default helm chart has some built-in application launchers whose assignments to example users (eric and bob) assume the existence of some JupyterHub groups - which must be replicated to exploit this configuration.</p> <ul> <li>In a browser, navigate to the Application Hub - https://applicationhub.192-168-49-2.nip.io/</li> <li>Login as the user eric (or bob) for admin access</li> <li>Select the <code>Admin</code> menu (top of page)</li> <li>Add groups <code>group-1</code>, <code>group-2</code>, <code>group-3</code> to ApplicationHub, and add users <code>eric</code>, <code>bob</code> to these groups</li> </ul>"},{"location":"eoepca/application-hub/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the Application Hub can be found at:</p> <ul> <li>Helm Chart</li> </ul>"},{"location":"eoepca/container-registry/","title":"Container Registry","text":"<p>To support the development (ref. Application Hub) and deployment/execution (ref. ADES) of user-defined applications, we deploy a container registry to host container images. This is provied by a deployment of the Harbor artefact repository.</p>"},{"location":"eoepca/container-registry/#helm-chart","title":"Helm Chart","text":"<p>Harbor is deployed via the <code>harbor</code> helm chart from the Harbor Helm Chart Repository.</p> <pre><code>helm install --version 1.7.3 --values harbor-values.yaml \\\n   --repo https://helm.goharbor.io \\\n  harbor harbor\n</code></pre>"},{"location":"eoepca/container-registry/#values","title":"Values","text":"<p>The chart is configured via values that are fully documented on the Harbor website.</p> <p>Example\u2026</p> <pre><code>expose:\n  ingress:\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      cert-manager.io/cluster-issuer: letsencrypt-production\n      nginx.ingress.kubernetes.io/proxy-read-timeout: '600'\n\n      # from chart:\n      ingress.kubernetes.io/ssl-redirect: letsencrypt-production\n      ingress.kubernetes.io/proxy-body-size: \"0\"\n      nginx.ingress.kubernetes.io/ssl-redirect: letsencrypt-production\n      nginx.ingress.kubernetes.io/proxy-body-size: \"0\"\n\n    hosts:\n      core: harbor.192-168-49-2.nip.io\n    tls:\n      enabled: \"true\"\n      certSource: secret\n      secret:\n        secretName: \"harbor-tls\"\n\npersistence:\n  persistentVolumeClaim:\n    registry:\n      storageClass: standard\n    chartmuseum:\n      storageClass: standard\n    jobservice:\n      storageClass: standard\n    database:\n      storageClass: standard\n    redis:\n      storageClass: standard\n    trivy:\n      storageClass: standard\n\nexternalURL: https://harbor.192-168-49-2.nip.io\n# initial password for logging in with user \"admin\"\nharborAdminPassword: \"changeme\"\n\nchartmuseum:\n  enabled: false\ntrivy:\n  enabled: false\nnotary:\n  enabled: false\n</code></pre> <p>Note</p> <ul> <li>We specify use of \u2018valid\u2019 certificates from Letsencrypt \u2018production\u2019. The Workspace API, which calls the Harbor API, expects valid certificates and will thus fail if presented with TLS certificates that fail validation.</li> <li>The <code>letsencrypt-production</code> Cluster Issuer relies upon the deployment being accessible from the public internet via the <code>expose.ingress.hosts.core</code> DNS name. If this is not the case, e.g. for a local minikube deployment in which this is unlikely to be so. In this case the TLS will fall-back to the self-signed certificate built-in to the nginx ingress controller. The Workspace API will not like this.</li> </ul>"},{"location":"eoepca/container-registry/#container-registry-usage","title":"Container Registry Usage","text":"<p>After deployemnt Harbor is accessible via its web interface at <code>https://harbor.&lt;domain&gt;/</code>e.g. https://harbor.192-168-49-2.nip.io/.</p> <p>Login as the admin user with the password specified in the helm values.</p>"},{"location":"eoepca/container-registry/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the Container Registry can be found at:</p> <ul> <li>Web Site</li> <li>Helm Chart Repository</li> <li>Helm Chart Description</li> <li>Harbor Documentation</li> </ul>"},{"location":"eoepca/data-access/","title":"Data Access","text":"<p>The Data Access provides standards-based services for access to platform hosted data - including OGC WMS/WMTS for visualisation, and OGC WCS for data retrieval. This component also includes Harvester and Registrar services to discover/watch the existing data holding of the infrastructure data layer and populate/maintain the data access and resource catalogue services accordingly.</p>"},{"location":"eoepca/data-access/#helm-chart","title":"Helm Chart","text":"<p>The Data Access is deployed via the <code>data-access</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are supplied with the instantiation of the helm release. The EOEPCA <code>data-access</code> chart provides a thin wrapper around the EOX View Server (<code>vs</code>) helm chart. The documentation for the View Server can be found here:</p> <ul> <li>User Guide: https://vs.pages.eox.at/documentation/user/main/</li> <li>Operator Guide: https://vs.pages.eox.at/documentation/operator/main/</li> </ul> <pre><code>helm install --version 1.3.1 --values data-access-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  data-access data-access\n</code></pre>"},{"location":"eoepca/data-access/#values","title":"Values","text":"<p>The Data Access supports many values to configure the service. These are documented in full in the View Server - Operator Guide Configuration page.</p>"},{"location":"eoepca/data-access/#core-configuration","title":"Core Configuration","text":"<p>Typically, values for the following attributes may be specified to override the chart defaults:</p> <ul> <li>The fully-qualified public URL for the service, ref. (<code>global.ingress.hosts.host[0]</code>)</li> <li>Metadata describing the service instance</li> <li>Dynamic provisioning StorageClass for persistence</li> <li>Persistent Volume Claims for <code>database</code> and <code>redis</code> components</li> <li>Object storage details for <code>data</code> and <code>cache</code></li> <li>Container images for <code>renderer</code> and <code>registrar</code></li> <li>(optional) Specification of Ingress for reverse-proxy access to the service Note that this is only required in the case that the Data Access will not be protected by the <code>resource-guard</code> component - ref. Resource Protection. Otherwise the ingress will be handled by the <code>resource-guard</code> - use <code>ingress.enabled: false</code>.</li> </ul> <pre><code>global:\n  env:\n    REGISTRAR_REPLACE: \"true\"\n    CPL_VSIL_CURL_ALLOWED_EXTENSIONS: .TIF,.tif,.xml,.jp2,.jpg,.jpeg\n    AWS_HTTPS: \"FALSE\"\n    startup_scripts:\n      - /registrar_pycsw/registrar_pycsw/initialize-collections.sh\n  ingress:\n    enabled: true\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      kubernetes.io/tls-acme: \"true\"\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      cert-manager.io/cluster-issuer: letsencrypt-production\n    hosts:\n      - host: data-access.192-168-49-2.nip.io\n    tls:\n      - hosts:\n          - data-access.192-168-49-2.nip.io\n        secretName: data-access-tls\n  storage:\n    data:\n      data:\n        type: S3\n        endpoint_url: http://data.cloudferro.com\n        access_key_id: access\n        secret_access_key: access\n        region_name: RegionOne\n        validate_bucket_name: false\n    cache:\n      type: S3\n      endpoint_url: \"http://minio.192-168-49-2.nip.io\"\n      host: \"minio.192-168-49-2.nip.io\"\n      access_key_id: xxx\n      secret_access_key: xxx\n      region: us-east-1\n      bucket: cache-bucket\n  metadata:\n    title: EOEPCA Data Access Service developed by EOX\n    abstract: EOEPCA Data Access Service developed by EOX\n    header: \"EOEPCA Data Access View Server (VS) Client powered by &lt;a href=\\\"//eox.at\\\"&gt;&lt;img src=\\\"//eox.at/wp-content/uploads/2017/09/EOX_Logo.svg\\\" alt=\\\"EOX\\\" style=\\\"height:25px;margin-left:10px\\\"/&gt;&lt;/a&gt;\"\n    url: https://data-access.192-168-49-2.nip.io/ows\n  layers:\n    # see section 'Data-layer Configuration'\n  collections:\n    # see section 'Data-layer Configuration'\n  productTypes:\n    # see section 'Data-layer Configuration'\nvs:\n  renderer:\n    replicaCount: 4\n    ingress:\n      enabled: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 300Mi\n      limits:\n        cpu: 1.5\n        memory: 3Gi\n  registrar:\n    replicaCount: 1\n    config:\n      # see section 'Registrar Routes Configuration'\n    resources:\n      requests:\n        cpu: 100m\n        memory: 100Mi\n  harvester:\n    # see section 'Harvester Configuration'\n    replicaCount: 1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 100Mi\n  client:\n    replicaCount: 1\n    ingress:\n      enabled: false\n  redis:\n    master:\n      persistence:\n        enabled: true\n        storageClass: standard\n  ingestor:\n    replicaCount: 0\n    ingress:\n      enabled: false\n  preprocessor:\n    replicaCount: 0\n  cache:\n    ingress:\n      enabled: false\n  scheduler:\n    resources:\n      requests:\n        cpu: 100m\n        memory: 100Mi\n</code></pre> <p>Note</p> <p>The <code>resources:</code> above have been limited for the benefit of a minikube deployment. For a production deployment the values should be tuned (upwards) according to operational needs.</p>"},{"location":"eoepca/data-access/#registrar-routes-configuration","title":"Registrar Routes Configuration","text":"<p>The Data Access <code>registrar</code> component supports a number of different resource types. For each a dedicated \u2018backend\u2019 is configured to handle the specific registration of the resource type\u2026</p> <pre><code>vs:\n  registrar:\n    config:\n      #--------------\n      # Default route\n      #--------------\n      disableDefaultRoute: false\n      # Additional backends for the default route\n      defaultBackends:\n        - path: registrar_pycsw.backend.ItemBackend\n          kwargs:\n            repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n            ows_url: https://data-access.192-168-49-2.nip.io/ows\n      defaultSuccessQueue: seed_queue\n      #----------------\n      # Specific routes\n      #----------------\n      routes:\n        collections:\n          path: registrar.route.stac.CollectionRoute\n          queue: register_collection_queue\n          replace: true\n          backends:\n            - path: registrar_pycsw.backend.CollectionBackend\n              kwargs:\n                repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n        ades:\n          path: registrar.route.json.JSONRoute\n          queue: register_ades_queue\n          replace: true\n          backends:\n            - path: registrar_pycsw.backend.ADESBackend\n              kwargs:\n                repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n        application:\n          path: registrar.route.json.JSONRoute\n          queue: register_application_queue\n          replace: true\n          backends:\n            - path: registrar_pycsw.backend.CWLBackend\n              kwargs:\n                repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n        catalogue:\n          path: registrar.route.json.JSONRoute\n          queue: register_catalogue_queue\n          replace: true\n          backends:\n            - path: registrar_pycsw.backend.CatalogueBackend\n              kwargs:\n                repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n        json:\n          path: registrar.route.json.JSONRoute\n          queue: register_json_queue\n          replace: true\n          backends:\n            - path: registrar_pycsw.backend.JSONBackend\n              kwargs:\n                repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n        xml:\n          path: registrar.route.json.JSONRoute\n          queue: register_xml_queue\n          replace: true\n          backends:\n            - path: registrar_pycsw.backend.XMLBackend\n              kwargs:\n                repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n</code></pre>"},{"location":"eoepca/data-access/#data-layer-configuration","title":"Data-layer Configuration","text":"<p>Configuration of the service data-layer - as described in the View Server Operator Guide. </p> <p>The data-access service data handling is configured by definition of <code>productTypes</code>, <code>collections</code> and <code>layers</code>\u2026</p> <ul> <li><code>productTypes</code> - Product Types   Identify the underlying file assets as WCS coverages and their visual representation</li> <li><code>collections</code> - Data Collections   Provides groupings into which products are organised</li> <li><code>layers</code> - Layers   Specifies the hoe the product visual representations are exposed through the WMS service</li> </ul> <p>For more information, see the worked example in section Data Specification for the example CREODIAS deployment.</p>"},{"location":"eoepca/data-access/#harvester","title":"Harvester","text":"<p>The Data Access service includes a Harvester component. The following subsections describe its configuration and usage.</p>"},{"location":"eoepca/data-access/#harvester-helm-configuration","title":"Harvester Helm Configuration","text":"<p>The Harvester can be configured through the helm chart values\u2026</p> <pre><code>vs:\n  harvester:\n    replicaCount: 1\n    config:\n      redis:\n        host: data-access-redis-master\n        port: 6379\n      harvesters:\n        - name: Creodias-Opensearch\n          resource:\n            url: https://datahub.creodias.eu/resto/api/collections/Sentinel2/describe.xml\n            type: OpenSearch\n            format_config:\n              type: 'application/json'\n              property_mapping:\n                start_datetime: 'startDate'\n                end_datetime: 'completionDate'\n                productIdentifier: 'productIdentifier'\n            query:\n              time:\n                property: sensed\n                begin: 2019-09-10T00:00:00Z\n                end: 2019-09-11T00:00:00Z\n              collection: null\n              bbox: 14.9,47.7,16.4,48.7\n          filter: {}\n          postprocess:\n            - type: harvester_eoepca.postprocess.CREODIASOpenSearchSentinel2Postprocessor\n          queue: register\n        - name: Creodias-Opensearch-Sentinel1\n          resource:\n            url: https://datahub.creodias.eu/resto/api/collections/Sentinel1/describe.xml\n            type: OpenSearch\n            format_config:\n              type: 'application/json'\n              property_mapping:\n                start_datetime: 'startDate'\n                end_datetime: 'completionDate'\n                productIdentifier: 'productIdentifier'\n            query:\n              time:\n                property: sensed\n                begin: 2019-09-10T00:00:00Z\n                end: 2019-09-11T00:00:00Z\n              collection: null\n              bbox: 14.9,47.7,16.4,48.7\n              extra_params:\n                productType: GRD-COG\n          filter: {}\n          postprocess:\n            - type: harvester_eoepca.postprocess.CREODIASOpenSearchSentinel1Postprocessor\n          queue: register\n</code></pre> <p>The <code>harvester.config.harvesters</code> list defines a set of pre-defined harvesters which can be invoked in a later stage. The name property must be unique for each harvester and must be unique among all harvesters in the list. Each harvester is associated with a <code>resource</code>, an optional <code>filter</code> or <code>postprocess</code> function, and a <code>queue</code>.</p> <p>The <code>resource</code> defines where each item is harvested from. This can be a file system, a search service, catalog file or something similar. The example above defines a connection to an OpenSearch service on CREODIAS, with associated default query parameters and a format configuration.</p> <p>The <code>filter</code> allows to filter elements within the harvester, when the resource does not provide a specific filter. This filter can be supplied using CQL2-JSON.</p> <p>The <code>postprocess</code> can adjust the harvested results. In this example the harvested items are not complete, and additional metadata must be retrieved from an object storage.</p> <p>The <code>queue</code> defines where harvested items will be pushed into. Usually this is a registration queue, where the registrar will pick up and start registration according to its configuration.</p>"},{"location":"eoepca/data-access/#starting-the-harvester","title":"Starting the Harvester","text":"<p>The harvester can either do one-off harvests via the CLI or listen on a redis queue to run consecutive harvests whenever a harvesting request is received on that queue.</p>"},{"location":"eoepca/data-access/#one-off-harvests-via-the-cli","title":"One-off harvests via the CLI","text":"<p>In order to start a harvest from the CLI, the operator first needs to connect to the kubernetes pod of the harvester. Within that pod, the harvest can be executed like this\u2026 <pre><code>python3 -m harvester harvest --config-file /config-run.yaml --host data-access-redis-master --port 6379 Creodias-Opensearch\n</code></pre></p> <p>This will invoke the Creodias-Opensearch harvester with default arguments. When some values are to be overridden, the \u2013values switch can be used to pass override values. These values must be a JSON string. The following example adjusts the begin and end times of the query parameters\u2026 <pre><code>python3 -m harvester harvest --config-file /config-run.yaml --host data-access-redis-master --port 6379 Creodias-Opensearch --values '{\"resource\": {\"query\": {\"time\": {\"begin\": \"2020-09-10T00:00:00Z\", \"end\": \"2020-09-11T00:00:00Z\"}}}}'\n</code></pre></p>"},{"location":"eoepca/data-access/#harvests-via-the-harvest-daemon","title":"Harvests via the harvest daemon","text":"<p>The harvester pod runs a service listening on a redis queue. When a message is read from the queue, it will be read as a JSON string, expecting an object with at least a <code>name</code> property. Optionally, it can also have a <code>values</code> property, working in the same way as with CLI <code>--values</code>.</p> <p>To send a harvesting request via the redis queue, it is necessary to connect to the redis pod and execute the redis-cli there. Then the following command can be used to achieve the same result as above with CLI harvesting\u2026 <pre><code>redis-cli LPUSH '{\"name\": \"Creodias-Opensearch\", \"values\": {\"resource\": {\"query\": {\"time\": {\"begin\": \"2020-09-10T00:00:00Z\", \"end\": \"2020-09-11T00:00:00Z\"}}}}}'\n</code></pre></p>"},{"location":"eoepca/data-access/#results-of-the-harvesting","title":"Results of the harvesting","text":"<p>The harvester produces a continous stream of STAC Items which are sent down via the configured queue. It is possible that the harvested metadata is not sufficient to create a fully functional STAC Item. In this case the postprocess must transform this intermediate item to a valid STAC Item. In our example, the postprocessor looks up the Sentinel-2 product file referenced by the product identifier which is then accessed on the object storage. From the stored metadata files, the STAC Items to be sent is created.</p>"},{"location":"eoepca/data-access/#storage","title":"Storage","text":"<p>Specification of PVCs and access to object storage.</p>"},{"location":"eoepca/data-access/#persistent-volume-claims","title":"Persistent Volume Claims","text":"<p>The PVCs specified in the helm chart values must be created.</p>"},{"location":"eoepca/data-access/#pvc-for-database","title":"PVC for Database","text":"<pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: data-access-db\n  namespace: rm\n  labels:\n    k8s-app: data-access\n    name: data-access\nspec:\n  storageClassName: standard\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 100Gi\n</code></pre>"},{"location":"eoepca/data-access/#pvc-for-redis","title":"PVC for Redis","text":"<pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: data-access-redis\n  namespace: rm\n  labels:\n    k8s-app: data-access\n    name: data-access\nspec:\n  storageClassName: standard\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>"},{"location":"eoepca/data-access/#object-storage","title":"Object Storage","text":"<p>The helm chart values expect specification of object storage details for:</p> <ul> <li><code>data</code>: to access the EO data of the underlying infrastructure</li> <li><code>cache</code>: a dedicated object storage bucket is used to support the cache function of the data access services</li> </ul>"},{"location":"eoepca/data-access/#platform-eo-data","title":"Platform EO Data","text":"<p>Specifies the details for the infrastructure object storage that provides direct access to the EO product files.</p> <p>For example, the CREODIAS metadata catalogue provides references to product files in their <code>eodata</code> object storage - the access details for which are configured in the data access services:</p> <pre><code>global:\n  storage:\n    data:\n      data:\n        type: S3\n        endpoint_url: http://data.cloudferro.com\n        access_key_id: access\n        secret_access_key: access\n        region_name: RegionOne\n        validate_bucket_name: false\n</code></pre>"},{"location":"eoepca/data-access/#data-access-cache","title":"Data Access Cache","text":"<p>The Data Access services maintain a cache, which relies on the usage of a dedicate object storage bucket for data persistence. This bucket must be created (manual step) and its access details configured in the data access services. Example based upon CREODIAS:</p> <pre><code>global:\n  storage:\n    cache:\n      type: S3\n      endpoint_url: \"https://cf2.cloudferro.com:8080/cache-bucket\"\n      host: \"cf2.cloudferro.com:8080\"\n      access_key_id: xxx\n      secret_access_key: xxx\n      region: RegionOne\n      bucket: cache-bucket\n</code></pre> <p>\u2026where <code>xxx</code> must be replaced with the bucket credentials.</p>"},{"location":"eoepca/data-access/#protection","title":"Protection","text":"<p>As described in section Resource Protection, the <code>resource-guard</code> component can be inserted into the request path of the Data Access service to provide access authorization decisions.</p> <pre><code>helm install --version 1.3.1 --values data-access-guard-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  data-access-guard resource-guard\n</code></pre> <p>The <code>resource-guard</code> must be configured with the values applicable to the Data Access for the Policy Enforcement Point (<code>pep-engine</code>) and the UMA User Agent (<code>uma-user-agent</code>)\u2026</p> <p>Example <code>data-access-guard-values.yaml</code>\u2026</p> <pre><code>#---------------------------------------------------------------------------\n# Global values\n#---------------------------------------------------------------------------\nglobal:\n  context: data-access\n  domain: 192-168-49-2.nip.io\n  nginxIp: 192.168.49.2\n  certManager:\n    clusterIssuer: letsencrypt-production\n#---------------------------------------------------------------------------\n# PEP values\n#---------------------------------------------------------------------------\npep-engine:\n  configMap:\n    asHostname: auth\n    pdpHostname: auth\n  volumeClaim:\n    name: eoepca-resman-pvc\n    create: false\n#---------------------------------------------------------------------------\n# UMA User Agent values\n#---------------------------------------------------------------------------\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: data-access\n        paths:\n          - path: /(ows.*)\n            service:\n              name: data-access-renderer\n              port: 80\n          - path: /(opensearch.*)\n            service:\n              name: data-access-renderer\n              port: 80\n          - path: /(coverages/metadata.*)\n            service:\n              name: data-access-renderer\n              port: 80\n          - path: /(admin.*)\n            service:\n              name: data-access-renderer\n              port: 80\n          - path: /cache/(.*)\n            service:\n              name: data-access-cache\n              port: 80\n          - path: /(.*)\n            service:\n              name: data-access-client\n              port: 80\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n  client:\n    credentialsSecretName: \"resman-client\"\n  logging:\n    level: \"info\"\n  unauthorizedResponse: 'Bearer realm=\"https://portal.192-168-49-2.nip.io/oidc/authenticate/\"'\n  openAccess: false\n  insecureTlsSkipVerify: true\n</code></pre> <p>Note</p> <ul> <li>TLS is enabled by the specification of <code>certManager.clusterIssuer</code></li> <li>The <code>letsencrypt</code> Cluster Issuer relies upon the deployment being accessible from the public internet via the <code>global.domain</code> DNS name. If this is not the case, e.g. for a local minikube deployment in which this is unlikely to be so. In this case the TLS will fall-back to the self-signed certificate built-in to the nginx ingress controller</li> <li><code>insecureTlsSkipVerify</code> may be required in the case that good TLS certificates cannot be established, e.g. if letsencrypt cannot be used for a local deployment. Otherwise the certificates offered by login-service Authorization Server will fail validation in the Resource Guard.</li> <li><code>customDefaultResources</code> can be specified to apply initial protection to the endpoint</li> </ul>"},{"location":"eoepca/data-access/#client-secret","title":"Client Secret","text":"<p>The Resource Guard requires confidential client credentials to be configured through the file <code>client.yaml</code>, delivered via a kubernetes secret..</p> <p>Example <code>client.yaml</code>\u2026</p> <pre><code>client-id: a98ba66e-e876-46e1-8619-5e130a38d1a4\nclient-secret: 73914cfc-c7dd-4b54-8807-ce17c3645558\n</code></pre> <p>Example <code>Secret</code>\u2026</p> <pre><code>kubectl -n rm create secret generic resman-client \\\n  --from-file=client.yaml \\\n  --dry-run=client -o yaml \\\n  &gt; resman-client-secret.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: resman-client\n  namespace: rm\ndata:\n  client.yaml: Y2xpZW50LWlkOiBhOThiYTY2ZS1lODc2LTQ2ZTEtODYxOS01ZTEzMGEzOGQxYTQKY2xpZW50LXNlY3JldDogNzM5MTRjZmMtYzdkZC00YjU0LTg4MDctY2UxN2MzNjQ1NTU4\n</code></pre> <p>The client credentials are obtained by registration of a client at the login service web interface - e.g. https://auth.192-168-49-2.nip.io. In addition there is a helper script that can be used to create a basic client and obtain the credentials, as described in section Resource Protection\u2026 <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io \"Resource Guard\" | tee client.yaml\n</code></pre></p>"},{"location":"eoepca/data-access/#data-access-usage","title":"Data Access Usage","text":""},{"location":"eoepca/data-access/#default-harvesting","title":"Default Harvesting","text":"<p>At deployment time the <code>harvester</code> helm values include configuration that populates a default harvester configuration, that is prepared in the file <code>/config.yaml</code> in the <code>harvester</code> pod.</p> <p>The Data Access and Resource Catalogue services are configured to properly interpret harvested data via these values specified in the instantiation of the helm release. See section Data-layer Configuration.</p> <p>The harvesting of data can be triggered (post deployment), in accordance with this default configuration, by connecting to the <code>rm/harvester</code> service and executing the command\u2026 <pre><code>python3 -m harvester harvest --config-file /config-run.yaml --host data-access-redis-master --port 6379 Creodias-Opensearch\n</code></pre></p>"},{"location":"eoepca/data-access/#ad-hoc-harvesting","title":"Ad-hoc Harvesting","text":"<p>Ad-hoc harvesting can be invoked by provision of a suitable <code>config.yaml</code> into the harvester pod, which can then be invoked as shown above for the default harvester configuration established at deploy time.</p> <p>The helper script <code>./deploy/bin/harvest</code> faciltates this\u2026</p> <pre><code>./deploy/bin/harvest &lt;path-to-config-file&gt;\n</code></pre> <p>See directory <code>./deploy/samples/harvester/</code> that contains some sample harvesting configuration files. For example\u2026</p> <pre><code>./deploy/bin/harvest ./deploy/samples/harvester/config-Sentinel2-2019.09.10.yaml\n</code></pre>"},{"location":"eoepca/data-access/#registration-of-collections","title":"Registration of Collections","text":"<p>The helper script <code>./deploy/bin/register-collection</code> is provided to faciltate the registration of collections that are specfied in STAC Collection format.</p> <pre><code>./deploy/bin/register-collection &lt;path-to-stac-collection-file&gt;\n</code></pre> <p>See directory <code>./deploy/samples/collections/</code> that contains some same STAC Collection files. For example\u2026</p> <pre><code>./deploy/bin/register-collection ./deploy/samples/collections/S2MSI2A.json\n</code></pre>"},{"location":"eoepca/data-access/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the Data Access can be found at:</p> <ul> <li>Helm Chart</li> <li>Documentation:<ul> <li>User Guide</li> <li>Operator Guide</li> </ul> </li> <li>Git Repository</li> </ul>"},{"location":"eoepca/iam-overview/","title":"IAM Overview","text":"<p>This guide includes two approaches for Identity &amp; Access Management:</p> <ul> <li>Keycloak Solution (NEW)</li> <li>Gluu Solution (deprecated)</li> </ul> <p>Until now, our IAM solution has been based solely upon Gluu. In the course of the project Keycloak has emerged as a preferred solution across EO platforms. Thus, we have introduced an IAM approach based upon Keycloak, whilst retaining the Gluu-based approach which will be deprecated.</p>"},{"location":"eoepca/keycloak/","title":"Keycloak","text":"<p>To cover:</p> <ul> <li>Keycloak</li> <li>Gatekeeper</li> <li>Client Creation</li> <li>Resource Protection</li> </ul>"},{"location":"eoepca/login-service/","title":"Login Service","text":"<p>The Login Service provides the platform Authorization Server for authenticated user identity and request authorization.</p>"},{"location":"eoepca/login-service/#helm-chart","title":"Helm Chart","text":"<p>The Login Service is deployed via the <code>login-service</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are fully documented in the README for the <code>login-service</code> chart.</p> <pre><code>helm install --version 1.2.8 --values login-service-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  login-service login-service\n</code></pre>"},{"location":"eoepca/login-service/#values","title":"Values","text":"<p>At minimum, values for the following attributes should be specified:</p> <ul> <li>Public hostname of the Authorization Server, e.g. <code>auth.192-168-49-2.nip.io</code></li> <li>IP Address of the public facing reverse proxy (Nginx Ingress Controller), e.g. <code>192.168.49.2</code></li> <li>Kubernetes <code>namespace</code> for the login-service components</li> <li>Initial password for the admin user Note that the password must meet the complexity: at least 6 characters and include one uppercase letter, one lowercase letter, one digit, and one special character</li> <li>Name of Persistent Volume Claim for <code>login-service</code> persistence, e.g. <code>eoepca-userman-pvc</code> The boolen value <code>volumeClaim.create</code> can be used for the PVC to be created by the helm release. This creates a volume of type <code>host-path</code> and, hence, is only useful for single-node development usage.</li> <li>TLS Certificate Provider, e.g. <code>letsencrypt-production</code></li> </ul> <p>Example <code>login-service-values.yaml</code>\u2026 <pre><code>global:\n  domain: auth.192-168-49-2.nip.io\n  nginxIp: 192.168.49.2\n  namespace: um\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\nconfig:\n  domain: auth.192-168-49-2.nip.io\n  adminPass: Chang3me!\n  ldapPass: Chang3me!\n  volumeClaim:\n    name: eoepca-userman-pvc\nopendj:\n  volumeClaim:\n    name: eoepca-userman-pvc\n  resources:\n    requests:\n      cpu: 100m\n      memory: 300Mi\noxauth:\n  volumeClaim:\n    name: eoepca-userman-pvc\n  resources:\n    requests:\n      cpu: 100m\n      memory: 1000Mi\noxtrust:\n  volumeClaim:\n    name: eoepca-userman-pvc\n  resources: \n    requests:\n      cpu: 100m\n      memory: 1500Mi\noxpassport:\n  resources:\n    requests:\n      cpu: 100m\n      memory: 100Mi\nnginx:\n  ingress:\n    annotations:\n      cert-manager.io/cluster-issuer: letsencrypt-production\n    hosts:\n      - auth.192-168-49-2.nip.io\n    tls:\n      - hosts:\n          - auth.192-168-49-2.nip.io\n        secretName: login-service-tls\n</code></pre></p> <p>Note</p> <p>The <code>resources:</code> above have been limited for the benefit of a minikube deployment. For a production deployment the values should be tuned (upwards) according to operational needs.</p>"},{"location":"eoepca/login-service/#post-deployment-manual-steps","title":"Post-deployment Manual Steps","text":"<p>The deployment of the Login Service has been designed, as far as possible, to automate the configuration. However, there remain some steps that must be performed manually after the scripted deployment has completed\u2026</p> <ul> <li>Configure <code>UMA Resource Lifetime</code></li> <li>Configure <code>Operator</code> user</li> </ul>"},{"location":"eoepca/login-service/#uma-resource-lifetime","title":"UMA Resource Lifetime","text":"<p>The Login Service maintains a background service that \u2018cleans\u2019 UMA resources that are older than aa certain age - by default 30 days (<code>2592000</code> secs). This lifetime does not fit the approach we are adopting, and so we must update this lifetime value to avoid the unexpected removal of UMA resources that would cause unexpected failures in policy enforcement.</p> <ul> <li>In a browser, navigate to the Login Service (Gluu) - <code>https://auth.192-168-49-2.nip.io/</code> - and login as the <code>admin</code> user</li> <li>Open <code>Configuration -&gt; JSON Configuration -&gt; OxAuth Configuration</code></li> <li>Search for the setting <code>umaResourceLifetime</code></li> <li>Update the values of <code>umaResourceLifetime</code> to <code>2147483647</code></li> <li>Select to <code>Save Configuration</code></li> <li>Restart the <code>oxauth</code> deployment\u2026 <pre><code>kubectl -n um rollout restart deploy/login-service-oxauth\n</code></pre></li> </ul>"},{"location":"eoepca/login-service/#configure-operator-user","title":"Configure <code>Operator</code> user","text":"<p>The default resource protection establishes policy in which \u2018operator\u2019 privilege is required for some services, such as the Workspace API. Thus, we need to configure a user with this privilege. For convenience we add this attribute to the built-in <code>admin</code> user - but alternatively you may choose to create a new user for this role.</p> <ul> <li>In a browser, navigate to the Login Service (Gluu) - <code>https://auth.192-168-49-2.nip.io/</code> - and login as the <code>admin</code> user</li> <li>Select <code>Users -&gt; Manage People</code> and search for user <code>admin</code></li> <li>For user <code>admin</code> select <code>Available User Claims -&gt; gluuCustomPerson</code></li> <li>Select <code>Is Operator</code> and ensure the value is set <code>True</code></li> <li>Select <code>Update</code> to confirm</li> </ul>"},{"location":"eoepca/login-service/#login-service-usage","title":"Login Service Usage","text":"<p>Once the deployment has been completed successfully, the Login Service is accessed at the endpoint <code>https://auth.&lt;domain&gt;/</code>, configured by your domain - e.g. https://auth.192-168-49-2.nip.io/.</p> <p>Login as the <code>admin</code> user with the credentials configured in the helm values - ref. <code>adminPass</code> / <code>ldapPass</code>.</p> <p>Typical first actions to undertake through the Gluu web interface include creation of users and clients.</p>"},{"location":"eoepca/login-service/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the Login Service can be found at:</p> <ul> <li>Helm Chart</li> <li>Wiki</li> <li>GitHub Repository</li> </ul>"},{"location":"eoepca/pdp/","title":"Policy Decision Point","text":"<p>The Policy Decision Point (PDP) provides the platform policy database and associated service for access policy decision requests.</p>"},{"location":"eoepca/pdp/#helm-chart","title":"Helm Chart","text":"<p>The PDP is deployed via the <code>pdp-engine</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are fully documented in the README for the <code>pdp-engine</code> chart.</p> <pre><code>helm install --version 1.1.12 --values pdp-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  pdp pdp-engine\n</code></pre>"},{"location":"eoepca/pdp/#values","title":"Values","text":"<p>At minimum, values for the following attributes should be specified:</p> <ul> <li>Public hostname of the Authorization Server, e.g. <code>auth.192-168-49-2.nip.io</code></li> <li>IP Address of the public facing reverse proxy (Nginx Ingress Controller), e.g. <code>192.168.49.2</code></li> <li>Name of Persistent Volume Claim for <code>pdp-engine</code> persistence, e.g. <code>eoepca-userman-pvc</code> The boolen value <code>volumeClaim.create</code> can be used for the PVC to be created by the helm release. This creates a volume of type <code>host-path</code> and, hence, is only useful for single-node development usage.</li> </ul> <p>Example <code>pdp-values.yaml</code>\u2026 <pre><code>global:\n  nginxIp: 192.168.49.2\n  domain: auth.192-168-49-2.nip.io\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\n</code></pre></p>"},{"location":"eoepca/pdp/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the PDP can be found at:</p> <ul> <li>Helm Chart</li> <li>Wiki</li> <li>GitHub Repository</li> </ul>"},{"location":"eoepca/persistence/","title":"Persistence","text":"<p>The EOEPCA building-blocks rely upon Kubernetes <code>Persistent Volumes</code> for their component persistence. Components integrate with the storage provided in the cluster by means of configurable <code>Persistent Volume Claims</code> and/or dynamic <code>Storage Class</code> that are specfied as values at time of deployment. Some components require storage of type  <code>ReadWriteMany</code> - which, for a multi-node cluster, implies a network-based storage solution.</p>"},{"location":"eoepca/persistence/#readwritemany-storage","title":"ReadWriteMany Storage","text":"<p>For the EOEPCA development deployment, an NFS server has been established to provide the persistence layer for <code>ReadWriteMany</code> storage.</p>"},{"location":"eoepca/persistence/#pre-defined-persistent-volume-claims","title":"Pre-defined Persistent Volume Claims","text":"<p>The EOEPCA development deployment establishes the following pre-defined Persistent Volume Claims, to provide a simple storage architecture that is organised around the \u2018domain areas\u2019 into which the Reference Implementation is split.</p> <ul> <li>Resource Managment (<code>resman</code>) - <code>persistentvolumeclaim/eoepca-resman-pvc</code></li> <li>Processing &amp; Chaining (<code>proc</code>) - <code>persistentvolumeclaim/eoepca-proc-pvc</code></li> <li>User Management (<code>userman</code>) - <code>persistentvolumeclaim/eoepca-userman-pvc</code></li> </ul> <p>NOTE that this is offered only as an example thay suits the approach of the development team. Each building-block has configuration through which its persistence (PV/PVC) can be configured according the needs of the deployment.</p> <p>The following Kubernetes yaml provides an example of provisioning such domain-specific PersistentVolumeClaims within the cluster - in this case using the minikube built-in storage-class <code>standard</code> for dynamic provisioning\u2026</p> <pre><code>---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: eoepca-proc-pvc\n  namespace: proc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: standard\n  resources:\n    requests:\n      storage: 5Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: eoepca-resman-pvc\n  namespace: rm\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: standard\n  resources:\n    requests:\n      storage: 5Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: eoepca-userman-pvc\n  namespace: um\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: standard\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <p>Once established, these PersistentVolumeClaims are then referenced within the deployment configurations of the building-blocks.</p>"},{"location":"eoepca/persistence/#dynamic-readwritemany-storage-provisioning","title":"Dynamic <code>ReadWriteMany</code> Storage Provisioning","text":"<p>In addition to the pre-defined PV/PVCs, the EOEPCA Reference Implementation also defines NFS-based storage classes for dynamic storage provisioning:</p> <ul> <li><code>managed-nfs-storage</code> With a <code>Reclaim Policy</code> of <code>Delete</code>.</li> <li><code>managed-nfs-storage-retain</code> With a <code>Reclaim Policy</code> of <code>Retain</code>.</li> </ul> <p>The building-blocks simply reference the required <code>Storage Class</code> in their volume specifications, to receive a <code>Persistent Volume Claim</code> that is dynamically provisioned at deployment time.</p> <p>This is acheived through the <code>nfs-provisioner</code> helm chart, with the following typical configurations\u2026</p> <p>Reclaim Policy <code>Delete</code>\u2026 <pre><code>provisionerName: nfs-storage\nstorageClass:\n  name: managed-nfs-storage\n  create: true\n  reclaimPolicy: Delete\n  archiveOnDelete: false\n  allowVolumeExpansion: true\nnfs:\n  server: \"&lt;your-nfs-ip-address-here&gt;\"\n  path: /data/dynamic  # your NFS server path here\n</code></pre></p> <p>Reclaim Policy <code>Retain</code>\u2026 <pre><code>provisionerName: nfs-storage-retain\nstorageClass:\n  name: managed-nfs-storage-retain\n  create: true\n  reclaimPolicy: Retain\n  allowVolumeExpansion: true\nnfs:\n  server: \"&lt;your-nfs-ip-address-here&gt;\"\n  path: /data/dynamic  # your NFS server path here\n</code></pre></p>"},{"location":"eoepca/persistence/#clustered-storage-solutions","title":"Clustered Storage Solutions","text":"<p>Clustered storage approaches offer an alternative to NFS. Clustered Storage provides a network-attached storage through a set of commodity hosts whose storage is aggregated to form a distributed file-system. Capacity is scaled by adding additional nodes or adding additional storage to the existing nodes. In the context of a multi-node Kubernetes cluster, then it is typical that the same commodity nodes provide both the cluster members and storage resources, i.e. the clustered storage is spread across the Kubernetes worker nodes.</p> <p>Candidate clustered storage solutions include:</p> <ul> <li>GlusterFS   GlusterFS is deployed as an operating system service across each node participating in the storage solution. Thus, with GlusterFS, the distributed storage nodes do not need to be one-and-the-same with the compute (cluster) nodes \u2013 although this may preferably be the case.</li> <li>Longhorn   Longhorn offers a solution that is similar to that of GlusterFS, except that Longhorn is \u2018cloud-native\u2019 in that its service layer deploys within the Kubernetes cluster itself. Thus, the storage nodes are also the cluster compute nodes by design.</li> </ul> <p>All things being equal, Longhorn is recommended as the best approach for Kubernetes clusters.</p>"},{"location":"eoepca/persistence/#local-cluster-storage","title":"Local Cluster Storage","text":"<p>For the purposes of the EOEPCA deployment, the default Storage Class included with the local Kubernetes distribution can be used for all storage concerns - e.g. <code>standard</code> for <code>minikube</code> which provides the <code>ReadWriteMany</code> persistence that is required by the ADES.</p>"},{"location":"eoepca/registration-api/","title":"Registration API","text":"<p>The Registration API provides a REST API through which resources can be registered with both the Resource Catalogue and (as applicable) with the Data Access services.</p>"},{"location":"eoepca/registration-api/#helm-chart","title":"Helm Chart","text":"<p>The Registration API is deployed via the <code>rm-registration-api</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are fully documented in the README for the <code>rm-registration-api</code> chart.</p> <pre><code>helm install --version 1.3.0 --values registration-api-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  registration-api rm-registration-api\n</code></pre>"},{"location":"eoepca/registration-api/#values","title":"Values","text":"<p>The Registration API supports many values to configure the service - as described in the Values section of the chart README.</p> <p>Typically, values for the following attributes may be specified:</p> <ul> <li>The fully-qualified public URL for the service</li> <li>(optional) Specification of Ingress for reverse-proxy access to the service Note that this is only required in the case that the Registration API will not be protected by the <code>resource-guard</code> component - ref. Resource Protection. Otherwise the ingress will be handled by the <code>resource-guard</code> - use <code>ingress.enabled: false</code>.</li> <li>Values for integration with the workspace-api and data-access services</li> </ul> <p>Example <code>registration-api-values.yaml</code>\u2026</p> <pre><code>fullnameOverride: registration-api\n# image: # {}\n  # repository: eoepca/rm-registration-api\n  # pullPolicy: Always\n  # Overrides the image tag whose default is the chart appVersion.\n  # tag: \"1.3-dev1\"\n\ningress:\n  enabled: false\n  hosts:\n    - host: registration-api-open.192-168-49-2.nip.io\n      paths: [\"/\"]\n  tls:\n    - hosts:\n        - registration-api-open.192-168-49-2.nip.io\n      secretName: registration-api-tls\n\n# some values for the workspace API\nworkspaceK8sNamespace: rm\nredisServiceName: \"data-access-redis-master\"\n</code></pre>"},{"location":"eoepca/registration-api/#protection","title":"Protection","text":"<p>As described in section Resource Protection, the <code>resource-guard</code> component can be inserted into the request path of the Registration API service to provide access authorization decisions</p> <pre><code>helm install --version 1.3.1 --values registration-api-guard-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  registration-api-guard resource-guard\n</code></pre> <p>The <code>resource-guard</code> must be configured with the values applicable to the Registration API for the Policy Enforcement Point (<code>pep-engine</code>) and the UMA User Agent (<code>uma-user-agent</code>)\u2026</p> <p>Example <code>registration-api-guard-values.yaml</code>\u2026</p> <pre><code>#---------------------------------------------------------------------------\n# Global values\n#---------------------------------------------------------------------------\nglobal:\n  context: registration-api\n  domain: 192-168-49-2.nip.io\n  nginxIp: 192.168.49.2\n  certManager:\n    clusterIssuer: letsencrypt-production\n#---------------------------------------------------------------------------\n# PEP values\n#---------------------------------------------------------------------------\npep-engine:\n  configMap:\n    asHostname: auth\n    pdpHostname: auth\n  volumeClaim:\n    name: eoepca-resman-pvc\n    create: false\n#---------------------------------------------------------------------------\n# UMA User Agent values\n#---------------------------------------------------------------------------\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: registration-api\n        paths:\n          - path: /(.*)\n            service:\n              name: registration-api\n              port: 8080\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n  client:\n    credentialsSecretName: \"resman-client\"\n  logging:\n    level: \"info\"\n  unauthorizedResponse: 'Bearer realm=\"https://portal.192-168-49-2.nip.io/oidc/authenticate/\"'\n  openAccess: true\n  insecureTlsSkipVerify: true\n</code></pre> <p>Note</p> <ul> <li>TLS is enabled by the specification of <code>certManager.clusterIssuer</code></li> <li>The <code>letsencrypt</code> Cluster Issuer relies upon the deployment being accessible from the public internet via the <code>global.domain</code> DNS name. If this is not the case, e.g. for a local minikube deployment in which this is unlikely to be so. In this case the TLS will fall-back to the self-signed certificate built-in to the nginx ingress controller</li> <li><code>insecureTlsSkipVerify</code> may be required in the case that good TLS certificates cannot be established, e.g. if letsencrypt cannot be used for a local deployment. Otherwise the certificates offered by login-service Authorization Server will fail validation in the Resource Guard.</li> <li><code>customDefaultResources</code> can be specified to apply initial protection to the endpoint</li> <li>In the example above <code>openAccess: true</code> has been specified, meaning that policy envorcement is skipped and all access is allowed</li> </ul>"},{"location":"eoepca/registration-api/#client-secret","title":"Client Secret","text":"<p>The Resource Guard requires confidential client credentials to be configured through the file <code>client.yaml</code>, delivered via a kubernetes secret..</p> <p>Example <code>client.yaml</code>\u2026</p> <pre><code>client-id: a98ba66e-e876-46e1-8619-5e130a38d1a4\nclient-secret: 73914cfc-c7dd-4b54-8807-ce17c3645558\n</code></pre> <p>Example <code>Secret</code>\u2026</p> <pre><code>kubectl -n rm create secret generic resman-client \\\n  --from-file=client.yaml \\\n  --dry-run=client -o yaml \\\n  &gt; resman-client-secret.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: resman-client\n  namespace: rm\ndata:\n  client.yaml: Y2xpZW50LWlkOiBhOThiYTY2ZS1lODc2LTQ2ZTEtODYxOS01ZTEzMGEzOGQxYTQKY2xpZW50LXNlY3JldDogNzM5MTRjZmMtYzdkZC00YjU0LTg4MDctY2UxN2MzNjQ1NTU4\n</code></pre> <p>The client credentials are obtained by registration of a client at the login service web interface - e.g. https://auth.192-168-49-2.nip.io. In addition there is a helper script that can be used to create a basic client and obtain the credentials, as described in section Resource Protection\u2026 <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io \"Resource Guard\" | tee client.yaml\n</code></pre></p>"},{"location":"eoepca/registration-api/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the Registration API can be found at:</p> <ul> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"eoepca/resource-catalogue/","title":"Resource Catalogue","text":"<p>The Resource Catalogue provides a standards-based EO metadata catalogue that includes support for OGC CSW / API Records, STAC and OpenSearch.</p>"},{"location":"eoepca/resource-catalogue/#helm-chart","title":"Helm Chart","text":"<p>The Resource Catalogue is deployed via the <code>rm-resource-catalogue</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are fully documented in the README for the <code>rm-resource-catalogue</code> chart.</p> <pre><code>helm install --version 1.3.1 --values resource-catalogue-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  resource-catalogue rm-resource-catalogue\n</code></pre>"},{"location":"eoepca/resource-catalogue/#values","title":"Values","text":"<p>The Resource Catalogue supports many values to configure the service - as described in the Values section of the chart README.</p> <p>Typically, values for the following attributes may be specified:</p> <ul> <li>The fully-qualified public URL for the service</li> <li>Dynamic provisioning StorageClass for database persistence</li> <li>(optional) Specification of Ingress for reverse-proxy access to the service Note that this is only required in the case that the Resource Catalogue will not be protected by the <code>resource-guard</code> component - ref. Resource Protection. Otherwise the ingress will be handled by the <code>resource-guard</code> - use <code>ingress.enabled: false</code>.</li> <li>Metadata describing the Catalogue instance</li> <li>Tuning configuration for PostgreSQL - see values <code>db.config.XXX</code>.</li> </ul> <p>Example <code>resource-catalogue-values.yaml</code>\u2026</p> <pre><code>global:\n  namespace: rm\n# For protected access disable this ingress, and rely upon the resource-guard\n# for ingress with protection.\ningress:\n  # Enabled for unprotected 'open' access to the resource-catalogue.\n  enabled: true\n  name: resource-catalogue\n  host: resource-catalogue.192-168-49-2.nip.io\n  tls_host: resource-catalogue.192-168-49-2.nip.io\n  tls_secret_name: resource-catalogue-tls\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-production\ndb:\n  volume_storage_type: standard\n  # config:\n  #   enabled: true\n  #   shared_buffers: 2GB\n  #   effective_cache_size: 6GB\n  #   maintenance_work_mem: 512MB\n  #   checkpoint_completion_target: 0.9\n  #   wal_buffers: 16MB\n  #   default_statistics_target: 100\n  #   random_page_cost: 4\n  #   work_mem: 4MB\n  #   cpu_tuple_cost: 0.4\npycsw:\n  config:\n    server:\n      url: https://resource-catalogue.192-168-49-2.nip.io/\n    manager:\n      transactions: \"true\"\n      allowed_ips: \"*\"\n</code></pre> <p>Note</p> <p>The above example values enable transactions (write-access) to the catalogue from any IP address. This is convenient for testing/demonstration of the capability, but should be disbaled or restricted for operational deployments.</p>"},{"location":"eoepca/resource-catalogue/#protection","title":"Protection","text":"<p>As described in section Resource Protection, the <code>resource-guard</code> component can be inserted into the request path of the Resource Catalogue service to provide access authorization decisions</p> <pre><code>helm install --version 1.3.1 --values resource-catalogue-guard-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  resource-catalogue-guard resource-guard\n</code></pre> <p>The <code>resource-guard</code> must be configured with the values applicable to the Resource Catalogue for the Policy Enforcement Point (<code>pep-engine</code>) and the UMA User Agent (<code>uma-user-agent</code>)\u2026</p> <p>Example <code>resource-catalogue-guard-values.yaml</code>\u2026</p> <pre><code>#---------------------------------------------------------------------------\n# Global values\n#---------------------------------------------------------------------------\nglobal:\n  context: resource-catalogue\n  domain: 192-168-49-2.nip.io\n  nginxIp: 192.168.49.2\n  certManager:\n    clusterIssuer: letsencrypt-production\n#---------------------------------------------------------------------------\n# PEP values\n#---------------------------------------------------------------------------\npep-engine:\n  configMap:\n    asHostname: auth\n    pdpHostname: auth\n  volumeClaim:\n    name: eoepca-resman-pvc\n    create: false\n#---------------------------------------------------------------------------\n# UMA User Agent values\n#---------------------------------------------------------------------------\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: resource-catalogue\n        paths:\n          - path: /(.*)\n            service:\n              name: resource-catalogue-service\n              port: 80\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n  client:\n    credentialsSecretName: \"resman-client\"\n  logging:\n    level: \"info\"\n  unauthorizedResponse: 'Bearer realm=\"https://portal.192-168-49-2.nip.io/oidc/authenticate/\"'\n  openAccess: false\n  insecureTlsSkipVerify: true\n</code></pre> <p>Note</p> <ul> <li>TLS is enabled by the specification of <code>certManager.clusterIssuer</code></li> <li>The <code>letsencrypt</code> Cluster Issuer relies upon the deployment being accessible from the public internet via the <code>global.domain</code> DNS name. If this is not the case, e.g. for a local minikube deployment in which this is unlikely to be so. In this case the TLS will fall-back to the self-signed certificate built-in to the nginx ingress controller</li> <li><code>insecureTlsSkipVerify</code> may be required in the case that good TLS certificates cannot be established, e.g. if letsencrypt cannot be used for a local deployment. Otherwise the certificates offered by login-service Authorization Server will fail validation in the Resource Guard.</li> <li><code>customDefaultResources</code> can be specified to apply initial protection to the endpoint</li> </ul>"},{"location":"eoepca/resource-catalogue/#client-secret","title":"Client Secret","text":"<p>The Resource Guard requires confidential client credentials to be configured through the file <code>client.yaml</code>, delivered via a kubernetes secret..</p> <p>Example <code>client.yaml</code>\u2026</p> <pre><code>client-id: a98ba66e-e876-46e1-8619-5e130a38d1a4\nclient-secret: 73914cfc-c7dd-4b54-8807-ce17c3645558\n</code></pre> <p>Example <code>Secret</code>\u2026</p> <pre><code>kubectl -n rm create secret generic resman-client \\\n  --from-file=client.yaml \\\n  --dry-run=client -o yaml \\\n  &gt; resman-client-secret.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: resman-client\n  namespace: rm\ndata:\n  client.yaml: Y2xpZW50LWlkOiBhOThiYTY2ZS1lODc2LTQ2ZTEtODYxOS01ZTEzMGEzOGQxYTQKY2xpZW50LXNlY3JldDogNzM5MTRjZmMtYzdkZC00YjU0LTg4MDctY2UxN2MzNjQ1NTU4\n</code></pre> <p>The client credentials are obtained by registration of a client at the login service web interface - e.g. https://auth.192-168-49-2.nip.io. In addition there is a helper script that can be used to create a basic client and obtain the credentials, as described in section Resource Protection\u2026 <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io \"Resource Guard\" | tee client.yaml\n</code></pre></p>"},{"location":"eoepca/resource-catalogue/#resource-catalogue-usage","title":"Resource Catalogue Usage","text":"<p>The Resource Catalogue is initially populated during the initialisation of the Data Access service. See section Data-layer Configuration.</p> <p>The Resource Catalogue is accessed at the endpoint <code>https://resource-catalogue.&lt;domain&gt;/</code>, configured by your domain - e.g. https://resource-catalogue.192-168-49-2.nip.io/.</p>"},{"location":"eoepca/resource-catalogue/#loading-records","title":"Loading Records","text":"<p>As described in the pycsw documentation, ISO XML records can be loaded into the resource-catalogue using the <code>pycsw-admin.py</code> admin utility\u2026</p> <pre><code>pycsw-admin.py load_records -c /path/to/cfg -p /path/to/records\n</code></pre> <p>The <code>/path/to/records</code> can either be a single metadata file, or a directory containing multiple metadata files.</p> <p>This is most easily achieved via connection to the pycsw pod, which includes the <code>pycsw-admin.py</code> utility and the pycsw configuration file at <code>/etc/pycsw/pycsw.cfg</code>\u2026</p> <pre><code>kubectl -n rm cp \"&lt;metadata-file-or-directory&gt;\" \"&lt;pycsw-pod-name&gt;\":/tmp/metadata\nkubectl -n rm exec -i \"&lt;pycsw-pod-name&gt;\" -- pycsw-admin.py load-records -c /etc/pycsw/pycsw.cfg -p /tmp/metadata\n</code></pre> <p>The name of the pycsw pod can be obtained using <code>kubectl</code>\u2026</p> <pre><code>kubectl -n rm get pod --selector='io.kompose.service=pycsw' --output=jsonpath={.items[0].metadata.name}\n</code></pre> <p>To facilitate the loading of records via the pycsw pod, a helper script <code>load-records</code> has been provided in the git repository that hosts this document\u2026</p> <pre><code>git clone git@github.com:EOEPCA/deployment-guide\ncd deployment-guide\n./deploy/bin/load-records \"&lt;metadata-file-or-directory&gt;\"\n</code></pre> <p>The helper script identifies the pycsw pod, copies the metadata files to the pod, and runs <code>pycsw-admin.py load-records</code> within the pod to load the records.</p>"},{"location":"eoepca/resource-catalogue/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the Resource Catalogue can be found at:</p> <ul> <li>Helm Chart</li> <li>pycsw Documentation</li> <li>GitHub Repository</li> </ul>"},{"location":"eoepca/resource-protection/","title":"Resource Protection","text":"<p>EOEPCA defines Building Blocks within a micro-service architecture. The services are subject to protection within an Identity and Access Management (IAM) approach that includes:</p> <ul> <li>Login Service (Authorization Server)</li> <li>Policy Decision Point (PDP)</li> <li>Policy Enforcement Point (PEP)</li> </ul> <p>Building Blocks that act as a Resource Server are individually protected by a Policy Enforcement Point (PEP). The PEP enforces the authorization decision in collaboration with the Login Service and Policy Decision Point (PDP).</p> <p>The PEP expects to interface to a client (user agent, e.g. browser) using User Managed Access (UMA) flows. It is not typical for a client to support UMA flows, and so the PEP can be deployed with a companion UMA User Agent component that interfaces between the client and the PEP, and performs the UMA Flow on behalf of the client.</p> <p>The Resource Guard is a \u2018convenience\u2019 component that deploys the PEP &amp; UMA User Agent as a cooperating pair.</p> <p>The Resource Guard \u2018inserts itself\u2019 into the request path of the target Resource Server using the <code>auth_request</code> facility offered by Nginx. Thus, the Resource Guard deploys with an Ingress specification that:</p> <ul> <li>Configures the <code>auth_request</code> module to defer access authorization to the <code>uma-user-agent</code> service</li> <li>Configures the ingress rules (host/path) for the target Resource Server</li> </ul>"},{"location":"eoepca/resource-protection/#helm-chart","title":"Helm Chart","text":"<p>The Resource Guard is deployed via the <code>resource-guard</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are fully documented in the README for the <code>resource-guard</code> chart.</p> <p>It is expected to deploy multiple instances of the Resource Guard chart, one for each Resource Server to be protected.</p> <pre><code>helm install --version 1.3.1 --values myservice-guard-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  myservice-guard resource-guard\n</code></pre>"},{"location":"eoepca/resource-protection/#values","title":"Values","text":"<p>The helm chart is deployed with values that are passed through to the subcharts for the <code>pep-engine</code> and <code>uma-user-agent</code>. Typical values to be specified include:</p> <ul> <li>Host/domain details for the Login Service and PDP, e.g. <code>auth.192-168-49-2.nip.io</code></li> <li>IP Address of the public facing reverse proxy (Nginx Ingress Controller), e.g. <code>192.168.49.2</code></li> <li>Name of Persistent Volume Claim for <code>pep-engine</code> persistence, e.g. <code>myservice-pep-pvc</code></li> <li>TLS Certificate Provider, e.g. <code>letsencrypt-production</code></li> <li>Optional specification of default resources with which to initialise the policy database for the component</li> <li>Ingress rules definition for reverse-proxy to the target Resource Server</li> <li>Name of <code>Secret</code> that contains the client credentials used by the <code>uma-user-agent</code> to interface with the Login Service. See section Client Secret below</li> </ul> <p>Example <code>myservice-guard-values.yaml</code>\u2026 <pre><code>#---------------------------------------------------------------------------\n# Global values\n#---------------------------------------------------------------------------\nglobal:\n  context: myservice\n  domain: 192-168-49-2.nip.io\n  nginxIp: 192.168.49.2\n  certManager:\n    clusterIssuer: letsencrypt-production\n#---------------------------------------------------------------------------\n# PEP values\n#---------------------------------------------------------------------------\npep-engine:\n  configMap:\n    asHostname: auth\n    pdpHostname: auth\n  customDefaultResources:\n  - name: \"Eric's space\"\n    description: \"Protected Access for eric to his space in myservice\"\n    resource_uri: \"/ericspace\"\n    scopes: []\n    default_owner: \"d3688daa-385d-45b0-8e04-2062e3e2cd86\"\n  volumeClaim:\n    name: myservice-pep-pvc\n    create: false\n#---------------------------------------------------------------------------\n# UMA User Agent values\n#---------------------------------------------------------------------------\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: myservice\n        paths:\n          - path: /(.*)\n            service:\n              name: myservice\n              port: 80\n          - path: /(doc.*)\n            service:\n              name: myservice-docs\n              port: 80\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n  client:\n    credentialsSecretName: \"myservice-agent\"\n  logging:\n    level: \"debug\"\n  unauthorizedResponse: 'Bearer realm=\"https://portal.192-168-49-2.nip.io/oidc/authenticate/\"'\n#---------------------------------------------------------------------------\n# END values\n#---------------------------------------------------------------------------\n</code></pre></p>"},{"location":"eoepca/resource-protection/#client-credentials","title":"Client Credentials","text":"<p>The <code>uma-user-agent</code> requires Client Credentials for its interactions with the <code>login-service</code>. The <code>uma-user-agent</code> expects to read these credentials from the file <code>client.yaml</code>, in the form\u2026</p> <pre><code>client-id: &lt;my-client-id&gt;\nclient-secret: &lt;my-secret&gt;\n</code></pre>"},{"location":"eoepca/resource-protection/#client-registration","title":"Client Registration","text":"<p>To obtain the Client Credentials required by the <code>uma-user-agent</code> it is necessary to register a client with the <code>login-service</code>, or use the credentials for an existing client.</p> <p>A helper script is provided to register a basic client and obtain the required credentials. The script is available in the <code>deployment-guide</code> repository, and can be obtained as follows\u2026</p> <pre><code>git clone git@github.com:EOEPCA/deployment-guide\ncd deployment-guide\n</code></pre> <p>The <code>register-client</code> helper script requires some command-line arguments\u2026</p> <pre><code>Usage:\n  register_client &lt;authorization-server-hostname&gt; &lt;client-name&gt; [&lt;redirect-uri&gt; [&lt;logout-uri&gt;]]\n</code></pre> <p>For example\u2026</p> <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io myclient\n\nINFO: Preparing docker image... [done]\nClient successfully registered.\nMake a note of the credentials:\nclient-id: a98ba66e-e876-46e1-8619-5e130a38d1a4\nclient-secret: 73914cfc-c7dd-4b54-8807-ce17c3645558\n</code></pre> <p>Or to register OIDC redirect URLs\u2026 <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io myclient https://portal.192-168-49-2.nip.io/oidc/callback/ https://portal.192-168-49-2.nip.io/logout\n</code></pre></p> <p>The script writes the \u2018client credentials\u2019 to stdout - in the expected YAML configuration file format - which can be redirected to file\u2026 <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io myclient | tee client.yaml\n</code></pre> \u2026writes the client credentials to the file <code>client.yaml</code>.</p> <p>NOTE that the <code>register-client</code> helper relies upon <code>docker</code> to build and run the script.</p>"},{"location":"eoepca/resource-protection/#client-secret","title":"Client Secret","text":"<p>The <code>client.yaml</code> configuration file is made available via a Kubernetes Secret\u2026</p> <pre><code>kubectl -n myservice-ns create secret generic myservice-agent \\\n  --from-file=client.yaml \\\n  --dry-run=client -o yaml \\\n  &gt; myservice-agent-secret.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: myservice-agent\n  namespace: myservice-ns\ndata:\n  client.yaml: Y2xpZW50LWlkOiBhOThiYTY2ZS1lODc2LTQ2ZTEtODYxOS01ZTEzMGEzOGQxYTQKY2xpZW50LXNlY3JldDogNzM5MTRjZmMtYzdkZC00YjU0LTg4MDctY2UxN2MzNjQ1NTU4\n</code></pre> <p>The <code>resource-guard</code> deployment is configured with the name of the <code>Secret</code> through the helm chart value <code>client.credentialsSecretName</code>.</p>"},{"location":"eoepca/resource-protection/#user-id-token","title":"User ID Token","text":"<p>As described in the README for the Resource Guard, it is necessary for a request to a protected resource to provide the User ID Token in the request header.</p>"},{"location":"eoepca/resource-protection/#obtaining-the-user-id-token","title":"Obtaining the User ID Token","text":"<p>In the simple case of a user with username/password held within the Login Service, the User ID Token can be obtained as follows:</p> <pre><code>curl --location --request POST 'https://auth.192-168-49-2.nip.io/oxauth/restv1/token' \\\n--header 'Cache-Control: no-cache' \\\n--header 'Content-Type: application/x-www-form-urlencoded' \\\n--data-urlencode 'scope=openid user_name is_operator' \\\n--data-urlencode 'grant_type=password' \\\n--data-urlencode 'username=&lt;username&gt;' \\\n--data-urlencode 'password=&lt;password&gt;' \\\n--data-urlencode 'client_id=&lt;client-id&gt;' \\\n--data-urlencode 'client_secret=&lt;client-password&gt;'\n</code></pre> <p>The User ID Token is included in the <code>id_token</code> field of the json response.</p> <p>Alternatively, OAuth/OIDC flows can be followed to authenticate via external identity providers.</p>"},{"location":"eoepca/resource-protection/#user-id-token-in-http-requests","title":"User ID Token in HTTP requests","text":"<p>The Resource Guard protection supports presentation of the User ID Token via the following HTTP request headers (in order of priority)\u2026</p> <ul> <li><code>Authorization</code> header as a bearer token - in the form: <code>Authorization: Bearer &lt;token&gt;</code></li> <li><code>X-User-Id</code> header</li> <li><code>Cookie: auth_user_id=&lt;token&gt;</code> <p>Note that the name of the cookie is configurable</p> </li> </ul>"},{"location":"eoepca/resource-protection/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the Resource Guard can be found at:</p> <ul> <li>Helm Chart</li> <li>README</li> <li>GitHub Repository:<ul> <li>pep-engine</li> <li>uma-user-agent</li> </ul> </li> </ul>"},{"location":"eoepca/user-profile/","title":"User Profile","text":"<p>The User Profile represents the user\u2019s \u2018account\u2019 within the platform.</p>"},{"location":"eoepca/user-profile/#helm-chart","title":"Helm Chart","text":"<p>The User Profile is deployed via the <code>user-profile</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are fully documented in the README for the <code>user-profile</code> chart.</p> <pre><code>helm install --version 1.1.12 --values user-profile-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  user-profile user-profile\n</code></pre>"},{"location":"eoepca/user-profile/#values","title":"Values","text":"<p>At minimum, values for the following attributes should be specified:</p> <ul> <li>Public hostname of the Authorization Server, e.g. <code>auth.192-168-49-2.nip.io</code></li> <li>IP Address of the public facing reverse proxy (Nginx Ingress Controller), e.g. <code>192.168.49.2</code></li> <li>Name of Persistent Volume Claim for <code>user-profile</code> persistence, e.g. <code>eoepca-userman-pvc</code> The boolen value <code>volumeClaim.create</code> can be used for the PVC to be created by the helm release. This creates a volume of type <code>host-path</code> and, hence, is only useful for single-node development usage.</li> </ul> <p>Example <code>user-profile-values.yaml</code>\u2026 <pre><code>global:\n  domain: auth.192-168-49-2.nip.io\n  nginxIp: 192.168.49.2\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\n</code></pre></p>"},{"location":"eoepca/user-profile/#user-profile-usage","title":"User Profile Usage","text":"<p>The User Profile is accessed through the <code>/web_ui</code> path of the Login Service, e.g. http://auth.kube.guide.eoepca.org/web_ui.</p>"},{"location":"eoepca/user-profile/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the User Profile can be found at:</p> <ul> <li>Helm Chart</li> <li>Wiki</li> <li>GitHub Repository</li> </ul>"},{"location":"eoepca/workspace/","title":"Workspace","text":"<p>The Workspace provides protected user resource management that includes dedicated storage and services for resource discovery and access.</p>"},{"location":"eoepca/workspace/#workspace-api","title":"Workspace API","text":"<p>The Workspace API provides a REST service through which user workspaces can be created, interrogated, managed and deleted.</p>"},{"location":"eoepca/workspace/#helm-chart","title":"Helm Chart","text":"<p>The Workspace API is deployed via the <code>rm-workspace-api</code> helm chart from the EOEPCA Helm Chart Repository.</p> <p>The chart is configured via values that are fully documented in the README for the <code>um-workspace-api</code> chart.</p> <pre><code>helm install --version 1.3.5 --values workspace-api-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  workspace-api rm-workspace-api\n</code></pre>"},{"location":"eoepca/workspace/#values","title":"Values","text":"<p>At minimum, values for the following attributes should be specified:</p> <ul> <li>The fully-qualified public URL for the service</li> <li>(optional) Specification of Ingress for reverse-proxy access to the service Note that this is only required in the case that the Workspace API will not be protected by the <code>resource-guard</code> component - ref. Resource Protection. Otherwise the ingress will be handled by the <code>resource-guard</code> - use <code>ingress.enabled: false</code>.</li> <li>Prefix for user projects in OpenStack</li> <li>Details for underlying S3 object storage service</li> <li>Identification of secret that provides the client credentials for resource protection</li> <li>Whether flux components should be installed - otherwise they must already be present - Flux Dependency</li> <li>Name of the ConfigMap for user workspace templates - See User Workspace Templates</li> </ul> <p>Example <code>workspace-api-values.yaml</code>\u2026 <pre><code>fullnameOverride: workspace-api\ningress:\n  enabled: true\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-production\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/enable-cors: \"true\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n  hosts:\n    - host: workspace-api-open.192-168-49-2.nip.io\n      paths: [\"/\"]\n  tls:\n    - hosts:\n        - workspace-api-open.192-168-49-2.nip.io\n      secretName: workspace-api-open-tls\nfluxHelmOperator:\n  enabled: true\nprefixForName: \"ws\"\nworkspaceSecretName: \"bucket\"\nnamespaceForBucketResource: \"rm\"\ns3Endpoint: \"https://minio.192-168-49-2.nip.io\"\ns3Region: \"RegionOne\"\nharborUrl: \"https://harbor.192-168-49-2.nip.io\"\nharborUsername: \"admin\"\nharborPasswordSecretName: \"harbor\"\numaClientSecretName: \"resman-client\"\numaClientSecretNamespace: \"rm\"\nworkspaceChartsConfigMap: \"workspace-charts\"\nbucketEndpointUrl: \"http://minio-bucket-api:8080/bucket\"\npepBaseUrl: \"http://workspace-api-pep:5576/resources\"\nautoProtectionEnabled: True\n</code></pre></p> <p>Note</p> <ul> <li>The Workspace API assumes a deployment of the Harbor Container Regsitry, as configured by the <code>harborXXX</code> values above.See section Container Registry.</li> <li>The password for the harbor <code>admin</code> user must be created as described in the section Harbor <code>admin</code> Password.</li> <li>If the workspace-api is access protected (ref. section Protection), then it is recommended to enable <code>autoProtectionEnabled</code> and to specifiy the <code>pepBaseUrl</code>.</li> <li>The workspace-api initiates the creation of a storage \u2018bucket\u2019 for each workspace - the actual bucket creation being abstracted via a webhook - the URL of which is specified in the value <code>bucketEndpointUrl</code>. See section Bucket Creation Webhook for details.</li> </ul>"},{"location":"eoepca/workspace/#harbor-admin-password","title":"Harbor <code>admin</code> Password","text":"<p>The password for the harbor <code>admin</code> user is provided to the workspace-api via the specified secret - defined as <code>harbor</code> above.</p> <p>This secret must be created - for example as follows\u2026</p> <pre><code>kubectl -n rm create secret generic harbor \\\n  --from-literal=HARBOR_ADMIN_PASSWORD=\"changeme\"\n</code></pre>"},{"location":"eoepca/workspace/#flux-dependency","title":"Flux Dependency","text":"<p>Workspaces are created by instantiating the <code>rm-user-workspace</code> helm chart for each user/group. The Workspace API uses Flux CD as a helper to manage these subordinate helm charts - via flux resources of type <code>HelmRelease</code>. Thus, it is necessary to deploy within the cluster the aspects of flux that support this helm chart management - namely the flux <code>helm-controller</code>, <code>source-controller</code> and the Kubernetes Custom Resource Definitions (CRD) for <code>HelmRelease</code> and <code>HelmRepository</code>.</p> <p>In case you are not already using flux within your clsuter, then the Workspace API helm chart can be configured to deploy the required flux components\u2026 <pre><code>fluxHelmOperator:\n  enabled: true  # true = install flux for me, false = I already have flux\n</code></pre></p>"},{"location":"eoepca/workspace/#user-workspace-templates","title":"User Workspace Templates","text":"<p>The Workspace API instantiates for each user a set of services, including a Resource Catalogue and Data Access services. These user services are instantiated via helm using templates. The templates are provided to the Workspace API in a <code>ConfigMap</code> that is, by default, named <code>workspace-charts</code>. Each file in the config-map is expected to be of <code>kind</code> <code>HelmRelease</code>. During creation of a new workspace, the Worksapce API applies each file to the cluster in the namespace of the newly created namespace.</p> <p>The default ConfigMap that is included with this guide contains the following templates for instantiation of user-specific components:</p> <ul> <li>Data Access: <code>template-hr-data-access.yaml</code></li> <li>Resource Catalogue: <code>template-hr-resource-catalogue.yaml</code></li> <li>Protection: <code>template-hr-resource-guard.yaml</code></li> </ul> <p>Each of these templates is expressed as a flux <code>HelmRelease</code> object that describes the helm chart and values required to deploy the service.</p> <p>In addition, ConfigMap templates are included that provide specific details required to access the user-scoped workspace resources, including access to S3 object storage and container registry:</p> <ul> <li>S3 client configuration: <code>template-cm-aws-config.yaml</code></li> <li>S3 client credentials: <code>template-cm-aws-credentials.yaml</code></li> <li>Container registry configuration: <code>template-cm-docker-config.yaml</code></li> </ul> <p>These ConfigMaps are designed to be mounted as files into the runtime environments of other components for user workspace integration. In particular the Application Hub makes use of this approach to provide a user experience that integrates with the user\u2019s workspace resources.</p>"},{"location":"eoepca/workspace/#templates-configmap","title":"Templates ConfigMap","text":"<p>The templates are provided to the Workspace API as a <code>ConfigMap</code> in the namespace of the Workspace API deployment\u2026</p> <p>(for full examples see https://github.com/EOEPCA/deployment-guide/tree/eoepca-v1.3/deploy/eoepca/workspace-templates)</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: workspace-charts\ndata:\n  template-hr-resource-catalogue.yaml: |\n    apiVersion: helm.toolkit.fluxcd.io/v2beta1\n    kind: HelmRelease\n    metadata:\n      name: rm-resource-catalogue\n    spec:\n      interval: 5m\n      chart:\n        spec:\n          chart: rm-resource-catalogue\n          version: 1.3.1\n          sourceRef:\n            kind: HelmRepository\n            name: eoepca\n            namespace: rm\n      values:\n        ...\n  template-hr-data-access.yaml: |\n    apiVersion: helm.toolkit.fluxcd.io/v2beta1\n    kind: HelmRelease\n    metadata:\n      name: vs\n    spec:\n      interval: 5m\n      chart:\n        spec:\n          chart: data-access\n          version: 1.3.1\n          sourceRef:\n            kind: HelmRepository\n            name: eoepca\n            namespace: rm\n      values:\n        ...\n  template-hr-resource-guard.yaml: |\n    apiVersion: helm.toolkit.fluxcd.io/v2beta1\n    kind: HelmRelease\n    metadata:\n      name: resource-guard\n    spec:\n      interval: 5m\n      chart:\n        spec:\n          chart: resource-guard\n          version: 1.3.1\n          sourceRef:\n            kind: HelmRepository\n            name: eoepca\n            namespace: rm\n      values:\n        ...\n  template-cm-aws-config.yaml: |\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: aws-config\n    data:\n      aws-config: |\n        [default]\n        region = {{ s3_region }}\n        s3 =\n          endpoint_url = {{ s3_endpoint_url }}\n        s3api =\n          endpoint_url = {{ s3_endpoint_url }}\n        [plugins]\n        endpoint = awscli_plugin_endpoint\n  template-cm-aws-credentials.yaml: |\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: aws-credentials\n    data:\n      aws-credentials: |\n        [default]\n        aws_access_key_id = {{ access_key_id }}\n        aws_secret_access_key = {{ secret_access_key }}\n  template-cm-docker-config.yaml: |\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: docker-config\n    data:\n      docker-config: |\n        {\n          \"auths\": {\n            \"{{ container_registry_host }}\": {\n              \"auth\": \"{{ container_registry_credentials }}\"\n            }\n        }\n</code></pre> <p>Notice the use of workspace template parameters <code>{{ param_name }}</code> that are used at workspace creation time to contextualise the workspace for the owning user. See section Workspace Template Parameters for more information.</p>"},{"location":"eoepca/workspace/#helmrepositories-for-templates","title":"HelmRepositories for Templates","text":"<p>As can be seen above, the HelmRelease templates rely upon objects of type HelmRepository that define the hosting helm chart repository. Thus, in support of the workspace templates, appropriate HelmRepository objects must be provisioned within the cluster. For example, in support of the above examples that rely upon the EOEPCA Helm Chart Repository\u2026</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta1\nkind: HelmRepository\nmetadata:\n  name: eoepca\n  namespace: rm\nspec:\n  interval: 2m\n  url: https://eoepca.github.io/helm-charts/\n</code></pre>"},{"location":"eoepca/workspace/#workspace-template-parameters","title":"Workspace Template Parameters","text":"<p>The Workspace API uses the <code>jinja2</code> templating engine when applying the resources for a user workspace. The current parameters are currently supported:</p> <ul> <li><code>workspace_name</code>   The name of the workspace - <code>{{ workspace_name }}</code> used to ensure unique naming of cluster resources, such as service ingress</li> <li><code>default_owner</code>   The <code>uuid</code> of the owner of the workspace - <code>{{ default_owner }}</code> used to initialise the workspace protection</li> <li>S3 Object Storage details\u2026<ul> <li><code>{{ s3_endpoint_url }}</code></li> <li><code>{{ s3_region }}</code></li> <li><code>{{ access_key_id }}</code></li> <li><code>{{ secret_access_key }}</code></li> </ul> </li> <li>Container Registry details\u2026<ul> <li><code>{{ container_registry_host }}</code></li> <li><code>{{ container_registry_credentials }}</code></li> </ul> </li> </ul>"},{"location":"eoepca/workspace/#protection","title":"Protection","text":"<p>As described in section Resource Protection, the <code>resource-guard</code> component can be inserted into the request path of the Workspace API service to provide access authorization decisions</p> <pre><code>helm install --version 1.3.1 --values workspace-api-guard-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  workspace-api-guard resource-guard\n</code></pre> <p>The <code>resource-guard</code> must be configured with the values applicable to the Workspace API for the Policy Enforcement Point (<code>pep-engine</code>) and the UMA User Agent (<code>uma-user-agent</code>)\u2026</p> <p>Example <code>workspace-api-guard-values.yaml</code>\u2026</p> <pre><code>#---------------------------------------------------------------------------\n# Global values\n#---------------------------------------------------------------------------\nglobal:\n  context: workspace-api\n  domain: 192-168-49-2.nip.io\n  nginxIp: 192.168.49.2\n  certManager:\n    clusterIssuer: letsencrypt-production\n#---------------------------------------------------------------------------\n# PEP values\n#---------------------------------------------------------------------------\npep-engine:\n  configMap:\n    asHostname: auth\n    pdpHostname: auth\n  defaultResources:\n    - name: \"Workspace API Base Path\"\n      description: \"Protected root path for operators only\"\n      resource_uri: \"/\"\n      scopes: []\n      default_owner: \"0000000000000\"\n  customDefaultResources:\n    - name: \"Workspace API Swagger Docs\"\n      description: \"Public access to workspace API swagger docs\"\n      resource_uri: \"/docs\"\n      scopes:\n        - \"public_access\"\n      default_owner: \"0000000000000\"\n    - name: \"Workspace API OpenAPI JSON\"\n      description: \"Public access to workspace API openapi.json file\"\n      resource_uri: \"/openapi.json\"\n      scopes:\n        - \"public_access\"\n      default_owner: \"0000000000000\"\n  volumeClaim:\n    name: eoepca-resman-pvc\n    create: false\n#---------------------------------------------------------------------------\n# UMA User Agent values\n#---------------------------------------------------------------------------\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: workspace-api\n        paths:\n          - path: /(.*)\n            service:\n              name: workspace-api\n              port: http\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n  client:\n    credentialsSecretName: \"resman-client\"\n  logging:\n    level: \"info\"\n  unauthorizedResponse: 'Bearer realm=\"https://portal.192-168-49-2.nip.io/oidc/authenticate/\"'\n  openAccess: false\n  insecureTlsSkipVerify: true\n</code></pre> <p>Note</p> <ul> <li>TLS is enabled by the specification of <code>certManager.clusterIssuer</code></li> <li>The <code>letsencrypt</code> Cluster Issuer relies upon the deployment being accessible from the public internet via the <code>global.domain</code> DNS name. If this is not the case, e.g. for a local minikube deployment in which this is unlikely to be so. In this case the TLS will fall-back to the self-signed certificate built-in to the nginx ingress controller</li> <li><code>insecureTlsSkipVerify</code> may be required in the case that good TLS certificates cannot be established, e.g. if letsencrypt cannot be used for a local deployment. Otherwise the certificates offered by login-service Authorization Server will fail validation in the Resource Guard.</li> <li><code>customDefaultResources</code> can be specified to apply initial protection to the endpoint. In the example above we open up access to the OpenAPI (swagger) documentation that does not require protection.</li> </ul>"},{"location":"eoepca/workspace/#client-secret","title":"Client Secret","text":"<p>The Resource Guard requires confidential client credentials to be configured through the file <code>client.yaml</code>, delivered via a kubernetes secret..</p> <p>Example <code>client.yaml</code>\u2026</p> <pre><code>client-id: a98ba66e-e876-46e1-8619-5e130a38d1a4\nclient-secret: 73914cfc-c7dd-4b54-8807-ce17c3645558\n</code></pre> <p>Example <code>Secret</code>\u2026</p> <pre><code>kubectl -n rm create secret generic resman-client \\\n  --from-file=client.yaml \\\n  --dry-run=client -o yaml \\\n  &gt; resman-client-secret.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: resman-client\n  namespace: rm\ndata:\n  client.yaml: Y2xpZW50LWlkOiBhOThiYTY2ZS1lODc2LTQ2ZTEtODYxOS01ZTEzMGEzOGQxYTQKY2xpZW50LXNlY3JldDogNzM5MTRjZmMtYzdkZC00YjU0LTg4MDctY2UxN2MzNjQ1NTU4\n</code></pre> <p>The client credentials are obtained by registration of a client at the login service web interface - e.g. https://auth.192-168-49-2.nip.io. In addition there is a helper script that can be used to create a basic client and obtain the credentials, as described in section Resource Protection\u2026 <pre><code>./deploy/bin/register-client auth.192-168-49-2.nip.io \"Resource Guard\" | tee client.yaml\n</code></pre></p>"},{"location":"eoepca/workspace/#workspace-api-usage","title":"Workspace API Usage","text":"<p>The Workspace API provides a REST interface that is accessed at the endpoint https://workspace-api.192-168-49-2.nip.io/. See the Swagger Docs.</p>"},{"location":"eoepca/workspace/#additional-information","title":"Additional Information","text":"<p>Additional information regarding the Workspace API can be found at:</p> <ul> <li>Helm Chart</li> <li>Wiki</li> <li>GitHub Repository</li> </ul>"},{"location":"eoepca/workspace/#bucket-creation-webhook","title":"Bucket Creation Webhook","text":"<p>With helm chart version <code>1.3.1</code> of the <code>workspace-api</code> the approach to bucket creation has been re-architected to use a webhook approach.</p>"},{"location":"eoepca/workspace/#approach","title":"Approach","text":"<p>During workspace creation the <code>workspace-api</code> needs to create an object storage bucket for the user. The method by which the bucket is created is a function of the hosting infrastructure object storage layer - i.e. there is no \u2018common\u2019 approach for the <code>workspace-api</code> to perform the bucket creation.</p> <p>In order to allow this bucket creation step to be customised by the platform integrator, the workspace-api is configured with a webhook endpoint that is invoked to effect the bucket creation on behalf of the workspace-api.</p> <p>The workspace-api is configured by the following value in its helm chart deployment, e.g\u2026 <pre><code>bucketEndpointUrl: \"http://my-bucket-webhook:8080/bucket\"\n</code></pre></p> <p>The webhook service must implement the following REST interface\u2026</p> <p>method: <code>POST</code> content-type: <code>application/json</code> data: <pre><code>{\n  bucketName: str\n  secretName: str\n  secretNamespace: str\n}\n</code></pre></p> <p>There are two possible approaches to implement this request, distinguished by the response code\u2026</p> <ul> <li><code>200</code>   The bucket is created and the credentials are included in the response body.   In this case only the supplied <code>bucketName</code> is relevant to fulfil the request.</li> <li><code>201</code>   The bucket will be created (asychronously) and the outcome is provided by the webhook via a Kubernetes secret, as per the <code>secretName</code> and <code>secretNamespace</code> request parameters</li> </ul> <p><code>200</code> Response</p> <p>In case <code>200</code> response, the response body should communicate the credentials with an <code>application/json</code> content-type in the form\u2026 <pre><code>{\n    \"bucketname\": \"...\",\n    \"access_key\": \"...\",\n    \"access_secret\": \"....\",\n    \"projectid\": \"...\",\n}\n</code></pre></p> <p>In this case the workspace-api will create the appropriate bucket secret using the returned credentials.</p> <p><code>201</code> Response</p> <p>In case <code>201</code> response, the secret should be created in the form\u2026 <pre><code>data:\n  bucketname: \"...\"\n  access: \"...\"\n  secret: \"...\"\n  projectid: \"...\"\n</code></pre></p> <p>In this case the workspace-api will wait for the (asynchronous) creation of the specified secret before continuing with the workspace creation.</p> <p>Overall Outcome</p> <p>In both cases the ultimate outcome is the creation of the bucket in the back-end object storage, and the creation of a Kubernetes secret that maintains the credentials for access to the bucket. The existence of the bucket secret is prerequisite to the continuation of the user workspace creation.</p>"},{"location":"eoepca/workspace/#minio-bucket-api-webhook","title":"Minio Bucket API (Webhook)","text":"<p>The Minio Bucket API provides an implementation of a Bucket Creation Webhook for a Minio S3 Object Storage backend. This is used as the default in this guide - but should be replaced for a production deployment with an appropriate webhook to integrate with the object storage solution of the deployment environment.</p>"},{"location":"eoepca/workspace/#helm-chart_1","title":"Helm Chart","text":"<p>The Minio Bucket API is deployed via the <code>rm-minio-bucket-api</code> helm chart from the EOEPCA Helm Chart Repository - ref. Helm Chart for the Minio Bucket API.</p> <pre><code>helm install --version 0.0.4 --values minio-bucket-api-values.yaml \\\n  --repo https://eoepca.github.io/helm-charts \\\n  rm-minio-bucket-api rm-minio-bucket-api\n</code></pre>"},{"location":"eoepca/workspace/#values_1","title":"Values","text":"<p>At minimum, values for the following attributes should be specified:</p> <ul> <li>The URL for the Minio endpoint - <code>minIOServerEndpoint</code></li> <li>The credentials for admin access to Minio - via the specified secret <code>accessCredentials.secretName</code> (ref. Minio Credentials Secret)</li> </ul> <p>Example <code>minio-bucket-api-values.yaml</code>\u2026 <pre><code>fullnameOverride: minio-bucket-api\nminIOServerEndpoint: https://minio.192-168-49-2.nip.io\naccessCredentials:\n  secretName: minio-auth\n</code></pre></p>"},{"location":"eoepca/workspace/#additional-information_1","title":"Additional Information","text":"<p>Additional information regarding the Minio Bucket API can be found at:</p> <ul> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"quickstart/creodias-deployment/","title":"CREODIAS Deployment","text":"<p>Based upon our development experiences on CREODIAS, there is a wrapper script <code>creodias</code> with particular customisations suited to the CREODIAS infrastructure and data offering. The customisations are expressed through environment variables that are captured in the file <code>creodias-options</code>.</p> <p>These scripts are examples that can be seen as a starting point, from which they can be adapted to your needs.</p> <p>The CREODIAS deployment applies the following configuration:</p> <ul> <li>Assumes a private deployment - i.e. no external-facing IP/ingress, and hence no TLS To configure an external-facing deployment with TLS protection, then see section Public Deployment</li> <li>No TLS for service ingress endpoints</li> <li>Protected service endpoints requiring IAM authorization See Endpoint Protection below for further information</li> </ul> <p>With reference to the file <code>creodias-options</code>, particular attention is drawn to the following environment variables that require tailoring to your CREODIAS (Cloudferro) environment\u2026</p> <ul> <li>Passwords: <code>LOGIN_SERVICE_ADMIN_PASSWORD</code>, <code>MINIO_ROOT_PASSWORD</code>, <code>HARBOR_ADMIN_PASSWORD</code></li> <li>OpenStack details: see section Openstack Configuration</li> <li>If configuring an external deployment - ref. Public Deployment\u2026<ul> <li><code>public_ip</code> - The public IP address through which the deployment is exposed via the ingress-controller</li> <li><code>domain</code> - The DNS domain name through which the deployment is accessed - forming the stem for all service hostnames in the ingress rules</li> </ul> </li> </ul> <p>Once the file <code>creodias-options</code> has been well populated for your environment, then the deployment is initiated with\u2026 <pre><code>./deploy/creodias/creodias\n</code></pre> \u2026noting that this step is a customised version of that described in section Deployment.</p>"},{"location":"quickstart/creodias-deployment/#endpoint-protection","title":"Endpoint Protection","text":"<p>Similarly the script <code>creodias-protection</code> is a customised version of that described in section Apply Protection. Once the main deployment has completed, then the test users can be created, their IDs (<code>Inum</code>) set in script <code>creodias-protection</code>, and the resource protection can then be applied\u2026</p> <pre><code>./deploy/creodias/creodias-protection\n</code></pre>"},{"location":"quickstart/creodias-deployment/#harvest-creodias-data","title":"Harvest CREODIAS Data","text":"<p>The harvester can be deployed with a default configuration file at <code>/config.yaml</code>. As described in the Data Access section, harvesting according to this configuration can be triggered with\u2026 <pre><code>kubectl -n rm exec -it deployment.apps/data-access-harvester -- python3 -m harvester harvest --config-file /config.yaml --host data-access-redis-master --port 6379 Creodias-Opensearch\n</code></pre></p> <p>See the Harvester section below for an explanation of this harvester configuration.</p>"},{"location":"quickstart/creodias-deployment/#data-specification-walkthrough","title":"Data Specification Walkthrough","text":"<p>The example scripts include optional specifcation of data-access/harvesting configuration that is tailored for the CREODIAS data offering. This is controlled via the option <code>CREODIAS_DATA_SPECIFICATION=true</code> - see Environment Variables.</p> <p>This section provides a walkthrough of this configuration for CREODIAS - to act as an aid to understanding by way of a worked example.</p>"},{"location":"quickstart/creodias-deployment/#harvester","title":"Harvester","text":"<p>The harvester configuration specifies datasets with spatial/temporal extents, which is configured into the file <code>/config.yaml</code> of the <code>data-access-harvester</code> deployment.</p> <p>The harvester is configured as follows\u2026</p> <pre><code>harvester:\n  replicaCount: 1\n  resources:\n    requests:\n      cpu: 100m\n      memory: 100Mi\n  config:\n    redis:\n      host: data-access-redis-master\n      port: 6379\n    harvesters:\n      - name: Creodias-Opensearch\n        resource:\n          url: https://datahub.creodias.eu/resto/api/collections/Sentinel2/describe.xml\n          type: OpenSearch\n          format_config:\n            type: 'application/json'\n            property_mapping:\n              start_datetime: 'startDate'\n              end_datetime: 'completionDate'\n              productIdentifier: 'productIdentifier'\n          query:\n            time:\n              property: sensed\n              begin: 2019-09-10T00:00:00Z\n              end: 2019-09-11T00:00:00Z\n            collection: null\n            bbox: 14.9,47.7,16.4,48.7\n        filter: {}\n        postprocess:\n          - type: harvester_eoepca.postprocess.CREODIASOpenSearchSentinel2Postprocessor\n        queue: register\n      - name: Creodias-Opensearch-Sentinel1\n        resource:\n          url: https://datahub.creodias.eu/resto/api/collections/Sentinel1/describe.xml\n          type: OpenSearch\n          format_config:\n            type: 'application/json'\n            property_mapping:\n              start_datetime: 'startDate'\n              end_datetime: 'completionDate'\n              productIdentifier: 'productIdentifier'\n          query:\n            time:\n              property: sensed\n              begin: 2019-09-10T00:00:00Z\n              end: 2019-09-11T00:00:00Z\n            collection: null\n            bbox: 14.9,47.7,16.4,48.7\n            extra_params:\n              productType: GRD-COG\n        filter: {}\n        postprocess:\n          - type: harvester_eoepca.postprocess.CREODIASOpenSearchSentinel1Postprocessor\n        queue: register\n</code></pre> <p>Based upon this harvester configuration we expect that the following query is made to discover data - i.e. an OpenSearch query, with json response representation, for a defined spatial and temporal extent\u2026</p> <pre><code>https://datahub.creodias.eu/resto/api/collections/Sentinel2/search.json?startDate=2019-09-10T00:00:00Z&amp;completionDate=2019-09-11T00:00:00Z&amp;box=14.9,47.7,16.4,48.7\n</code></pre> <p>From the result returned, the path to each product (<code>feature</code>) is obtained from the <code>productIdentifier</code> property, e.g.</p> <pre><code>{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {\n        \"productIdentifier\": \"/eodata/Sentinel-2/MSI/L1C/2019/09/10/S2B_MSIL1C_20190910T095029_N0208_R079_T33TXN_20190910T120910.SAFE\"\n        ...\n      }\n      ...\n    }\n    ...\n  ]\n}\n</code></pre> <p>The harvester is configured with a Sentinel-2/CREODIAS specific post-processor <code>harvester_eoepca.postprocess.CREODIASOpenSearchSentinel2Postprocessor</code> which transforms the product path from <code>/eodata/...</code> to <code>s3://EODATA/...</code>.</p> <p>The harvester post-processor follows this path to the Sentinel-2 scene and uses stactools (with built-in support for Sentinel-2) to establish a STAC item representing the product. This includes enumeration of <code>assets</code> for <code>inspire-metadata</code> and <code>product-metadata</code> - which are used by the registrar pycsw backend to embelesh the product record metadata.</p> <p>Note</p> <p>The above description considers Sentinel-2 data. Similar considerations apply for Sentinel-1 that is also detailed in the above harvester configuration.</p> <p>The harvester outputs the STAC item for each product, which is pushed to the registrar via the <code>register</code> redis queue.</p>"},{"location":"quickstart/creodias-deployment/#registration","title":"Registration","text":"<p>The registrar is configured at deployment to have the access details for the CREODIAS data in S3\u2026</p> <pre><code>global:\n  storage:\n    data:\n      data:\n        type: S3\n        endpoint_url: http://data.cloudferro.com\n        access_key_id: access\n        secret_access_key: access\n        region_name: RegionOne\n        validate_bucket_name: false\n</code></pre> <p>Using this S3 configuration, the registrar pycsw backend uses the product metadata linked in the STAC item (ref. assets <code>inspire-metadata</code> and <code>product-metadata</code>) to embelesh the metadata. For example, <code>product-metadata</code> in the file\u2026</p> <pre><code>s3://EODATA/Sentinel-2/MSI/L1C/2019/09/10/S2B_MSIL1C_20190910T095029_N0208_R079_T33TXN_20190910T120910.SAFE/MTD_MSIL1C.xml\n</code></pre> <p>The registrar uses this information to create the ISO XML metadata that is loaded into the resource-catalogue.</p>"},{"location":"quickstart/creodias-deployment/#product-type","title":"Product Type","text":"<p>The registrar recognises the product as Sentinel-2 and so reads its metadata XML files to obtain additional information. From the metadata XML file (e.g. <code>MTD_MSIL1C.xml</code>) the registrar obtains the Product Type for each product from the field <code>&lt;PRODUCT_TYPE&gt;</code>\u2026</p> <pre><code>&lt;n1:Level-1C_User_Product&gt;\n  &lt;n1:General_Info&gt;\n    &lt;Product_Info&gt;\n      &lt;PRODUCT_TYPE&gt;S2MSI1C&lt;/PRODUCT_TYPE&gt;\n      ...\n    &lt;/Product_Info&gt;\n    ...\n  &lt;/n1:General_Info&gt;\n  ...\n&lt;n1:Level-1C_User_Product&gt;\n</code></pre>"},{"location":"quickstart/creodias-deployment/#resource-catalogue-collections","title":"Resource Catalogue Collections","text":"<p>The registrar (<code>eoepca/rm-data-access-core</code>) container image is pre-loaded with two collections at the path <code>/registrar_pycsw/registrar_pycsw/resources</code>, (in the built container the files are at the path <code>/usr/local/lib/python3.8/dist-packages/registrar_pycsw/resources/</code>):</p> <ul> <li>S2MSI1C.yml - identifier: <code>S2MSI1C</code></li> <li>S2MSI2A.yml - identifier: <code>S2MSI2A</code></li> </ul> <p>The registrar applies these collections into the resource-catalogue during start-up - to create pre-defined out-of-the-box collections in pycsw.</p> <p>During registration, the <code>PycswBackend</code> of the registrar uses the Product Type to map the product into the collection of the same name - using metadata field <code>parentidentifier</code>.</p>"},{"location":"quickstart/creodias-deployment/#data-specification","title":"Data Specification","text":"<p>The data-access service data handling is configured by definition of <code>productTypes</code>, <code>collections</code> and <code>layers</code>\u2026</p> <ul> <li><code>productTypes</code> identify the underlying file assets as WCS coverages and their visual representation</li> <li><code>collections</code> provide groupings into which products are organised</li> <li><code>layers</code> specifies the hoe the product visual representations are exposed through the WMS service</li> </ul>"},{"location":"quickstart/creodias-deployment/#producttype","title":"<code>productType</code>","text":"<p>During registration, products are mapped into a <code>productType</code> via a <code>filter</code> that is applied against the STAC item metadata.</p> <p>The registrar uses the <code>product_type</code> of each product to determine the <code>collection</code> into which the product should be registered - noting that the <code>name</code> of the product type does not take part in the matching logic (and hence can be any text name)\u2026</p> <pre><code>  productTypes:\n    - name: S2MSI1C\n      filter:\n        s2:product_type: S2MSI1C\n</code></pre> <p>In the above example, the field <code>s2:product_type</code> is populated by the <code>stactools</code> that prepares the STAC item from the product metadata.</p>"},{"location":"quickstart/creodias-deployment/#producttype-coverages","title":"<code>productType</code> - <code>coverages</code>","text":"<p><code>coverages</code> defines the coverages for the WCS service. Each coverage links to the <code>assets</code> that are defined within the product STAC item.</p>"},{"location":"quickstart/creodias-deployment/#producttype-browses","title":"<code>productType</code> - <code>browses</code>","text":"<p><code>browses</code> defines the images that are visualised in the View Server Client. Expressions are used to map the product assets into their visual representation.</p>"},{"location":"quickstart/creodias-deployment/#collections","title":"<code>collections</code>","text":"<p>Collections are defined by reference to the defined <code>productTypes</code> and <code>coverages</code>.</p>"},{"location":"quickstart/creodias-deployment/#layers","title":"<code>layers</code>","text":"<p><code>layers</code> defines the layers that are presented through the WMS service - each layer being linked to the underlying <code>browse</code> that provides the image source. Layers are defined via their <code>id</code> that relies upon the naming convection <code>&lt;collection&gt;__&lt;browse&gt;</code> to identify the browse and so define the layer.</p>"},{"location":"quickstart/creodias-deployment/#example-configuration","title":"Example Configuration","text":"<p>Example configuration for Sentinel-2 L1C and L2A data.</p> <pre><code>global:\n  layers:\n    - id: S2L1C\n      title: Sentinel-2 Level 1C True Color\n      abstract: Sentinel-2 Level 2A True Color\n      displayColor: '#eb3700'\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L1C\n    - id: S2L1C__TRUE_COLOR\n      title: Sentinel-2 Level 1C True Color\n      abstract: Sentinel-2 Level 2A True Color\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L1C\n    - id: S2L1C__masked_clouds\n      title: Sentinel-2 Level 1C True Color with cloud masks\n      abstract: Sentinel-2 Level 1C True Color with cloud masks\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L1C\n    - id: S2L1C__FALSE_COLOR\n      title: Sentinel-2 Level 1C False Color\n      abstract: Sentinel-2 Level 1C False Color\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L1C\n    - id: S2L1C__NDVI\n      title: Sentinel-2 Level 21CNDVI\n      abstract: Sentinel-2 Level 1C NDVI\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L1C\n    - id: S2L2A\n      title: Sentinel-2 Level 2A True Color\n      abstract: Sentinel-2 Level 2A True Color\n      displayColor: '#eb3700'\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L2A\n    - id: S2L2A__TRUE_COLOR\n      title: Sentinel-2 Level 2A True Color\n      abstract: Sentinel-2 Level 2A True Color\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L2A\n    - id: S2L2A__masked_clouds\n      title: Sentinel-2 Level 2A True Color with cloud masks\n      abstract: Sentinel-2 Level 2A True Color with cloud masks\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L2A\n    - id: S2L2A__FALSE_COLOR\n      title: Sentinel-2 Level 2A False Color\n      abstract: Sentinel-2 Level 2A False Color\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L2A\n    - id: S2L2A__NDVI\n      title: Sentinel-2 Level 2A NDVI\n      abstract: Sentinel-2 Level 2A NDVI\n      grids:\n        - name: WGS84\n          zoom: 13\n      parentLayer: S2L2A\n  collections:\n    S2L1C:\n      product_types:\n        - S2MSI1C\n      coverage_types:\n        - S2L1C_B01\n        - S2L1C_B02\n        - S2L1C_B03\n        - S2L1C_B04\n        - S2L1C_B05\n        - S2L1C_B06\n        - S2L1C_B07\n        - S2L1C_B08\n        - S2L1C_B8A\n        - S2L1C_B09\n        - S2L1C_B10\n        - S2L1C_B11\n        - S2L1C_B12\n    S2L2A:\n      product_types:\n        - S2MSI2A\n      product_levels:\n        - Level-2A\n      coverage_types:\n        - S2L2A_B01\n        - S2L2A_B02\n        - S2L2A_B03\n        - S2L2A_B04\n        - S2L2A_B05\n        - S2L2A_B06\n        - S2L2A_B07\n        - S2L2A_B08\n        - S2L2A_B8A\n        - S2L2A_B09\n        - S2L2A_B11\n        - S2L2A_B12\n  productTypes:\n    - name: S2MSI1C\n      filter:\n        s2:product_type: S2MSI1C\n      metadata_assets: []\n      coverages:\n        S2L1C_B01:\n          assets:\n            - B01\n        S2L1C_B02:\n          assets:\n            - B02\n        S2L1C_B03:\n          assets:\n            - B03\n        S2L1C_B04:\n          assets:\n            - B04\n        S2L1C_B05:\n          assets:\n            - B05\n        S2L1C_B06:\n          assets:\n            - B06\n        S2L1C_B07:\n          assets:\n            - B07\n        S2L1C_B08:\n          assets:\n            - B08\n        S2L1C_B8A:\n          assets:\n            - B8A\n        S2L1C_B09:\n          assets:\n            - B09\n        S2L1C_B10:\n          assets:\n            - B10\n        S2L1C_B11:\n          assets:\n            - B11\n        S2L1C_B12:\n          assets:\n            - B12\n      defaultBrowse: TRUE_COLOR\n      browses:\n        TRUE_COLOR:\n          asset: visual\n          red:\n            expression: B04\n            range: [0, 4000]\n            nodata: 0\n          green:\n            expression: B03\n            range: [0, 4000]\n            nodata: 0\n          blue:\n            expression: B02\n            range: [0, 4000]\n            nodata: 0\n        FALSE_COLOR:\n          red:\n            expression: B08\n            range: [0, 4000]\n            nodata: 0\n          green:\n            expression: B04\n            range: [0, 4000]\n            nodata: 0\n          blue:\n            expression: B03\n            range: [0, 4000]\n            nodata: 0\n        NDVI:\n          grey:\n            expression: (B08-B04)/(B08+B04)\n            range: [-1, 1]\n      masks:\n        clouds:\n          validity: false\n    - name: S2MSI2A\n      filter:\n        s2:product_type: S2MSI2A\n      metadata_assets: []\n      coverages:\n        S2L2A_B01:\n          assets:\n            - B01\n        S2L2A_B02:\n          assets:\n            - B02\n        S2L2A_B03:\n          assets:\n            - B03\n        S2L2A_B04:\n          assets:\n            - B04\n        S2L2A_B05:\n          assets:\n            - B05\n        S2L2A_B06:\n          assets:\n            - B06\n        S2L2A_B07:\n          assets:\n            - B07\n        S2L2A_B08:\n          assets:\n            - B08\n        S2L2A_B8A:\n          assets:\n            - B8A\n        S2L2A_B09:\n          assets:\n            - B09\n        S2L2A_B11:\n          assets:\n            - B11\n        S2L2A_B12:\n          assets:\n            - B12\n      default_browse_locator: TCI_10m\n      browses:\n        TRUE_COLOR:\n          asset: visual-10m\n          red:\n            expression: B04\n            range: [0, 4000]\n            nodata: 0\n          green:\n            expression: B03\n            range: [0, 4000]\n            nodata: 0\n          blue:\n            expression: B02\n            range: [0, 4000]\n            nodata: 0\n        FALSE_COLOR:\n          red:\n            expression: B08\n            range: [0, 4000]\n            nodata: 0\n          green:\n            expression: B04\n            range: [0, 4000]\n            nodata: 0\n          blue:\n            expression: B03\n            range: [0, 4000]\n            nodata: 0\n        NDVI:\n          grey:\n            expression: (B08-B04)/(B08+B04)\n            range: [-1, 1]\n      masks:\n        clouds:\n          validity: false\n</code></pre>"},{"location":"quickstart/data-access-deployment/","title":"Data Access Deployment","text":"<p>A deployment wrapper script has been prepared for a \u2018data access\u2019 deployment - that is focused on the Resource Catalogue and Data Access services.</p> <p>The script <code>deploy/data-access/data-access</code> achieves this by appropriate configuration of the environment variables, before launching the eoepca.sh deployment script. The deployment configuration is captured in the file <code>deploy/data-access/data-access-options</code>.</p> <p>The data-access deployment applies the following configuration:</p> <ul> <li>Assumes a private deployment - i.e. no external-facing IP/ingress, and hence no TLS To configure an external-facing deployment with TLS protection, then see section Public Deployment</li> <li>No TLS for service ingress endpoints</li> <li>Services deployed:<ul> <li>Resource Catalogue for data discovery</li> <li>Data Access for data visualisation and download</li> </ul> </li> <li>Includes data specification for CREODIAS Sentinel-2, which can be exploited if running in a CREODIAS VM connected to the <code>eodata</code> network - see description of variable <code>CREODIAS_DATA_SPECIFICATION</code></li> <li>Open ingress are enabled for unauthenticated access to resource-catalogue and data-access services</li> <li>Other eoepca services not deployed</li> </ul>"},{"location":"quickstart/data-access-deployment/#initiate-deployment","title":"Initiate Deployment","text":"<p>Deployment is initiated by invoking the script\u2026</p> <pre><code>./deploy/data-access/data-access\n</code></pre> <p>The Resource Catalogue is accessed at the endpoint <code>resource-catalogue-open.&lt;domain&gt;</code> - e.g. <code>resource-catalogue-open.192-168-49-2.nip.io</code>.</p> <p>The Data Access View Server is accessed at the endpoint <code>data-access-open.&lt;domain&gt;</code> - e.g. <code>data-access-open.192-168-49-2.nip.io</code>.</p>"},{"location":"quickstart/data-access-deployment/#data-harvesting","title":"Data Harvesting","text":"<p>See section Harvest CREODIAS Data to harvest the default data specification from the CREODIAS data offering.</p>"},{"location":"quickstart/exploitation-deployment/","title":"Exploitation Deployment","text":"<p>A deployment wrapper script has been prepared for an \u2018exploitation\u2019 deployment - that provides deployment/execution of processing via the ADES, supported by Resource Catalogue and Data Access services.</p> <p>The script <code>deploy/exploitation/exploitation</code> achieves this by appropriate configuration of the environment variables, before launching the eoepca.sh deployment script. The deployment configuration is captured in the file <code>deploy/exploitation/exploitation-options</code>.</p> <p>The exploitation deployment applies the following configuration:</p> <ul> <li>Assumes a private deployment - i.e. no external-facing IP/ingress, and hence no TLS To configure an external-facing deployment with TLS protection, then see section Public Deployment</li> <li>No TLS for service ingress endpoints</li> <li>Services deployed:<ul> <li>ADES for processing</li> <li>Resource Catalogue for data discovery</li> <li>Data Access for data visualisation and download</li> <li>Minio for S3 object storage</li> </ul> </li> <li>ADES stage-out to Minio</li> <li>Includes data specification for CREODIAS Sentinel-2, which can be exploited if running in a CREODIAS VM connected to the <code>eodata</code> network - see description of variable <code>CREODIAS_DATA_SPECIFICATION</code></li> <li>Open ingress are enabled for unauthenticated access to ADES, resource-catalogue and data-access services</li> <li>Other eoepca services not deployed</li> </ul>"},{"location":"quickstart/exploitation-deployment/#initiate-deployment","title":"Initiate Deployment","text":"<p>Deployment is initiated by invoking the script\u2026</p> <pre><code>./deploy/exploitation/exploitation\n</code></pre> <p>The ADES service is accessed at the endpoint <code>ades-open.&lt;domain&gt;</code> - e.g. <code>ades-open.192-168-49-2.nip.io</code>.</p> <p>The Resource Catalogue is accessed at the endpoint <code>resource-catalogue-open.&lt;domain&gt;</code> - e.g. <code>resource-catalogue-open.192-168-49-2.nip.io</code>.</p> <p>The Data Access View Server is accessed at the endpoint <code>data-access-open.&lt;domain&gt;</code> - e.g. <code>data-access-open.192-168-49-2.nip.io</code>.</p>"},{"location":"quickstart/exploitation-deployment/#example-requests-s-expression-on-creodias","title":"Example Requests - <code>s-expression</code> on CREODIAS","text":"<p>NOTE that this example processing request requires harvesting data from CREODIAS, which can only be performed if the deployment is made to a CREODIAS VM with access to the <code>eodata</code> network - see description of variable <code>CREODIAS_DATA_SPECIFICATION</code>.</p> <p>Section Processing provides an example of a simple self-contained processing deployment and execution, and access to the processing results.</p> <p>In addition to the <code>snuggs</code> example, the file <code>deploy/samples/requests/processing/s-expression.http</code> has been prepared to exploit data that has been registered within the Resource Catalogue and Data Access services.</p> <p>First the input data for processing must be harvested into the resource management services. Sentinel-2 data on 2nd Sept 2020\u2026</p> <pre><code>./deploy/bin/harvest ./deploy/samples/harvester/config-Sentinel2-2020.09.02.yaml\n</code></pre> <p>Then the <code>s-expression.http</code> file provides sample requests for OGC API Processes operations:</p> <ul> <li>List Processes</li> <li>Deploy Process</li> <li>Get Process Details</li> <li>Execute Process</li> <li>Get Job Status</li> <li>Get Job Results</li> </ul> <p>NOTE that the first requests in the file provide optional calls to obtain a user ID token (<code>openidConfiguration</code> / <code>authenticate</code>) - to be used in the case that protected (not \u2018open\u2019) endpoints are deployed.</p> <p>The file <code>snuggs.http</code> describes the HTTP requests for the ADES OGC API Processes endpoint, and is designed for use with the Visual Studio Code (vscode) extension REST Client. Install in vscode with <code>ext install humao.rest-client</code>.</p> <p>Various variables, such as to specify the <code>@domain</code> for your deployment, can be configured at the top of the file.</p> <p>At the completion of successful processing execution, the procesing results are obtained as described in section Processing Results.</p>"},{"location":"quickstart/exploitation-deployment/#data-harvesting","title":"Data Harvesting","text":"<p>See section Harvest CREODIAS Data to harvest the default data specification from the CREODIAS data offering.</p>"},{"location":"quickstart/processing-deployment/","title":"Processing Deployment","text":"<p>A deployment wrapper script has been prepared for a \u2018processing\u2019 deployment - that is focused on the ADES and the deployment/execution of processing jobs.</p> <p>The script <code>deploy/processing/processing</code> achieves this by appropriate configuration of the environment variables, before launching the eoepca.sh deployment script. The deployment configuration is captured in the file <code>deploy/processing/processing-options</code>.</p> <p>The processing deployment applies the following configuration:</p> <ul> <li>Assumes a private deployment - i.e. no external-facing IP/ingress, and hence no TLS To configure an external-facing deployment with TLS protection, then see section Public Deployment</li> <li>No TLS for service ingress endpoints</li> <li>Services deployed:<ul> <li>ADES for processing</li> <li>Minio for S3 object storage</li> </ul> </li> <li>ADES stage-out to Minio</li> <li>Open ingress are enabled for unauthenticated access to ADES service</li> <li>Other eoepca services not deployed</li> </ul>"},{"location":"quickstart/processing-deployment/#initiate-deployment","title":"Initiate Deployment","text":"<p>Deployment is initiated by invoking the script\u2026</p> <pre><code>./deploy/processing/processing\n</code></pre> <p>The ADES service is accessed at the endpoint <code>ades-open.&lt;domain&gt;</code> - e.g. <code>ades-open.192-168-49-2.nip.io</code>.</p>"},{"location":"quickstart/processing-deployment/#example-requests","title":"Example Requests","text":"<p>Some sample requests have been prepared in the subdirectory <code>deploy/samples/requests/processing</code> - for example\u2026</p> <ul> <li><code>convert</code> Provides a \u2018hello world\u2019 processing example that can be used simply to check that the processing capability has been well deployed</li> <li><code>snuggs</code> Provides a packaged EO exploitation algorithm that perform \u2018real\u2019 work and, as such, is more resource demanding (10GB RAM, 3 CPU) - and so may not be suitable for execution within a local minikube deployment (depending on resource allocations)</li> </ul> <p>These sample <code>http</code> files have been prepared with sample requests for OGC API Processes operations:</p> <ul> <li>List Processes</li> <li>Deploy Process</li> <li>Get Process Details</li> <li>Execute Process</li> <li>Get Job Status</li> <li>Get Job Results</li> </ul> <p>Note</p> <ul> <li>The first requests in the file provide optional calls to obtain a user ID token (<code>openidConfiguration</code> / <code>authenticate</code>). These are to be used in the case that protected (not \u2018open\u2019) endpoints are deployed.</li> <li>The file describes the HTTP requests for the ADES OGC API Processes endpoint, and is designed for use with the Visual Studio Code (vscode) extension REST Client. Install in vscode with <code>ext install humao.rest-client</code>.</li> <li>The variables <code>@hostname</code> and <code>@domain</code> can be configured at the top of the file.</li> </ul>"},{"location":"quickstart/processing-deployment/#alternative-curl-commands","title":"Alternative <code>curl</code> Commands","text":"<p>Alternatively the following <code>curl</code> commands can be used\u2026</p> List Processes <pre><code>curl -k \\\n  --request GET \\\n  --url https://ades-open.192-168-49-2.nip.io/eric/wps3/processes \\\n  --header 'accept: application/json'\n</code></pre> Deploy &amp; Execute (<code>convert</code>) Deploy Process (<code>convert</code>) <pre><code>curl -k \\\n  --request POST \\\n  --url https://ades-open.192-168-49-2.nip.io/eric/wps3/processes \\\n  --header 'accept: application/json' \\\n  --header 'content-type: application/json' \\\n  --data '{\"executionUnit\": {\"href\": \"https://raw.githubusercontent.com/EOEPCA/convert/main/convert-url-app.cwl\",\"type\": \"application/cwl\"}}'\n</code></pre> Get Process Details (<code>convert</code>) <pre><code>curl -k \\\n  --request GET \\\n  --url https://ades-open.192-168-49-2.nip.io/eric/wps3/processes/convert-url-0_1_2 \\\n  --header 'accept: application/json'\n</code></pre> Execute Process (<code>convert</code>) <pre><code>curl -k -v \\\n  --request POST \\\n  --url https://ades-open.192-168-49-2.nip.io/eric/wps3/processes/convert-url-0_1_2/execution \\\n  --header 'accept: application/json' \\\n  --header 'content-type: application/json' \\\n  --header 'prefer: respond-async' \\\n  --data '{\"inputs\": {\"fn\":  \"resize\",\"url\": \"https://eoepca.org/media_portal/images/logo6_med.original.png\", \"size\": \"50%\"},\"response\":\"raw\"}'\n</code></pre> Deploy &amp; Execute (<code>snuggs</code>) Deploy Process (<code>snuggs</code>) <pre><code>curl -k \\\n  --request POST \\\n  --url https://ades-open.192-168-49-2.nip.io/eric/wps3/processes \\\n  --header 'accept: application/json' \\\n  --header 'content-type: application/json' \\\n  --data '{\"executionUnit\": {\"href\": \"https://raw.githubusercontent.com/EOEPCA/deployment-guide/eoepca-v1.3/deploy/samples/requests/processing/snuggs.cwl\",\"type\": \"application/cwl\"}}'\n</code></pre> Get Process Details (<code>snuggs</code>) <pre><code>curl -k \\\n  --request GET \\\n  --url https://ades-open.192-168-49-2.nip.io/eric/wps3/processes/snuggs-0_3_0 \\\n  --header 'accept: application/json'\n</code></pre> Execute Process (<code>snuggs</code>) <pre><code>curl -k -v \\\n  --request POST \\\n  --url https://ades-open.192-168-49-2.nip.io/eric/wps3/processes/snuggs-0_3_0/execution \\\n  --header 'accept: application/json' \\\n  --header 'content-type: application/json' \\\n  --header 'prefer: respond-async' \\\n  --data '{\"inputs\": {\"input_reference\":  \"https://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs/items/S2B_36RTT_20191205_0_L2A\",\"s_expression\": \"ndvi:(/ (- B05 B03) (+ B05 B03))\"},\"response\":\"raw\"}'\n</code></pre> Get Job Status <p>This request requires the <code>Location</code> header from the response to the execute request. This will be of the form <code>/{user}/wps3/jobs/{job-id}</code> - e.g. <code>/eric/wps3/jobs/7b58bc38-64d4-11ed-b962-0242ac11000e</code>.</p> <pre><code>curl -k \\\n  --request GET \\\n  --url https://ades-open.192-168-49-2.nip.io{location-header} \\\n  --header 'accept: application/json'\n</code></pre> Get Job Results <p>This request uses the same URL as <code>Get Job Status</code>, with the additional URL path <code>/result</code> - i.e. <code>/{user}/wps3/jobs/{job-id}/result</code> - e.g. <code>/eric/wps3/jobs/7b58bc38-64d4-11ed-b962-0242ac11000e/result</code></p> <pre><code>curl -k \\\n  --request GET \\\n  --url https://ades-open.192-168-49-2.nip.io{location-header}/result \\\n  --header 'accept: application/json'\n</code></pre> <p>The response indicates the location of the results, which should be in the <code>minio</code> object storage. See Processing Results </p> List Jobs <pre><code>curl -k \\\n  --request GET \\\n  --url https://ades-open.192-168-49-2.nip.io/eric/wps3/jobs \\\n  --header 'accept: application/json'\n</code></pre>"},{"location":"quickstart/processing-deployment/#processing-results","title":"Processing Results","text":"<p>The outputs are published as a static STAC catalogue to a path that includes the unique job ID.</p> <p>In the default configuration, the processing results are pushed to the Minio S3 object storage. This can be checked via browser access at the endpoint <code>console.minio.&lt;domain&gt;</code> e.g. https://console.minio.192-168-49-2.nip.io/, or using an S3 client such as\u2026</p> <pre><code>s3cmd -c ./deploy/cluster/s3cfg ls s3://eoepca\n</code></pre> <p>For the default credentials to connect to Minio see Minio Object Storage Default Credentials. </p>"},{"location":"quickstart/quickstart/","title":"Quick Start","text":"<p>The deployment of the EOEPCA components and the supporting Kubernetes cluster is described in the sections Cluster and EOEPCA. These sections should be consulted for more detailed information.</p>"},{"location":"quickstart/quickstart/#scripted-deployment","title":"Scripted Deployment","text":"<p>As a companion to these descriptions, we have developed a set of scripts to provide a demonstration of an example deployment. This is described in the following section Scripted Deployment.</p> <p>Note</p> <p>The scripted deployment assumes that installation of the Prerequisite Tooling has been performed</p>"},{"location":"quickstart/quickstart/#customised-deployments","title":"Customised Deployments","text":"<p>The Scripted Deployment can be quickly exploited through the following customisations for particular use cases:</p> <ul> <li>Simple Basic local deployment</li> <li>Processing Deployment focused on processing</li> <li>Data Access Deployment focused on the Resource Catalogue and Data Access services</li> <li>Exploitation Deployment providing deployment/execution of processing via the ADES, supported by Resource Catalogue and Data Access services</li> <li>User Management Deployment focused on the User Management services</li> <li>CREODIAS Deployment with access to CREODIAS EO data</li> </ul> <p>Each customisation is introduced in their respective sections.</p>"},{"location":"quickstart/quickstart/#quick-example","title":"Quick Example","text":"<p>Follow these steps to create a simple local deployment in minikube\u2026</p> <ol> <li>Prerequisite Tooling    Follow the steps in section Prerequisite Tooling to install the required tooling.</li> <li>Clone the repository <code>git clone https://github.com/EOEPCA/deployment-guide</code></li> <li>Initiate the deployment <pre><code>cd deployment-guide\n./deploy/simple/simple\n</code></pre></li> <li>Wait for deployment ready<ol> <li>List pod status <code>watch kubectl get pod -A</code></li> <li>Wait until all pods report either <code>Running</code> or <code>Completed</code> This may take 10-20 mins depending on the capabilities of your platform.</li> </ol> </li> <li>Test the deployment    Make the sample requests to the ADES processing service.</li> </ol>"},{"location":"quickstart/scripted-deployment/","title":"Scripted Deployment","text":"<p>The Scripted Deployment provides a demonstration of an example deployment, and can found in the subdirectory <code>deployment-guide/deploy</code> of the source repository for this guide\u2026</p> <pre><code>git clone https://github.com/EOEPCA/deployment-guide \\\n&amp;&amp; cd deployment-guide \\\n&amp;&amp; ls deploy\n</code></pre> <p>The script <code>deploy/eoepca/eoepca.sh</code> acts as an entry-point to the full system deployment. In order to tailor the deployment for your target environment, the script is configured through environment variables and command-line arguments. By default the script assumes deployment to a local minikube.</p> <p>Note</p> <p>The scripted deployment assumes that installation of the Prerequisite Tooling has been performed.</p> <p>The following subsections lead through the steps for a full local deployment. Whilst minikube is assumed, minimal adaptions are required to make the deployment to your existing Kubernetes cluster.</p> <p>The deployment follows these broad steps:</p> <ul> <li>Configuration   Tailoring of deployment options.</li> <li>Deployment   Creation of cluster and deployment of eoepca services.</li> <li>Protection   Application of protection for authorized access to services.</li> </ul> <p>The Protection step is split from Deployment as there are some manual steps to be performed before the Protection can be applied.</p>"},{"location":"quickstart/scripted-deployment/#configuration","title":"Configuration","text":"<p>The script <code>deploy/eoepca/eoepca.sh</code> is configured by some environment variables and command-line arguments.</p>"},{"location":"quickstart/scripted-deployment/#environment-variables","title":"Environment Variables","text":"Environment Variables Variable Description Default REQUIRE_&lt;cluster-component&gt; A set of variables that can be used to control which CLUSTER components are deployed by the script, as follows (with defaults):<code>REQUIRE_MINIKUBE=true</code><code>REQUIRE_INGRESS_NGINX=true</code><code>REQUIRE_CERT_MANAGER=true</code><code>REQUIRE_LETSENCRYPT=true</code><code>REQUIRE_SEALED_SECRETS=false</code><code>REQUIRE_MINIO=false</code> see description REQUIRE_&lt;eoepca-component&gt; A set of variables that can be used to control which EOEPCA components are deployed by the script, as follows (with defaults):<code>REQUIRE_STORAGE=true</code><code>REQUIRE_DUMMY_SERVICE=false</code><code>REQUIRE_LOGIN_SERVICE=true</code><code>REQUIRE_PDP=true</code><code>REQUIRE_USER_PROFILE=true</code><code>REQUIRE_ADES=true</code><code>REQUIRE_RESOURCE_CATALOGUE=true</code><code>REQUIRE_DATA_ACCESS=true</code><code>REQUIRE_REGISTRATION_API=true</code><code>REQUIRE_WORKSPACE_API=true</code><code>REQUIRE_HARBOR=true</code><code>REQUIRE_PORTAL=true</code><code>REQUIRE_APPLICATION_HUB=true</code> see description REQUIRE_&lt;protection-component&gt; A set of variables that can be used to control which PROTECTION components are deployed by the script, as follows (with defaults):<code>REQUIRE_DUMMY_SERVICE_PROTECTION=false</code><code>REQUIRE_ADES_PROTECTION=true</code><code>REQUIRE_RESOURCE_CATALOGUE_PROTECTION=true</code><code>REQUIRE_DATA_ACCESS_PROTECTION=true</code><code>REGISTRATION_API_PROTECTION=true</code><code>REQUIRE_WORKSPACE_API_PROTECTION=true</code> see description MINIKUBE_VERSION The Minikube version to be (optionally) installedNote that the EOEPCA development has been conducted using the default stated here. <code>v1.32.0</code> MINIKUBE_KUBERNETES_VERSION The Kubernetes version to be used by minikubeNote that the EOEPCA development has been conducted primarily using version 1.22.5. <code>v1.22.5</code> MINIKUBE_MEMORY_AMOUNT Amount of memory to allocate to the docker containers used by minikube to implement the cluster. <code>12g</code> MINIKUBE_DISK_AMOUNT Amount of disk space to allocate to the docker containers used by minikube to implement the cluster. <code>20g</code> MINIKUBE_EXTRA_OPTIONS Additional options to pass to <code>minikube start</code> command-line <code>--ports=80:80,443:443</code> USE_METALLB Enable use of minikube\u2019s built-in load-balancer.The load-balancer can be used to facilitate exposing services publicly. However, the same can be achieved using minikube\u2019s built-in ingress-controller. Therefore, this option is suppressed by default. <code>false</code> USE_INGRESS_NGINX_HELM Install the ingress-nginx controller using the published helm chart, rather than relying upon the version that is built-in to minikube. By default we prefer the version that is built in to minikube. <code>false</code> USE_INGRESS_NGINX_LOADBALANCER Patch the built-in minikube nginx-ingress-controller to offer a service of type <code>LoadBalancer</code>, rather than the default <code>NodePort</code>. It was initially thought that this would be necessary to achieve public access to the ingress services - but was subsequently found that the default <code>NodePort</code> configuration of the ingress-controller was sufficient. This option is left in case it proves useful.Only applicable for <code>USE_INGRESS_NGINX_HELM=false</code> (i.e. when using the minikube built-in ) <code>false</code> OPEN_INGRESS Create \u2018open\u2019 ingress endpoints that are not subject to authorization protection. For a secure system the open endpoints should be disabled (<code>false</code>) and access to resource should be protected via ingress that apply protection <code>false</code> USE_TLS Indicates whether TLS will be configured for service <code>Ingress</code> rules.If not (i.e. <code>USE_TLS=false</code>), then the ingress-controller is configured to disable <code>ssl-redirect</code>, and <code>TLS_CLUSTER_ISSUER=notls</code> is set. <code>true</code> TLS_CLUSTER_ISSUER The name of the ClusterIssuer to satisfy ingress tls certificates.Out-of-the-box ClusterIssuer instances are configured in the file <code>deploy/cluster/letsencrypt.sh</code>. <code>letsencrypt-staging</code> LOGIN_SERVICE_ADMIN_PASSWORD Initial password for the <code>admin</code> user in the login-service. <code>changeme</code> MINIO_ROOT_USER Name of the \u2018root\u2019 user for the Minio object storage service. <code>eoepca</code> MINIO_ROOT_PASSWORD Password for the \u2018root\u2019 user for the Minio object storage service. <code>changeme</code> HARBOR_ADMIN_PASSWORD Password for the \u2018admin\u2019 user for the Harbor artefact registry service. <code>changeme</code> DEFAULT_STORAGE Storage Class to be used by default for all components requiring dynamic storage provisioning.See variables <code>&lt;component&gt;_STORAGE</code> for per-component overrides. <code>standard</code> &lt;component&gt;_STORAGE A set of variables to control the dynamic provisioning Storage Class for individual components, as follows:MINIO_STORAGEADES_STORAGEAPPLICATION_HUB_STORAGEDATA_ACCESS_STORAGEHARBOR_STORAGERESOURCE_CATALOGUE_STORAGE <code>&lt;DEFAULT_STORAGE&gt;</code> PROCESSING_MAX_RAM Max RAM allocated to an individual processing job <code>8Gi</code> PROCESSING_MAX_CORES Max number of CPU cores allocated to an individual processing job <code>4</code> STAGEOUT_TARGET Configures the ADES with the destination to which it should push processing results:<code>workspace</code> - via the Workspace API<code>minio</code> - to minio S3 object storage <code>workspace</code> INSTALL_FLUX The Workspace API relies upon Flux CI/CD, and has the capability to install the required flux components to the cluster. If your deployment already has flux installed then set this value <code>false</code> to suppress the Workspace API flux install <code>true</code> CREODIAS_DATA_SPECIFICATION Apply the data specification to harvest from the CREODIAS data offering into the resource-catalogue and data-access services.Can only be used when running in the CREODIAS (Cloudferro) cloud, with access to the <code>eodata</code> network. <code>false</code>"},{"location":"quickstart/scripted-deployment/#command-line-arguments","title":"Command-line Arguments","text":"<p>The eoepca.sh script is further configured via command-line arguments\u2026</p> <pre><code>eoepca.sh &lt;action&gt; &lt;cluster-name&gt; &lt;public-ip&gt; &lt;domain&gt;\n</code></pre> <code>eoepca.sh</code> Command-line Arguments Argument Description Default action Action to perform: <code>apply</code> | <code>delete</code> | <code>template</code>.<code>apply</code> makes the deployment<code>delete</code> removes the deployment<code>template</code> outputs generated kubernetes yaml to stdout <code>apply</code> cluster-name The name of the minikube \u2018profile\u2019 for the created minikube cluster <code>eoepca</code> public-ip The public IP address through which the deployment is exposed via the ingress-controller.By default, the value is deduced from the assigned cluster minikube IP address - ref. command <code>minikube ip</code>. <code>&lt;minikube-ip&gt;</code> domain The DNS domain name through which the deployment is accessed. Forms the stem for all service hostnames in the ingress rules - i.e. <code>&lt;service-name&gt;.&lt;domain&gt;</code>.By default, the value is deduced from the assigned cluster minikube IP address, using <code>nip.io</code> to establish a DNS lookup - i.e. <code>&lt;minikube ip&gt;.nip.io</code>. <code>&lt;minikube ip&gt;.nip.io</code>"},{"location":"quickstart/scripted-deployment/#public-deployment","title":"Public Deployment","text":"<p>For simplicity, the out-of-the-box scripts assume a \u2018private\u2019 deployment - with no public IP / DNS and hence no use of TLS for service ingress endpoints.</p> <p>In the case that an external-facing public deployment is desired, then the following configuration selections should be made:</p> <ul> <li><code>public_ip</code> - set to the external-facing public IP of your deployment, e.g. the floating IP of your load-balancer in a cloud deployment</li> <li><code>domain</code> - set to the domain (as per DNS records) for your deployment Note that the EOEPCA components typically configure their ingress with hostname prefixes applied to this <code>domain</code>. Thus, it is necessary that the DNS record for the domain is established as a wildcard record - i.e. <code>*.&lt;domain&gt;</code></li> <li><code>USE_TLS=true</code> - to enable configuration of TLS endpoints in each component service ingress</li> <li><code>TLS_CLUSTER_ISSUER=&lt;issuer&gt;</code> - should be configured ~ e.g. using the <code>letsencrypt-production</code> or <code>letsencrypt-staging</code> (testing only) Cluster Issuer that are configured by the scripted deployment</li> </ul>"},{"location":"quickstart/scripted-deployment/#deployment","title":"Deployment","text":"<p>The deployment is initiated by setting the appropriate environment variables and invoking the <code>eoepca.sh</code> script with suitable command-line arguments. You may find it convenient to do so using a wrapper script that customises the environment varaibles according to your cluster, and then invokes the <code>eoepca.sh</code> script.</p> <p>Customised examples are provided for Simple, CREODIAS and Processing deployments.</p> <p>NOTE that if a prior deployment has been attempted then, before redeploying, a clean-up should be performed as described in the Clean-up section below. This is particularly important in the case that the minikube <code>none</code> driver is used, as the persistence is maintained on the host and so is not naturally removed when the minikube cluster is destroyed.</p> <p>Initiate the deployment\u2026 <pre><code>./deploy/eoepca/eoepca.sh apply \"&lt;cluster-name&gt;\" \"&lt;public-ip&gt;\" \"&lt;domain&gt;\"\n</code></pre></p> <p>The deployment takes 10+ minutes - depending on the resources of your host/cluster. The progress can be monitored\u2026 <pre><code>kubectl get pods -A\n</code></pre></p> <p>The deployment is ready once all pods are either <code>Running</code> or <code>Completed</code>. This can be further confirmed by accessing the login-service web interface at <code>https://auth.&lt;domain&gt;/</code> and logging in as user <code>admin</code> using the credentials configured via <code>LOGIN_SERVICE_ADMIN_PASSWORD</code>.</p>"},{"location":"quickstart/scripted-deployment/#post-deployment-manual-steps","title":"Post-deployment Manual Steps","text":"<p>The scripted deployment has been designed, as far as possible, to automate the configuration of the deployed components. However, there remain some steps that must be performed manually after the scripted deployment has completed. See the building block specific pages\u2026</p> <ul> <li>Login Service: Post-deployment Manual Steps</li> </ul> <p>Note</p> <p>See also Post-protection Manual Steps for additional interventions to be performed later in the process.</p>"},{"location":"quickstart/scripted-deployment/#default-credentials","title":"Default Credentials","text":""},{"location":"quickstart/scripted-deployment/#login-service","title":"Login Service","text":"<p>By default, the Login Service is accessed at the URL <code>https://auth.&lt;domain&gt;/</code> with the credentials\u2026</p> <pre><code>username: admin\npassword: Chang3me!\n</code></pre> <p>\u2026unless the password is overridden via the variable <code>LOGIN_SERVICE_ADMIN_PASSWORD</code>.</p>"},{"location":"quickstart/scripted-deployment/#minio-object-storage","title":"Minio Object Storage","text":"<p>By default, Minio is accessed at the URL <code>http://console.minio.&lt;domain&gt;/</code> with the credentials\u2026</p> <pre><code>username: eoepca\npassword: changeme\n</code></pre> <p>\u2026unless the username/password are overridden via the variables <code>MINIO_ROOT_USER</code> and <code>MINIO_ROOT_PASSWORD</code>.</p>"},{"location":"quickstart/scripted-deployment/#harbor-container-registry","title":"Harbor Container Registry","text":"<p>By default, Harbor is accessed at the URL <code>https://harbor.&lt;domain&gt;/</code> with the credentials\u2026</p> <pre><code>username: admin\npassword: changeme\n</code></pre> <p>\u2026unless the password is overridden via the variable <code>HARBOR_ADMIN_PASSWORD</code>.</p>"},{"location":"quickstart/scripted-deployment/#protection","title":"Protection","text":"<p>The protection of resource server endpoints is applied with the script <code>deploy/eoepca/eoepca-protection.sh</code>. This script should be executed with environment variables and command-line options that are consistent with those of the main deployment (ref. script <code>eoepca.sh</code>).</p> <p>The script <code>eoepca-protection.sh</code> introduces two users <code>eric</code> and <code>bob</code> to demonstrate the application of authorized access to various service endpoints: ADES, Workspace API and dummy-service (simple endpoint used for debugging).</p> <p>Thus, the users must first be created in the login-service and their unique IDs passed to the protection script.</p> <pre><code>Usage: eoepca-protection.sh &lt;action&gt; &lt;eric-id&gt; &lt;bob-id&gt; &lt;public-ip&gt; &lt;domain&gt;\n</code></pre>"},{"location":"quickstart/scripted-deployment/#create-test-users","title":"Create Test Users","text":"<p>Access the login-service web interface (<code>https://auth.&lt;domain&gt;/</code>) as user admin using the credentials configured via <code>LOGIN_SERVICE_ADMIN_PASSWORD</code>.</p> <p>Select <code>Users -&gt; Add person</code> to add users <code>eric</code> and <code>bob</code> (dummy details can be used). Note the <code>Inum</code> (unique user ID) for each user for use with the <code>eoepca-protection.sh</code> script.</p>"},{"location":"quickstart/scripted-deployment/#apply-protection","title":"Apply Protection","text":"<p>Apply the protection\u2026 Ensure that the script is executed with the environment variables and command-line options that are consistent with those of the main deployment.</p> <pre><code>./deploy/eoepca/eoepca-protection.sh apply \"&lt;eric-id&gt;\" \"&lt;bob-id&gt;\" \"&lt;public-ip&gt;\" \"&lt;domain&gt;\"\n</code></pre>"},{"location":"quickstart/scripted-deployment/#post-protection-manual-steps","title":"Post-protection Manual Steps","text":"<p>The scripted deployment has been designed, as far as possible, to automate the configuration of the deployed components. However, there remain some steps that must be performed manually after the scripted deployment has completed. See the building block specific pages\u2026</p> <ul> <li>Application Hub: Post-deployment Manual Steps</li> </ul>"},{"location":"quickstart/scripted-deployment/#create-user-workspaces","title":"Create User Workspaces","text":"<p>The protection steps created the test users <code>eric</code> and <code>bob</code>. For completeness we use the Workspace API to create their user workspaces, which hold their personal resources (data, processing results, etc.) within the platform - see Workspace.</p>"},{"location":"quickstart/scripted-deployment/#using-workspace-swagger-ui","title":"Using Workspace Swagger UI","text":"<p>The Workspace API provides a Swagger UI that facilitates interaction with the API - at the URL <code>https://workspace-api.&lt;domain&gt;/docs#</code>. Access to The Workspace API is protected, such that the necessary access tokens must be supplied in requests, which is most easily achieved by logging in via the \u2018portal\u2019.</p> <p>The portal is accessed at <code>https://portal.&lt;domain&gt;/</code>. It is a rudimentary web service that facilitates establishing the appropriate tokens in the user\u2019s browser context. Login to the portal as the <code>admin</code> user, using the configured credentials.</p> <p>Access the Workspace Swagger UI at <code>https://workspace-api.&lt;domain&gt;/docs</code>. Workspaces are created using <code>POST  /workspaces</code> (Create Workspace). Expand the node and select <code>Try it out</code>. Complete the request body, such as\u2026 <pre><code>{\n  \"preferred_name\": \"eric\",\n  \"default_owner\": \"d95b0c2b-ea74-4b3f-9c6a-85198dec974d\"\n}\n</code></pre> \u2026where the <code>default_owner</code> is the user ID (<code>Inum</code>) for the user - thus protecting the created workspace for the identified user.</p>"},{"location":"quickstart/scripted-deployment/#using-curl","title":"Using <code>curl</code>","text":"<p>The same can be achieved with a straight http request, for example using <code>curl</code>\u2026</p> <pre><code>curl -X 'POST' \\\n  'https://workspace-api.192-168-49-2.nip.io/workspaces' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -H 'X-User-Id: &lt;admin-id-token&gt;' \\\n  -d '{\n  \"preferred_name\": \"&lt;workspace-name&gt;\",\n  \"default_owner\": \"&lt;user-inum&gt;\"\n}'\n</code></pre> <p>Values must be provided for:</p> <ul> <li><code>admin-id-token</code> - User ID token for the admin user</li> <li><code>workspace-name</code> - name of the workspace, typically the username</li> <li><code>user-inum</code> - the ID of the user for which the created workspace will be protected</li> </ul> <p>The ID token for the <code>admin</code> user can be obtained with a call to the token endpoint of the Login Service - supplying the credentials for the <code>admin</code> user and the pre-registered client\u2026</p> <pre><code>curl -L -X POST 'https://auth.&lt;domain&gt;/oxauth/restv1/token' \\\n  -H 'Cache-Control: no-cache' \\\n  -H 'Content-Type: application/x-www-form-urlencoded' \\\n  --data-urlencode 'scope=openid user_name is_operator' \\\n  --data-urlencode 'grant_type=password' \\\n  --data-urlencode 'username=admin' \\\n  --data-urlencode 'password=&lt;admin-password&gt;' \\\n  --data-urlencode 'client_id=&lt;client-id&gt;' \\\n  --data-urlencode 'client_secret=&lt;client-secret&gt;'\n</code></pre> <p>A json response is returned, in which the field <code>id_token</code> provides the user ID token for the <code>admin</code> user.</p>"},{"location":"quickstart/scripted-deployment/#using-create-workspace-helper-script","title":"Using <code>create-workspace</code> helper script","text":"<p>As an aide there is a helper script <code>create-workspace</code>. The script is available in the <code>deployment-guide</code> repository, and can be obtained as follows\u2026</p> <pre><code>git clone git@github.com:EOEPCA/deployment-guide\ncd deployment-guide\n</code></pre> <p>The <code>create-workspace</code> helper script requires some command-line arguments\u2026</p> <pre><code>Usage:\n  create-workspace &lt;domain&gt; &lt;user&gt; &lt;user-inum&gt; [&lt;client-id&gt; &lt;client-secret&gt;]\n</code></pre> <p>For example\u2026</p> <pre><code>./deploy/bin/create-workspace 192-168-49-2.nip.io eric d95b0c2b-ea74-4b3f-9c6a-85198dec974d\n</code></pre> <p>The script prompts for the password of the <code>admin</code> user.</p> <p>By default <code>&lt;client-id&gt;</code> and <code>&lt;client-secret&gt;</code> are read from the <code>client.yaml</code> file that is created by the deployment script, which auto-registers a Login Service client. Thus, these args can be ommited to use the default client credentials.</p>"},{"location":"quickstart/scripted-deployment/#clean-up","title":"Clean-up","text":"<p>Before initiating a fresh deployment, if a prior deployment has been attempted, then it is necessary to remove any persistent artefacts of the prior deployment. This includes\u2026</p> <ol> <li> <p>Minikube cluster   Delete the minikube cluster\u2026 <code>minikube delete</code>   If necessary specify the cluster (profile)\u2026 <code>minikube -p &lt;profile&gt; delete</code></p> </li> <li> <p>Persistent Data   In the case that the minikube <code>none</code> driver is used, the persistence is maintained on the host and so is not naturally removed when the minikube cluster is destroyed. In this case, the minikube <code>standard</code> StorageClass is fulfilled by the <code>hostpath</code> provisioner, whose persistence is removed as follows\u2026 <code>sudo rm -rf /tmp/hostpath-provisioner</code></p> </li> <li> <p>Client Credentials   During the deployment a client of the Authorisation Server is registered, and its credentials stored for reuse in the file <code>client.yaml</code>. Once the cluster has been destroyed, then these client credentials become stale and so should be removed to avoid polluting subsequent deployments\u2026 <code>rm -rf ./deploy/eoepca/client.yaml</code></p> </li> </ol> <p>There is a helper script <code>clean</code> that can be used for steps 2 and 3 above, (the script does not delete the cluster). <pre><code>./deploy/cluster/clean\n</code></pre></p>"},{"location":"quickstart/simple-deployment/","title":"Simple Deployment","text":"<p>A deployment wrapper script has been prepared for a \u2018simple\u2019 deployment - designed to get a core local deployment of the primary servies.</p> <p>The script <code>deploy/simple/simple</code> achieves this by appropriate configuration of the environment variables, before launching the eoepca.sh deployment script. The deployment configuration is captured in the file <code>deploy/simple/simple-options</code>.</p> <p>The simple deployment applies the following configuration:</p> <ul> <li>Assumes a private deployment - i.e. no external-facing IP/ingress, and hence no TLS To configure an external-facing deployment with TLS protection, then see section Public Deployment</li> <li>No TLS for service ingress endpoints</li> <li>Configuration of \u2018open\u2019 interfaces - i.e. service/API endpoints that are not protected and can accessed without authentication. This facilitates experimentation with the services</li> <li>Configuration of ADES stage-out to a local instance of <code>minio</code>, on the assumption that access to CREODIAS buckets for stage-out (via Workspace) is not an option</li> </ul>"},{"location":"quickstart/simple-deployment/#initiate-deployment","title":"Initiate Deployment","text":"<p>Deployment is initiated by invoking the script\u2026</p> <pre><code>./deploy/simple/simple\n</code></pre> <p>See section Deployment for more details regarding the outcome of the scripted deployment.</p>"},{"location":"quickstart/simple-deployment/#protection","title":"Protection","text":"<p>See section Protection for more details regarding the protection of the deployed services - which, for the simple deployment, is performed via the script <code>deploy/simple/simple-protection</code>\u2026</p> <pre><code>./deploy/simple/simple-protection\n</code></pre>"},{"location":"quickstart/userman-deployment/","title":"User Management Deployment","text":"<p>A deployment wrapper script has been prepared for a \u2018user management\u2019 deployment - that is focused on the Login Service, PDP and User Profile.</p> <p>The script <code>deploy/userman/userman</code> achieves this by appropriate configuration of the environment variables, before launching the eoepca.sh deployment script. The deployment configuration is captured in the file <code>deploy/userman/userman-options</code>.</p> <p>The user-management deployment applies the following configuration:</p> <ul> <li>Assumes a private deployment - i.e. no external-facing IP/ingress, and hence no TLS To configure an external-facing deployment with TLS protection, then see section Public Deployment</li> <li>No TLS for service ingress endpoints</li> <li>Services deployed:<ul> <li>Login Service</li> <li>Policy Decision Point (PDP)</li> <li>User Profile</li> </ul> </li> <li>Other eoepca services not deployed</li> </ul>"},{"location":"quickstart/userman-deployment/#initiate-deployment","title":"Initiate Deployment","text":"<p>Deployment is initiated by invoking the script\u2026</p> <pre><code>./deploy/userman/userman\n</code></pre> <p>The Login Service is accessed at the endpoint <code>auth.&lt;domain&gt;</code> - e.g. <code>auth.192-168-49-2.nip.io</code>.</p>"}]}