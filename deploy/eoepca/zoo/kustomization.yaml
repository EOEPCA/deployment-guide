apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
helmCharts:
- releaseName: zoo-project-dru
  namespace: zp
  name: zoo-project-dru
  version: 0.2.4
  repo: https://zoo-project.github.io/charts/
  valuesInline:
    customConfig:
      main:
        eoepca: |-
          domain=eoepca.svc.rconway.uk
          workspace_prefix=ws
    cookiecutter:
      templateUrl: https://github.com/EOEPCA/eoepca-proc-service-template.git
      templateBranch: master
    iam:
      enabled: false
      # clientId: 536510d0-0a88-4c08-a273-35090d1031db
      # clientSecret: cd5b1516-549a-4dbb-836c-f5aead66eb97
      # name: OpenIDAuth
      # openIdConnectUrl: https://auth.c23121301.guide.eoepca.org/.well-known/openid-configuration
      # realm: Secured section
      # type: openIdConnect
      # userinfoUrl: https://auth.c23121301.guide.eoepca.org/oxauth/restv1/userinfo
    ingress:
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-production
      enabled: true
      hosts:
      - host: zoo-project-dru.eoepca.svc.rconway.uk
        paths:
        - path: /
          pathType: ImplementationSpecific
      tls:
      - hosts:
        - zoo-project-dru.eoepca.svc.rconway.uk
        secretName: zoo-project-dru-tls
    minio:
      enabled: false
      # auth:
      #   rootPassword: minio-secret-password
      #   rootUser: minio-admin
      # defaultBuckets: processingresults
      # persistence:
      #   accessMode: ReadWriteOnce
      #   enabled: true
      #   size: 2Gi
      #   storageClass: standard
    persistence:
      procServicesStorageClass: standard
      storageClass: standard
      tmpStorageClass: standard
    postgresql:
      primary:
        persistence:
          storageClass: standard
      readReplicas:
        persistence:
          storageClass: standard
    rabbitmq:
      persistence:
        storageClass: standard
    websocketd:
      enabled: false
    workflow:
      additionalInputs:
        APP: zoo-project-dru
        STAGEOUT_AWS_ACCESS_KEY_ID: minio-admin
        STAGEOUT_AWS_REGION: RegionOne
        STAGEOUT_AWS_SECRET_ACCESS_KEY: minio-secret-password
        STAGEOUT_AWS_SERVICEURL: http://zoo-project-dru-minio.zp.svc.cluster.local:9000
        STAGEOUT_OUTPUT: s3://processingresults
      nodeSelector:
        minikube.k8s.io/primary: "true"
        # node-role.kubernetes.io/worker: "true"
      storageClass: standard
      # defaultMaxRam: 512
      # defaultMaxCores: 1
    zoofpm:
      autoscaling:
        enabled: false
      extraMountPoints: []
      image:
        pullPolicy: IfNotPresent
        repository: zooproject/zoo-project
        tag: eoepca-983b8de2b98ce925cdc24753b17bc277261c0330
      replicaCount: 1
    zookernel:
      extraMountPoints: []
      image:
        pullPolicy: IfNotPresent
        repository: zooproject/zoo-project
        tag: eoepca-983b8de2b98ce925cdc24753b17bc277261c0330
    files:
      # Directory `files/cwlwrapper-assets` - assets for ConfigMap `XXX-cwlwrapper-config`
      cwlwrapperAssets:
        # main.yaml: ""
        # rules.yaml: ""
        # stagein.yaml: ""
        stageout.yaml: |-
          cwlVersion: v1.0
          class: CommandLineTool
          id: stage-out
          doc: "Stage-out the results to S3"
          inputs:
            process:
              type: string
            collection_id:
              type: string
            STAGEOUT_OUTPUT:
              type: string
            STAGEOUT_AWS_ACCESS_KEY_ID:
              type: string
            STAGEOUT_AWS_SECRET_ACCESS_KEY:
              type: string
            STAGEOUT_AWS_REGION:
              type: string
            STAGEOUT_AWS_SERVICEURL:
              type: string
          outputs:
            StacCatalogUri:
              outputBinding:
                outputEval: ${  return "s3://" + inputs.STAGEOUT_OUTPUT + "/" + inputs.process + "/catalog.json"; }
              type: string
          baseCommand:
            - python
            - stage.py
          arguments:
            - $( inputs.wf_outputs.path )
            - $( inputs.STAGEOUT_OUTPUT )
            - $( inputs.process )
            - $( inputs.collection_id )
          requirements:
            DockerRequirement:
              dockerPull: ghcr.io/terradue/ogc-eo-application-package-hands-on/stage:1.3.2
            InlineJavascriptRequirement: {}
            EnvVarRequirement:
              envDef:
                AWS_ACCESS_KEY_ID: $( inputs.STAGEOUT_AWS_ACCESS_KEY_ID )
                AWS_SECRET_ACCESS_KEY: $( inputs.STAGEOUT_AWS_SECRET_ACCESS_KEY )
                AWS_REGION: $( inputs.STAGEOUT_AWS_REGION )
                AWS_S3_ENDPOINT: $( inputs.STAGEOUT_AWS_SERVICEURL )
            InitialWorkDirRequirement:
              listing:
                - entryname: stage.py
                  entry: |-
                    import os
                    import sys
                    import pystac
                    import botocore
                    import boto3
                    import shutil
                    from pystac.stac_io import DefaultStacIO, StacIO
                    from urllib.parse import urlparse
                    from datetime import datetime

                    cat_url = sys.argv[1]
                    bucket = sys.argv[2]
                    subfolder = sys.argv[3]
                    collection_id = sys.argv[4]

                    print(f"cat_url: {cat_url}", file=sys.stderr)
                    print(f"bucket: {bucket}", file=sys.stderr)
                    print(f"subfolder: {subfolder}", file=sys.stderr)

                    aws_access_key_id = os.environ["AWS_ACCESS_KEY_ID"]
                    aws_secret_access_key = os.environ["AWS_SECRET_ACCESS_KEY"]
                    region_name = os.environ["AWS_REGION"]
                    endpoint_url = os.environ["AWS_S3_ENDPOINT"]

                    shutil.copytree(cat_url, "/tmp/catalog")
                    cat = pystac.read_file(os.path.join("/tmp/catalog", "catalog.json"))

                    class CustomStacIO(DefaultStacIO):
                        """Custom STAC IO class that uses boto3 to read from S3."""

                        def __init__(self):
                            self.session = botocore.session.Session()
                            self.s3_client = self.session.create_client(
                                service_name="s3",
                                use_ssl=True,
                                aws_access_key_id=aws_access_key_id,
                                aws_secret_access_key=aws_secret_access_key,
                                endpoint_url=endpoint_url,
                                region_name=region_name,
                            )

                        def write_text(self, dest, txt, *args, **kwargs):
                            parsed = urlparse(dest)
                            if parsed.scheme == "s3":
                                self.s3_client.put_object(
                                    Body=txt.encode("UTF-8"),
                                    Bucket=parsed.netloc,
                                    Key=parsed.path[1:],
                                    ContentType="application/geo+json",
                                )
                            else:
                                super().write_text(dest, txt, *args, **kwargs)


                    client = boto3.client(
                        "s3",
                        aws_access_key_id=aws_access_key_id,
                        aws_secret_access_key=aws_secret_access_key,
                        endpoint_url=endpoint_url,
                        region_name=region_name,
                    )

                    StacIO.set_default(CustomStacIO)

                    # create a STAC collection for the process
                    date = datetime.now().strftime("%Y-%m-%d")

                    dates = [datetime.strptime(
                        f"{date}T00:00:00", "%Y-%m-%dT%H:%M:%S"
                    ), datetime.strptime(f"{date}T23:59:59", "%Y-%m-%dT%H:%M:%S")]

                    collection = pystac.Collection(
                      id=collection_id,
                      description="description",
                      extent=pystac.Extent(
                        spatial=pystac.SpatialExtent([[-180, -90, 180, 90]]), 
                        temporal=pystac.TemporalExtent(intervals=[[min(dates), max(dates)]])
                      ),
                      title="Processing results",
                      href=f"s3://{bucket}/{subfolder}/collection.json",
                      stac_extensions=[],
                      keywords=["eoepca"],
                      license="proprietary",
                    )

                    for index, link in enumerate(cat.links):
                      if link.rel == "root":
                          cat.links.pop(index) # remove root link

                    for item in cat.get_items():

                        item.set_collection(collection)
                        
                        collection.add_item(item)
                        
                        for key, asset in item.get_assets().items():
                            s3_path = os.path.normpath(
                                os.path.join(subfolder, collection_id, item.id, os.path.basename(asset.href))
                            )
                            print(f"upload {asset.href} to s3://{bucket}/{s3_path}",file=sys.stderr)
                            client.upload_file(
                                asset.get_absolute_href(),
                                bucket,
                                s3_path,
                            )
                            asset.href = f"s3://{bucket}/{s3_path}"
                            item.add_asset(key, asset)

                    collection.update_extent_from_items() 

                    cat.clear_items()
                    
                    cat.add_child(collection)

                    cat.normalize_hrefs(f"s3://{bucket}/{subfolder}")

                    for item in collection.get_items():
                        # upload item to S3
                        print(f"upload {item.id} to s3://{bucket}/{subfolder}", file=sys.stderr)
                        pystac.write_file(item, item.get_self_href())

                    # upload collection to S3
                    print(f"upload collection.json to s3://{bucket}/{subfolder}", file=sys.stderr)
                    pystac.write_file(collection, collection.get_self_href())

                    # upload catalog to S3
                    print(f"upload catalog.json to s3://{bucket}/{subfolder}", file=sys.stderr)
                    pystac.write_file(cat, cat.get_self_href())

                    print(f"s3://{bucket}/{subfolder}/catalog.json", file=sys.stdout)
