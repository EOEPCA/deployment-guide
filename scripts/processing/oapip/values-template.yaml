cookiecutter:
  {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "calrissian" }}
  templateUrl: https://github.com/gfenoy/eoepca-proc-service-template.git
  templateBranch: feature/output-handling
  {{- else if eq (getenv "OAPIP_EXECUTION_ENGINE") "toil" }}
  templateUrl: https://github.com/gfenoy/eoepca-proc-service-template-wes.git
  templateBranch: develop
  {{- end }}
zoo:
  openapi:
    enabled: true
# zoofpm:
#   image:
#     tag: eoepca-39b6fc74b6592b4584be5a0b2740dde11a1ffafc
zookernel:
  env:
    ZOO_OUTPUT_FORMAT: "stac-collection"
  # image:
  #   tag: eoepca-39b6fc74b6592b4584be5a0b2740dde11a1ffafc
customConfig:
  main:
    eoepca: |-
      domain={{ getenv "INGRESS_HOST" }}
      {{- if eq (getenv "USE_WORKSPACE_API") "true" }}
      workspace_url=https://workspace-api.{{ getenv "INGRESS_HOST" }}
      {{- end }}
      workspace_prefix=ws
      workspace_catalog_register=false
workflow:
  defaultMaxRam: "1024"
  defaultMaxCores: "2"
  inputs:
  {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "toil" }}
    WES_URL: {{ getenv "OAPIP_TOIL_WES_URL" }}
    WES_USER: {{ getenv "OAPIP_TOIL_WES_USER" }}
    WES_PASSWORD: {{ getenv "OAPIP_TOIL_WES_PASSWORD" }}
  additionalInputs:
  {{- end }}
    STAGEIN_AWS_SERVICEURL: {{ getenv "STAGEIN_S3_ENDPOINT" }}
    STAGEIN_AWS_ACCESS_KEY_ID: {{ getenv "STAGEIN_S3_ACCESS_KEY" }}
    STAGEIN_AWS_SECRET_ACCESS_KEY: {{ getenv "STAGEIN_S3_SECRET_KEY" }}
    STAGEIN_AWS_REGION: {{ getenv "STAGEIN_S3_REGION" }}
    STAGEOUT_AWS_SERVICEURL: {{ getenv "S3_ENDPOINT" }}
    STAGEOUT_AWS_ACCESS_KEY_ID: {{ getenv "S3_ACCESS_KEY" }}
    STAGEOUT_AWS_SECRET_ACCESS_KEY: {{ getenv "S3_SECRET_KEY" }}
    STAGEOUT_AWS_REGION: {{ getenv "S3_REGION" }}
    STAGEOUT_OUTPUT: eoepca
  nodeSelector:
    {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "calrissian" }}
    {{ getenv "NODE_SELECTOR_KEY" }}: "{{ getenv "NODE_SELECTOR_VALUE" }}"
    {{- end }}
  storageClass: {{ getenv "SHARED_STORAGECLASS" }}
ingress:
  enabled: {{ getenv "OAPIP_INGRESS_ENABLED" }}
  hosturl: {{ getenv "OAPIP_HOST" }}
  className: {{ getenv "INGRESS_CLASS" }}
  annotations:
    kubernetes.io/ingress.class: {{ getenv "INGRESS_CLASS" }}
    {{- range $key, $value := (datasource "annotations") }}
    {{ $key }}: "{{ $value }}"
    {{- end }}
    argocd.argoproj.io/ignore-healthcheck: "true"
  hosts:
  - host: zoo.{{ getenv "INGRESS_HOST" }}
    paths:
    - path: /
      pathType: Prefix
  {{- if eq ( getenv "HTTP_SCHEME" ) "https" }}
  tls:
  - hosts:
    - zoo.{{ getenv "INGRESS_HOST" }}
    secretName: zoo-tls
  {{ end }}
persistence:
  procServicesStorageClass: {{ getenv "SHARED_STORAGECLASS" }}
  storageClass: {{ getenv "SHARED_STORAGECLASS" }}
  tmpStorageClass: {{ getenv "SHARED_STORAGECLASS" }}
postgresql:
  persistence:
    storageClass: {{ getenv "PERSISTENT_STORAGECLASS" }}
rabbitmq:
  autoSetup:
    enabled: true
  persistence:
    storageClass: {{ getenv "PERSISTENT_STORAGECLASS" }}
iam:
  enabled: false
documentation:
  enabled: true

{{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "toil" }}
files:
  cwlwrapperAssets:
    stageout.yaml: |-
      cwlVersion: v1.0
      class: CommandLineTool
      id: stage-out
      doc: "Stage-out the results to S3"
      inputs:
        sub_path:
          type: string
        collection_id:
          type: string
          default: "processing-results"
        STAGEOUT_OUTPUT:
          type: string
        STAGEOUT_AWS_ACCESS_KEY_ID:
          type: string
        STAGEOUT_AWS_SECRET_ACCESS_KEY:
          type: string
        STAGEOUT_AWS_REGION:
          type: string
        STAGEOUT_AWS_SERVICEURL:
          type: string
        stac_catalog:
          type: Directory
      outputs:
        StacCatalogUri:
          outputBinding:
            outputEval: |
              ${ 
                return { "value": "s3://" + inputs.STAGEOUT_OUTPUT + "/" + inputs.sub_path + "/catalog.json", "type": "https://raw.githubusercontent.com/eoap/schemas/main/string_format.yaml#URI" };
              }
          type: https://raw.githubusercontent.com/eoap/schemas/main/string_format.yaml#URI
      baseCommand:
        - python
        - stageout.py
      arguments:
        - $( inputs.STAGEOUT_OUTPUT )
        - $( inputs.sub_path )
        - $( inputs.collection_id )
        - $( inputs.stac_catalog.path )
      requirements:
        DockerRequirement:
          dockerPull: ghcr.io/terradue/ogc-eo-application-package-hands-on/stage:1.3.2
        SchemaDefRequirement:
          types:
          - $import: https://raw.githubusercontent.com/eoap/schemas/main/string_format.yaml
        InlineJavascriptRequirement: {}
        EnvVarRequirement:
          envDef:
            AWS_ACCESS_KEY_ID: $( inputs.STAGEOUT_AWS_ACCESS_KEY_ID )
            AWS_SECRET_ACCESS_KEY: $( inputs.STAGEOUT_AWS_SECRET_ACCESS_KEY )
            AWS_REGION: $( inputs.STAGEOUT_AWS_REGION )
            AWS_S3_ENDPOINT: $( inputs.STAGEOUT_AWS_SERVICEURL )
        InitialWorkDirRequirement:
          listing:
            - entryname: stageout.py
              entry: |-
                import os
                import sys
                import boto3
                
                bucket = sys.argv[1]
                subfolder = sys.argv[2]
                collection_id = sys.argv[3]
                cat_url = sys.argv[4]
                
                print(f"bucket: {bucket}", file=sys.stderr)
                print(f"subfolder: {subfolder}", file=sys.stderr)
                print(f"cat_url: {cat_url}", file=sys.stderr)
                
                client = boto3.client(
                    "s3",
                    aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
                    aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
                    endpoint_url=os.environ["AWS_S3_ENDPOINT"],
                    region_name=os.environ["AWS_REGION"],
                )
                
                # Upload all files in the directory
                for root, dirs, files in os.walk(cat_url):
                    for filename in files:
                        local_path = os.path.join(root, filename)
                        rel_path = os.path.relpath(local_path, cat_url)
                        s3_path = f"{subfolder}/{collection_id}/{rel_path}"
                        print(f"Uploading {local_path} to s3://{bucket}/{s3_path}", file=sys.stderr)
                        client.upload_file(local_path, bucket, s3_path)
                
                print(f"s3://{bucket}/{subfolder}/catalog.json", file=sys.stdout)

{{- end }}