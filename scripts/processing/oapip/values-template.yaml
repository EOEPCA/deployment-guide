cookiecutter:
  {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "calrissian" }}
  templateUrl: https://github.com/EOEPCA/eoepca-proc-service-template.git
  # templateBranch: feature/python3.8
  {{- else if eq (getenv "OAPIP_EXECUTION_ENGINE") "toil" }}
  templateUrl: https://github.com/EOEPCA/eoepca-proc-service-template-wes.git
  templateBranch: main
  {{- end }}
customConfig:
  main:
    eoepca: |-
      domain={{ getenv "INGRESS_HOST" }}
      {{ if eq (getenv "USE_WORKSPACE_API") "true" }}
      workspace_url=https://workspace-api.{{ getenv "INGRESS_HOST" }}
      {{ end }}
      workspace_prefix=ws
# Uncomment if docker registry credentials have been defined
# Ref.  https://eoepca.readthedocs.io/projects/deploy/en/latest/prerequisites/kubernetes/#creating-an-image-pull-secret
# imagePullSecrets:
#   - name: regcred
zoofpm:
  image:
    tag: eoepca-e912be9fd38445c7416a2b4b3d749b1ae67247ea
zookernel:
  env:
    ZOO_OUTPUT_FORMAT: "stac-collection"
  image:
    tag: eoepca-e912be9fd38445c7416a2b4b3d749b1ae67247ea
workflow:
  defaultMaxRam: 1024
  defaultMaxCores: 2
  inputs:
  {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "toil" }}
    WES_URL: {{ getenv "OAPIP_TOIL_WES_URL" }}
    WES_USER: {{ getenv "OAPIP_TOIL_WES_USER" }}
    WES_PASSWORD: {{ getenv "OAPIP_TOIL_WES_PASSWORD" }}
  additionalInputs:
  {{- end }}
    STAGEIN_AWS_SERVICEURL: {{ getenv "STAGEIN_S3_ENDPOINT" }}
    STAGEIN_AWS_ACCESS_KEY_ID: {{ getenv "STAGEIN_S3_ACCESS_KEY" }}
    STAGEIN_AWS_SECRET_ACCESS_KEY: {{ getenv "STAGEIN_S3_SECRET_KEY" }}
    STAGEIN_AWS_REGION: {{ getenv "STAGEIN_S3_REGION" }}
    STAGEOUT_AWS_SERVICEURL: {{ getenv "S3_ENDPOINT" }}
    STAGEOUT_AWS_ACCESS_KEY_ID: {{ getenv "S3_ACCESS_KEY" }}
    STAGEOUT_AWS_SECRET_ACCESS_KEY: {{ getenv "S3_SECRET_KEY" }}
    STAGEOUT_AWS_REGION: {{ getenv "S3_REGION" }}
    STAGEOUT_OUTPUT: eoepca
  nodeSelector:
    {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "calrissian" }}
    {{ getenv "NODE_SELECTOR_KEY" }}: "{{ getenv "NODE_SELECTOR_VALUE" }}"
    {{- end }}
  storageClass: {{ getenv "STORAGE_CLASS" }}
ingress:
  enabled: {{ getenv "OAPIP_INGRESS_ENABLED" }}
  hosturl: {{ getenv "OAPIP_HOST" }}
  ingressClassName: {{ getenv "INGRESS_CLASS" }}
  class: {{ getenv "INGRESS_CLASS" }}
  className: {{ getenv "INGRESS_CLASS" }}
  annotations:
    kubernetes.io/ingress.class: {{ getenv "INGRESS_CLASS" }}
    {{- range $key, $value := (datasource "annotations") }}
    {{ $key }}: "{{ $value }}"
    {{- end }}
  hosts:
  - host: zoo.{{ getenv "INGRESS_HOST" }}
    paths:
    - path: /
      pathType: Prefix
  {{- if eq ( getenv "HTTP_SCHEME" ) "https" }}
  tls:
  - hosts:
    - zoo.{{ getenv "INGRESS_HOST" }}
    secretName: zoo-tls
  {{ end }}
persistence:
  procServicesStorageClass: {{ getenv "STORAGE_CLASS" }}
  storageClass: {{ getenv "STORAGE_CLASS" }}
  tmpStorageClass: {{ getenv "STORAGE_CLASS" }}
postgresql:
  primary:
    persistence:
      storageClass: {{ getenv "STORAGE_CLASS" }}
  readReplicas:
    persistence:
      storageClass: {{ getenv "STORAGE_CLASS" }}
rabbitmq:
  persistence:
    storageClass: {{ getenv "STORAGE_CLASS" }}
iam:
  enabled: false
documentation:
  enabled: true
files:
  cwlwrapperAssets:
    stageout.yaml: |-
      cwlVersion: v1.0
      class: CommandLineTool
      id: stage-out
      doc: "Stage-out the results to S3"
      inputs:
        STAGEOUT_OUTPUT:
          type: string
        STAGEOUT_AWS_ACCESS_KEY_ID:
          type: string
        STAGEOUT_AWS_SECRET_ACCESS_KEY:
          type: string
        STAGEOUT_AWS_REGION:
          type: string
        STAGEOUT_AWS_SERVICEURL:
          type: string
        {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "toil" }}
        sub_path:
        {{- else if eq (getenv "OAPIP_EXECUTION_ENGINE") "calrissian" }}
        process:
        {{- end }}
          type: string
      outputs:
        StacCatalogUri:
          outputBinding:
            {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "toil" }}
            outputEval: ${  return "s3://" + inputs.STAGEOUT_OUTPUT + "/" + inputs.sub_path + "/catalog.json"; }
            {{- else if eq (getenv "OAPIP_EXECUTION_ENGINE") "calrissian" }}
            outputEval: ${  return "s3://" + inputs.STAGEOUT_OUTPUT + "/" + inputs.process + "/catalog.json"; }
            {{- end }}
          type: string
      baseCommand:
        - python
        - stageout.py
      arguments:
        - $( inputs.STAGEOUT_OUTPUT )
        {{- if eq (getenv "OAPIP_EXECUTION_ENGINE") "toil" }}
        - $( inputs.sub_path )
        {{- else if eq (getenv "OAPIP_EXECUTION_ENGINE") "calrissian" }}
        - $( inputs.process )
        {{- end }}
        - valueFrom: |
            ${
              if( !Array.isArray(inputs.wf_outputs) ) 
              {
                return inputs.wf_outputs.path;
              }
              var args=[];
              for (var i = 0; i < inputs.wf_outputs.length; i++) 
              {
                args.push(inputs.wf_outputs[i].path);
              }
              return args;
            }
      requirements:
        DockerRequirement:
          dockerPull: ghcr.io/terradue/ogc-eo-application-package-hands-on/stage:1.3.2
        InlineJavascriptRequirement: {}
        EnvVarRequirement:
          envDef:
            AWS_ACCESS_KEY_ID: $( inputs.STAGEOUT_AWS_ACCESS_KEY_ID )
            AWS_SECRET_ACCESS_KEY: $( inputs.STAGEOUT_AWS_SECRET_ACCESS_KEY )
            AWS_REGION: $( inputs.STAGEOUT_AWS_REGION )
            AWS_S3_ENDPOINT: $( inputs.STAGEOUT_AWS_SERVICEURL )
        InitialWorkDirRequirement:
          listing:
            - entryname: stageout.py
              entry: |-
                import os
                import sys
                import pystac
                import botocore
                import boto3
                import shutil
                from pystac.stac_io import DefaultStacIO, StacIO
                from urllib.parse import urlparse
                from datetime import datetime

                bucket = sys.argv[1]
                subfolder = sys.argv[2]
                collection_id = sys.argv[2]
                print(f"bucket: {bucket}", file=sys.stderr)
                print(f"subfolder: {subfolder}", file=sys.stderr)
                print(f"collection_id: {collection_id}", file=sys.stderr)

                # cat_url
                # Should really be one or more, for cwl outputs of type Directory[]
                # - but as a quick fix we just take the first one for now.
                cat_url = sys.argv[3]
                print(f"cat_url: {cat_url}", file=sys.stderr)

                aws_access_key_id = os.environ["AWS_ACCESS_KEY_ID"]
                aws_secret_access_key = os.environ["AWS_SECRET_ACCESS_KEY"]
                region_name = os.environ["AWS_REGION"]
                endpoint_url = os.environ["AWS_S3_ENDPOINT"]
                print(f"aws_access_key_id: {aws_access_key_id}", file=sys.stderr)
                print(f"aws_secret_access_key: {aws_secret_access_key}", file=sys.stderr)
                print(f"region_name: {region_name}", file=sys.stderr)
                print(f"endpoint_url: {endpoint_url}", file=sys.stderr)

                shutil.copytree(cat_url, "/tmp/catalog")
                cat = pystac.read_file(os.path.join("/tmp/catalog", "catalog.json"))

                class CustomStacIO(DefaultStacIO):
                    """Custom STAC IO class that uses boto3 to read from S3."""

                    def __init__(self):
                        self.session = botocore.session.Session()
                        self.s3_client = self.session.create_client(
                            service_name="s3",
                            use_ssl=True,
                            aws_access_key_id=aws_access_key_id,
                            aws_secret_access_key=aws_secret_access_key,
                            endpoint_url=endpoint_url,
                            region_name=region_name,
                        )

                    def write_text(self, dest, txt, *args, **kwargs):
                        parsed = urlparse(dest)
                        if parsed.scheme == "s3":
                            self.s3_client.put_object(
                                Body=txt.encode("UTF-8"),
                                Bucket=parsed.netloc,
                                Key=parsed.path[1:],
                                ContentType="application/geo+json",
                            )
                        else:
                            super().write_text(dest, txt, *args, **kwargs)


                client = boto3.client(
                    "s3",
                    aws_access_key_id=aws_access_key_id,
                    aws_secret_access_key=aws_secret_access_key,
                    endpoint_url=endpoint_url,
                    region_name=region_name,
                )

                StacIO.set_default(CustomStacIO)

                # create a STAC collection for the process
                date = datetime.now().strftime("%Y-%m-%d")

                dates = [datetime.strptime(
                    f"{date}T00:00:00", "%Y-%m-%dT%H:%M:%S"
                ), datetime.strptime(f"{date}T23:59:59", "%Y-%m-%dT%H:%M:%S")]

                collection = pystac.Collection(
                  id=collection_id,
                  description="description",
                  extent=pystac.Extent(
                    spatial=pystac.SpatialExtent([[-180, -90, 180, 90]]), 
                    temporal=pystac.TemporalExtent(intervals=[[min(dates), max(dates)]])
                  ),
                  title="Processing results",
                  href=f"s3://{bucket}/{subfolder}/collection.json",
                  stac_extensions=[],
                  keywords=["eoepca"],
                  license="proprietary",
                )

                for index, link in enumerate(cat.links):
                  if link.rel == "root":
                      cat.links.pop(index) # remove root link

                for item in cat.get_items():

                    item.set_collection(collection)
                    
                    collection.add_item(item)
                    
                    for key, asset in item.get_assets().items():
                        s3_path = os.path.normpath(
                            os.path.join(subfolder, collection_id, item.id, os.path.basename(asset.href))
                        )
                        print(f"upload {asset.href} to s3://{bucket}/{s3_path}",file=sys.stderr)
                        client.upload_file(
                            asset.get_absolute_href(),
                            bucket,
                            s3_path,
                        )
                        asset.href = f"s3://{bucket}/{s3_path}"
                        item.add_asset(key, asset)

                collection.update_extent_from_items() 

                cat.clear_items()

                cat.add_child(collection)

                cat.normalize_hrefs(f"s3://{bucket}/{subfolder}")

                for item in collection.get_items():
                    # upload item to S3
                    print(f"upload {item.id} to s3://{bucket}/{subfolder}", file=sys.stderr)
                    pystac.write_file(item, item.get_self_href())

                # upload collection to S3
                print(f"upload collection.json to s3://{bucket}/{subfolder}", file=sys.stderr)
                pystac.write_file(collection, collection.get_self_href())

                # upload catalog to S3
                print(f"upload catalog.json to s3://{bucket}/{subfolder}", file=sys.stderr)
                pystac.write_file(cat, cat.get_self_href())

                print(f"s3://{bucket}/{subfolder}/catalog.json", file=sys.stdout)
